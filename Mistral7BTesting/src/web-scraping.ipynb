{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = \"https://arxiv.org/html/2306.05685v4\"\n",
    "paper = requests.get(uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!DOCTYPE html>\\n\\n<html lang=\"en\">\\n<head>\\n<meta content=\"text/html; charset=utf-8\" http-equiv=\"content-type\"/>\\n<title>Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena</title>\\n<!--Generated on Sun Dec 24 02:00:19 2023 by LaTeXML (version 0.8.7) http://dlmf.nist.gov/LaTeXML/.-->\\n<meta content=\"width=device-width, initial-scale=1, shrink-to-fit=no\" name=\"viewport\"/>\\n<link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css\" rel=\"stylesheet\" type=\"text/css\"/>\\n<link href=\"/static/browse/0.3.4/css/ar5iv_0.7.4.min.css\" rel=\"stylesheet\" type=\"text/css\"/>\\n<link href=\"/static/browse/0.3.4/css/latexml_styles.css\" rel=\"stylesheet\" type=\"text/css\"/>\\n<script src=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js\"></script>\\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js\"></script>\\n<script src=\"/static/browse/0.3.4/js/addons.js\"></script>\\n<script src=\"/static/browse/0.3.4/js/feedbackOverlay.js\"></script>\\n<base href=\"/html/2306.05685v4/\"/></head>\\n<body>\\n<nav class=\"ltx_page_navbar\">\\n<nav class=\"ltx_TOC\">\\n<ol class=\"ltx_toclist\">\\n<li class=\"ltx_tocentry ltx_tocentry_section\"><a class=\"ltx_ref\" href=\"#S1\" title=\"1 Introduction \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">1 </span>Introduction</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_section\">\\n<a class=\"ltx_ref\" href=\"#S2\" title=\"2 MT-Bench and Chatbot Arena \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">2 </span>MT-Bench and Chatbot Arena</span></a>\\n<ol class=\"ltx_toclist ltx_toclist_section\">\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"#S2.SS1\" title=\"2.1 Motivation \\xe2\\x80\\xa3 2 MT-Bench and Chatbot Arena \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">2.1 </span>Motivation</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"#S2.SS2\" title=\"2.2 MT-Bench \\xe2\\x80\\xa3 2 MT-Bench and Chatbot Arena \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">2.2 </span>MT-Bench</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"#S2.SS3\" title=\"2.3 Chatbot Arena \\xe2\\x80\\xa3 2 MT-Bench and Chatbot Arena \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">2.3 </span>Chatbot Arena</span></a></li>\\n</ol>\\n</li>\\n<li class=\"ltx_tocentry ltx_tocentry_section\">\\n<a class=\"ltx_ref\" href=\"#S3\" title=\"3 LLM as a Judge \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">3 </span>LLM as a Judge</span></a>\\n<ol class=\"ltx_toclist ltx_toclist_section\">\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"#S3.SS1\" title=\"3.1 Types of LLM-as-a-Judge \\xe2\\x80\\xa3 3 LLM as a Judge \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">3.1 </span>Types of LLM-as-a-Judge</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"#S3.SS2\" title=\"3.2 Advantages of LLM-as-a-Judge \\xe2\\x80\\xa3 3 LLM as a Judge \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">3.2 </span>Advantages of LLM-as-a-Judge</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"#S3.SS3\" title=\"3.3 Limitations of LLM-as-a-Judge \\xe2\\x80\\xa3 3 LLM as a Judge \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">3.3 </span>Limitations of LLM-as-a-Judge</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"#S3.SS4\" title=\"3.4 Addressing limitations \\xe2\\x80\\xa3 3 LLM as a Judge \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">3.4 </span>Addressing limitations</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"#S3.SS5\" title=\"3.5 Multi-turn judge \\xe2\\x80\\xa3 3 LLM as a Judge \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">3.5 </span>Multi-turn judge</span></a></li>\\n</ol>\\n</li>\\n<li class=\"ltx_tocentry ltx_tocentry_section\">\\n<a class=\"ltx_ref\" href=\"#S4\" title=\"4 Agreement Evaluation \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">4 </span>Agreement Evaluation</span></a>\\n<ol class=\"ltx_toclist ltx_toclist_section\">\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"#S4.SS1\" title=\"4.1 Setup \\xe2\\x80\\xa3 4 Agreement Evaluation \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">4.1 </span>Setup</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"#S4.SS2\" title=\"4.2 High agreement between GPT-4 and humans \\xe2\\x80\\xa3 4 Agreement Evaluation \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">4.2 </span>High agreement between GPT-4 and humans</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"#S4.SS3\" title=\"4.3 Win rates under different judges \\xe2\\x80\\xa3 4 Agreement Evaluation \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">4.3 </span>Win rates under different judges</span></a></li>\\n</ol>\\n</li>\\n<li class=\"ltx_tocentry ltx_tocentry_section\"><a class=\"ltx_ref\" href=\"#S5\" title=\"5 Human Preference Benchmark and Standardized Benchmark \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">5 </span>Human Preference Benchmark and Standardized Benchmark</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_section\"><a class=\"ltx_ref\" href=\"#S6\" title=\"6 Discussion \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">6 </span>Discussion</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_section\"><a class=\"ltx_ref\" href=\"#S7\" title=\"7 Conclusion \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">7 </span>Conclusion</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_appendix\"><a class=\"ltx_ref\" href=\"#A1\" title=\"Appendix A Prompt templates \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">A </span>Prompt templates</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_appendix\"><a class=\"ltx_ref\" href=\"#A2\" title=\"Appendix B Case Study \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">B </span>Case Study</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_appendix\">\\n<a class=\"ltx_ref\" href=\"#A3\" title=\"Appendix C Data Collection \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">C </span>Data Collection</span></a>\\n<ol class=\"ltx_toclist ltx_toclist_appendix\">\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"#A3.SS1\" title=\"C.1 MT-bench human evaluation \\xe2\\x80\\xa3 Appendix C Data Collection \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">C.1 </span>MT-bench human evaluation</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"#A3.SS2\" title=\"C.2 Chatbot Arena \\xe2\\x80\\xa3 Appendix C Data Collection \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">C.2 </span>Chatbot Arena</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"#A3.SS3\" title=\"C.3 Data Release \\xe2\\x80\\xa3 Appendix C Data Collection \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">C.3 </span>Data Release</span></a></li>\\n</ol>\\n</li>\\n<li class=\"ltx_tocentry ltx_tocentry_appendix\">\\n<a class=\"ltx_ref\" href=\"#A4\" title=\"Appendix D Additional Experimental Results \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">D </span>Additional Experimental Results</span></a>\\n<ol class=\"ltx_toclist ltx_toclist_appendix\">\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"#A4.SS1\" title=\"D.1 Position bias \\xe2\\x80\\xa3 Appendix D Additional Experimental Results \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">D.1 </span>Position bias</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"#A4.SS2\" title=\"D.2 Few-shot judge \\xe2\\x80\\xa3 Appendix D Additional Experimental Results \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">D.2 </span>Few-shot judge</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"#A4.SS3\" title=\"D.3 Agreement Evaluation \\xe2\\x80\\xa3 Appendix D Additional Experimental Results \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">D.3 </span>Agreement Evaluation</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"#A4.SS4\" title=\"D.4 Category-wise scores with single-answer grading \\xe2\\x80\\xa3 Appendix D Additional Experimental Results \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">D.4 </span>Category-wise scores with single-answer grading</span></a></li>\\n</ol>\\n</li>\\n<li class=\"ltx_tocentry ltx_tocentry_appendix\"><a class=\"ltx_ref\" href=\"#A5\" title=\"Appendix E Training Details of Vicuna Models \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">E </span>Training Details of Vicuna Models</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_appendix\">\\n<a class=\"ltx_ref\" href=\"#A6\" title=\"Appendix F Exploring Vicuna as a judge \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">F </span>Exploring Vicuna as a judge</span></a>\\n<ol class=\"ltx_toclist ltx_toclist_appendix\">\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\"><a class=\"ltx_ref\" href=\"#A6.SS1\" title=\"F.1 Zero-Shot Vicuna \\xe2\\x80\\xa3 Appendix F Exploring Vicuna as a judge \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">F.1 </span>Zero-Shot Vicuna</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_subsection\">\\n<a class=\"ltx_ref\" href=\"#A6.SS2\" title=\"F.2 Arena Fine-tuned Vicuna \\xe2\\x80\\xa3 Appendix F Exploring Vicuna as a judge \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_title\"><span class=\"ltx_tag ltx_tag_ref\">F.2 </span>Arena Fine-tuned Vicuna</span></a>\\n<ol class=\"ltx_toclist ltx_toclist_subsection\">\\n<li class=\"ltx_tocentry ltx_tocentry_paragraph\"><a class=\"ltx_ref\" href=\"#A6.SS2.SSS0.Px1\" title=\"Training \\xe2\\x80\\xa3 F.2 Arena Fine-tuned Vicuna \\xe2\\x80\\xa3 Appendix F Exploring Vicuna as a judge \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_title\">Training</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_paragraph\"><a class=\"ltx_ref\" href=\"#A6.SS2.SSS0.Px2\" title=\"Position bias results \\xe2\\x80\\xa3 F.2 Arena Fine-tuned Vicuna \\xe2\\x80\\xa3 Appendix F Exploring Vicuna as a judge \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_title\">Position bias results</span></a></li>\\n<li class=\"ltx_tocentry ltx_tocentry_paragraph\"><a class=\"ltx_ref\" href=\"#A6.SS2.SSS0.Px3\" title=\"Agreement results \\xe2\\x80\\xa3 F.2 Arena Fine-tuned Vicuna \\xe2\\x80\\xa3 Appendix F Exploring Vicuna as a judge \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_title\">Agreement results</span></a></li>\\n</ol>\\n</li>\\n</ol>\\n</li>\\n</ol></nav>\\n</nav>\\n<div class=\"ltx_page_main\">\\n<div class=\"ltx_page_content\"><div class=\"section\" id=\"target-section\"><div id=\"license-tr\">License: arXiv.org perpetual non-exclusive license</div><div id=\"watermark-tr\">arXiv:2306.05685v4 [cs.CL] 24 Dec 2023</div></div>\\n<article class=\"ltx_document ltx_authors_1line\"><span class=\"ltx_ERROR undefined\" id=\"id1\">\\\\setitemize</span>\\n<div class=\"ltx_para\" id=\"p1\">\\n<p class=\"ltx_p\" id=\"p1.1\">noitemsep,topsep=0pt,parsep=0pt,partopsep=0pt,leftmargin=1.5em\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n</p>\\n</div>\\n<h1 class=\"ltx_title ltx_title_document\">Judging LLM-as-a-Judge \\n<br class=\"ltx_break\"/>with MT-Bench and Chatbot Arena</h1>\\n<div class=\"ltx_authors\">\\n<span class=\"ltx_creator ltx_role_author\">\\n<span class=\"ltx_personname\">\\nLianmin Zheng<math alttext=\"{}^{1}\" class=\"ltx_Math\" display=\"inline\" id=\"id1.1.m1.1\"><semantics id=\"id1.1.m1.1a\"><msup id=\"id1.1.m1.1.1\" xref=\"id1.1.m1.1.1.cmml\"><mi id=\"id1.1.m1.1.1a\" xref=\"id1.1.m1.1.1.cmml\"></mi><mn id=\"id1.1.m1.1.1.1\" xref=\"id1.1.m1.1.1.1.cmml\">1</mn></msup><annotation-xml encoding=\"MathML-Content\" id=\"id1.1.m1.1b\"><apply id=\"id1.1.m1.1.1.cmml\" xref=\"id1.1.m1.1.1\"><cn id=\"id1.1.m1.1.1.1.cmml\" type=\"integer\" xref=\"id1.1.m1.1.1.1\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"id1.1.m1.1c\">{}^{1}</annotation><annotation encoding=\"application/x-llamapun\" id=\"id1.1.m1.1d\">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math> \\xc2\\xa0\\xc2\\xa0\\xc2\\xa0 Wei-Lin Chiang<math alttext=\"{}^{1*}\" class=\"ltx_Math\" display=\"inline\" id=\"id3.3.m3.2\"><semantics id=\"id3.3.m3.2a\"><msup id=\"id3.3.m3.2.2\" xref=\"id3.3.m3.2.2.cmml\"><mi id=\"id3.3.m3.2.2a\" xref=\"id3.3.m3.2.2.cmml\"></mi><mrow id=\"id3.3.m3.2.2.2.4\" xref=\"id3.3.m3.2.2.2.3.cmml\"><mn id=\"id3.3.m3.1.1.1.1\" xref=\"id3.3.m3.1.1.1.1.cmml\">1</mn><mo id=\"id3.3.m3.2.2.2.4.1\" lspace=\"0.222em\" xref=\"id3.3.m3.2.2.2.3.cmml\">\\xe2\\x81\\xa3</mo><mo id=\"id3.3.m3.2.2.2.2\" xref=\"id3.3.m3.2.2.2.2.cmml\">*</mo></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"id3.3.m3.2b\"><apply id=\"id3.3.m3.2.2.cmml\" xref=\"id3.3.m3.2.2\"><list id=\"id3.3.m3.2.2.2.3.cmml\" xref=\"id3.3.m3.2.2.2.4\"><cn id=\"id3.3.m3.1.1.1.1.cmml\" type=\"integer\" xref=\"id3.3.m3.1.1.1.1\">1</cn><times id=\"id3.3.m3.2.2.2.2.cmml\" xref=\"id3.3.m3.2.2.2.2\"></times></list></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"id3.3.m3.2c\">{}^{1*}</annotation><annotation encoding=\"application/x-llamapun\" id=\"id3.3.m3.2d\">start_FLOATSUPERSCRIPT 1 * end_FLOATSUPERSCRIPT</annotation></semantics></math> \\xc2\\xa0\\xc2\\xa0\\xc2\\xa0 Ying Sheng<math alttext=\"{}^{4*}\" class=\"ltx_Math\" display=\"inline\" id=\"id5.5.m5.2\"><semantics id=\"id5.5.m5.2a\"><msup id=\"id5.5.m5.2.2\" xref=\"id5.5.m5.2.2.cmml\"><mi id=\"id5.5.m5.2.2a\" xref=\"id5.5.m5.2.2.cmml\"></mi><mrow id=\"id5.5.m5.2.2.2.4\" xref=\"id5.5.m5.2.2.2.3.cmml\"><mn id=\"id5.5.m5.1.1.1.1\" xref=\"id5.5.m5.1.1.1.1.cmml\">4</mn><mo id=\"id5.5.m5.2.2.2.4.1\" lspace=\"0.222em\" xref=\"id5.5.m5.2.2.2.3.cmml\">\\xe2\\x81\\xa3</mo><mo id=\"id5.5.m5.2.2.2.2\" xref=\"id5.5.m5.2.2.2.2.cmml\">*</mo></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"id5.5.m5.2b\"><apply id=\"id5.5.m5.2.2.cmml\" xref=\"id5.5.m5.2.2\"><list id=\"id5.5.m5.2.2.2.3.cmml\" xref=\"id5.5.m5.2.2.2.4\"><cn id=\"id5.5.m5.1.1.1.1.cmml\" type=\"integer\" xref=\"id5.5.m5.1.1.1.1\">4</cn><times id=\"id5.5.m5.2.2.2.2.cmml\" xref=\"id5.5.m5.2.2.2.2\"></times></list></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"id5.5.m5.2c\">{}^{4*}</annotation><annotation encoding=\"application/x-llamapun\" id=\"id5.5.m5.2d\">start_FLOATSUPERSCRIPT 4 * end_FLOATSUPERSCRIPT</annotation></semantics></math> \\xc2\\xa0\\xc2\\xa0\\xc2\\xa0 Siyuan Zhuang<math alttext=\"{}^{1}\" class=\"ltx_Math\" display=\"inline\" id=\"id7.7.m7.1\"><semantics id=\"id7.7.m7.1a\"><msup id=\"id7.7.m7.1.1\" xref=\"id7.7.m7.1.1.cmml\"><mi id=\"id7.7.m7.1.1a\" xref=\"id7.7.m7.1.1.cmml\"></mi><mn id=\"id7.7.m7.1.1.1\" xref=\"id7.7.m7.1.1.1.cmml\">1</mn></msup><annotation-xml encoding=\"MathML-Content\" id=\"id7.7.m7.1b\"><apply id=\"id7.7.m7.1.1.cmml\" xref=\"id7.7.m7.1.1\"><cn id=\"id7.7.m7.1.1.1.cmml\" type=\"integer\" xref=\"id7.7.m7.1.1.1\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"id7.7.m7.1c\">{}^{1}</annotation><annotation encoding=\"application/x-llamapun\" id=\"id7.7.m7.1d\">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>\\n<span class=\"ltx_ERROR undefined\" id=\"id28.28.id1\">\\\\And</span>Zhanghao Wu<math alttext=\"{}^{1}\" class=\"ltx_Math\" display=\"inline\" id=\"id8.8.m8.1\"><semantics id=\"id8.8.m8.1a\"><msup id=\"id8.8.m8.1.1\" xref=\"id8.8.m8.1.1.cmml\"><mi id=\"id8.8.m8.1.1a\" xref=\"id8.8.m8.1.1.cmml\"></mi><mn id=\"id8.8.m8.1.1.1\" xref=\"id8.8.m8.1.1.1.cmml\">1</mn></msup><annotation-xml encoding=\"MathML-Content\" id=\"id8.8.m8.1b\"><apply id=\"id8.8.m8.1.1.cmml\" xref=\"id8.8.m8.1.1\"><cn id=\"id8.8.m8.1.1.1.cmml\" type=\"integer\" xref=\"id8.8.m8.1.1.1\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"id8.8.m8.1c\">{}^{1}</annotation><annotation encoding=\"application/x-llamapun\" id=\"id8.8.m8.1d\">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math> \\xc2\\xa0\\xc2\\xa0\\xc2\\xa0 Yonghao Zhuang<math alttext=\"{}^{3}\" class=\"ltx_Math\" display=\"inline\" id=\"id10.10.m10.1\"><semantics id=\"id10.10.m10.1a\"><msup id=\"id10.10.m10.1.1\" xref=\"id10.10.m10.1.1.cmml\"><mi id=\"id10.10.m10.1.1a\" xref=\"id10.10.m10.1.1.cmml\"></mi><mn id=\"id10.10.m10.1.1.1\" xref=\"id10.10.m10.1.1.1.cmml\">3</mn></msup><annotation-xml encoding=\"MathML-Content\" id=\"id10.10.m10.1b\"><apply id=\"id10.10.m10.1.1.cmml\" xref=\"id10.10.m10.1.1\"><cn id=\"id10.10.m10.1.1.1.cmml\" type=\"integer\" xref=\"id10.10.m10.1.1.1\">3</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"id10.10.m10.1c\">{}^{3}</annotation><annotation encoding=\"application/x-llamapun\" id=\"id10.10.m10.1d\">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math> \\xc2\\xa0\\xc2\\xa0\\xc2\\xa0 Zi Lin<math alttext=\"{}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"id12.12.m12.1\"><semantics id=\"id12.12.m12.1a\"><msup id=\"id12.12.m12.1.1\" xref=\"id12.12.m12.1.1.cmml\"><mi id=\"id12.12.m12.1.1a\" xref=\"id12.12.m12.1.1.cmml\"></mi><mn id=\"id12.12.m12.1.1.1\" xref=\"id12.12.m12.1.1.1.cmml\">2</mn></msup><annotation-xml encoding=\"MathML-Content\" id=\"id12.12.m12.1b\"><apply id=\"id12.12.m12.1.1.cmml\" xref=\"id12.12.m12.1.1\"><cn id=\"id12.12.m12.1.1.1.cmml\" type=\"integer\" xref=\"id12.12.m12.1.1.1\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"id12.12.m12.1c\">{}^{2}</annotation><annotation encoding=\"application/x-llamapun\" id=\"id12.12.m12.1d\">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math> \\xc2\\xa0\\xc2\\xa0\\xc2\\xa0 Zhuohan Li<math alttext=\"{}^{1}\" class=\"ltx_Math\" display=\"inline\" id=\"id14.14.m14.1\"><semantics id=\"id14.14.m14.1a\"><msup id=\"id14.14.m14.1.1\" xref=\"id14.14.m14.1.1.cmml\"><mi id=\"id14.14.m14.1.1a\" xref=\"id14.14.m14.1.1.cmml\"></mi><mn id=\"id14.14.m14.1.1.1\" xref=\"id14.14.m14.1.1.1.cmml\">1</mn></msup><annotation-xml encoding=\"MathML-Content\" id=\"id14.14.m14.1b\"><apply id=\"id14.14.m14.1.1.cmml\" xref=\"id14.14.m14.1.1\"><cn id=\"id14.14.m14.1.1.1.cmml\" type=\"integer\" xref=\"id14.14.m14.1.1.1\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"id14.14.m14.1c\">{}^{1}</annotation><annotation encoding=\"application/x-llamapun\" id=\"id14.14.m14.1d\">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math> \\xc2\\xa0\\xc2\\xa0\\xc2\\xa0 Dacheng Li<math alttext=\"{}^{13}\" class=\"ltx_Math\" display=\"inline\" id=\"id16.16.m16.1\"><semantics id=\"id16.16.m16.1a\"><msup id=\"id16.16.m16.1.1\" xref=\"id16.16.m16.1.1.cmml\"><mi id=\"id16.16.m16.1.1a\" xref=\"id16.16.m16.1.1.cmml\"></mi><mn id=\"id16.16.m16.1.1.1\" xref=\"id16.16.m16.1.1.1.cmml\">13</mn></msup><annotation-xml encoding=\"MathML-Content\" id=\"id16.16.m16.1b\"><apply id=\"id16.16.m16.1.1.cmml\" xref=\"id16.16.m16.1.1\"><cn id=\"id16.16.m16.1.1.1.cmml\" type=\"integer\" xref=\"id16.16.m16.1.1.1\">13</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"id16.16.m16.1c\">{}^{13}</annotation><annotation encoding=\"application/x-llamapun\" id=\"id16.16.m16.1d\">start_FLOATSUPERSCRIPT 13 end_FLOATSUPERSCRIPT</annotation></semantics></math>\\n<span class=\"ltx_ERROR undefined\" id=\"id29.29.id2\">\\\\And</span>Eric P. Xing<math alttext=\"{}^{35}\" class=\"ltx_Math\" display=\"inline\" id=\"id17.17.m17.1\"><semantics id=\"id17.17.m17.1a\"><msup id=\"id17.17.m17.1.1\" xref=\"id17.17.m17.1.1.cmml\"><mi id=\"id17.17.m17.1.1a\" xref=\"id17.17.m17.1.1.cmml\"></mi><mn id=\"id17.17.m17.1.1.1\" xref=\"id17.17.m17.1.1.1.cmml\">35</mn></msup><annotation-xml encoding=\"MathML-Content\" id=\"id17.17.m17.1b\"><apply id=\"id17.17.m17.1.1.cmml\" xref=\"id17.17.m17.1.1\"><cn id=\"id17.17.m17.1.1.1.cmml\" type=\"integer\" xref=\"id17.17.m17.1.1.1\">35</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"id17.17.m17.1c\">{}^{35}</annotation><annotation encoding=\"application/x-llamapun\" id=\"id17.17.m17.1d\">start_FLOATSUPERSCRIPT 35 end_FLOATSUPERSCRIPT</annotation></semantics></math> \\xc2\\xa0\\xc2\\xa0\\xc2\\xa0 Hao Zhang<math alttext=\"{}^{12}\" class=\"ltx_Math\" display=\"inline\" id=\"id19.19.m19.1\"><semantics id=\"id19.19.m19.1a\"><msup id=\"id19.19.m19.1.1\" xref=\"id19.19.m19.1.1.cmml\"><mi id=\"id19.19.m19.1.1a\" xref=\"id19.19.m19.1.1.cmml\"></mi><mn id=\"id19.19.m19.1.1.1\" xref=\"id19.19.m19.1.1.1.cmml\">12</mn></msup><annotation-xml encoding=\"MathML-Content\" id=\"id19.19.m19.1b\"><apply id=\"id19.19.m19.1.1.cmml\" xref=\"id19.19.m19.1.1\"><cn id=\"id19.19.m19.1.1.1.cmml\" type=\"integer\" xref=\"id19.19.m19.1.1.1\">12</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"id19.19.m19.1c\">{}^{12}</annotation><annotation encoding=\"application/x-llamapun\" id=\"id19.19.m19.1d\">start_FLOATSUPERSCRIPT 12 end_FLOATSUPERSCRIPT</annotation></semantics></math> \\xe2\\x80\\x83Joseph E. Gonzalez<math alttext=\"{}^{1}\" class=\"ltx_Math\" display=\"inline\" id=\"id20.20.m20.1\"><semantics id=\"id20.20.m20.1a\"><msup id=\"id20.20.m20.1.1\" xref=\"id20.20.m20.1.1.cmml\"><mi id=\"id20.20.m20.1.1a\" xref=\"id20.20.m20.1.1.cmml\"></mi><mn id=\"id20.20.m20.1.1.1\" xref=\"id20.20.m20.1.1.1.cmml\">1</mn></msup><annotation-xml encoding=\"MathML-Content\" id=\"id20.20.m20.1b\"><apply id=\"id20.20.m20.1.1.cmml\" xref=\"id20.20.m20.1.1\"><cn id=\"id20.20.m20.1.1.1.cmml\" type=\"integer\" xref=\"id20.20.m20.1.1.1\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"id20.20.m20.1c\">{}^{1}</annotation><annotation encoding=\"application/x-llamapun\" id=\"id20.20.m20.1d\">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math> \\xc2\\xa0\\xc2\\xa0\\xc2\\xa0 Ion Stoica<math alttext=\"{}^{1}\" class=\"ltx_Math\" display=\"inline\" id=\"id22.22.m22.1\"><semantics id=\"id22.22.m22.1a\"><msup id=\"id22.22.m22.1.1\" xref=\"id22.22.m22.1.1.cmml\"><mi id=\"id22.22.m22.1.1a\" xref=\"id22.22.m22.1.1.cmml\"></mi><mn id=\"id22.22.m22.1.1.1\" xref=\"id22.22.m22.1.1.1.cmml\">1</mn></msup><annotation-xml encoding=\"MathML-Content\" id=\"id22.22.m22.1b\"><apply id=\"id22.22.m22.1.1.cmml\" xref=\"id22.22.m22.1.1\"><cn id=\"id22.22.m22.1.1.1.cmml\" type=\"integer\" xref=\"id22.22.m22.1.1.1\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"id22.22.m22.1c\">{}^{1}</annotation><annotation encoding=\"application/x-llamapun\" id=\"id22.22.m22.1d\">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math>\\n<br class=\"ltx_break\"/>\\n<br class=\"ltx_break\"/><math alttext=\"{}^{1}\" class=\"ltx_Math\" display=\"inline\" id=\"id23.23.m23.1\"><semantics id=\"id23.23.m23.1a\"><msup id=\"id23.23.m23.1.1\" xref=\"id23.23.m23.1.1.cmml\"><mi id=\"id23.23.m23.1.1a\" xref=\"id23.23.m23.1.1.cmml\"></mi><mn id=\"id23.23.m23.1.1.1\" xref=\"id23.23.m23.1.1.1.cmml\">1</mn></msup><annotation-xml encoding=\"MathML-Content\" id=\"id23.23.m23.1b\"><apply id=\"id23.23.m23.1.1.cmml\" xref=\"id23.23.m23.1.1\"><cn id=\"id23.23.m23.1.1.1.cmml\" type=\"integer\" xref=\"id23.23.m23.1.1.1\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"id23.23.m23.1c\">{}^{1}</annotation><annotation encoding=\"application/x-llamapun\" id=\"id23.23.m23.1d\">start_FLOATSUPERSCRIPT 1 end_FLOATSUPERSCRIPT</annotation></semantics></math> UC Berkeley \\xe2\\x80\\x83<math alttext=\"{}^{2}\" class=\"ltx_Math\" display=\"inline\" id=\"id24.24.m24.1\"><semantics id=\"id24.24.m24.1a\"><msup id=\"id24.24.m24.1.1\" xref=\"id24.24.m24.1.1.cmml\"><mi id=\"id24.24.m24.1.1a\" xref=\"id24.24.m24.1.1.cmml\"></mi><mn id=\"id24.24.m24.1.1.1\" xref=\"id24.24.m24.1.1.1.cmml\">2</mn></msup><annotation-xml encoding=\"MathML-Content\" id=\"id24.24.m24.1b\"><apply id=\"id24.24.m24.1.1.cmml\" xref=\"id24.24.m24.1.1\"><cn id=\"id24.24.m24.1.1.1.cmml\" type=\"integer\" xref=\"id24.24.m24.1.1.1\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"id24.24.m24.1c\">{}^{2}</annotation><annotation encoding=\"application/x-llamapun\" id=\"id24.24.m24.1d\">start_FLOATSUPERSCRIPT 2 end_FLOATSUPERSCRIPT</annotation></semantics></math> UC San Diego \\xe2\\x80\\x83<math alttext=\"{}^{3}\" class=\"ltx_Math\" display=\"inline\" id=\"id25.25.m25.1\"><semantics id=\"id25.25.m25.1a\"><msup id=\"id25.25.m25.1.1\" xref=\"id25.25.m25.1.1.cmml\"><mi id=\"id25.25.m25.1.1a\" xref=\"id25.25.m25.1.1.cmml\"></mi><mn id=\"id25.25.m25.1.1.1\" xref=\"id25.25.m25.1.1.1.cmml\">3</mn></msup><annotation-xml encoding=\"MathML-Content\" id=\"id25.25.m25.1b\"><apply id=\"id25.25.m25.1.1.cmml\" xref=\"id25.25.m25.1.1\"><cn id=\"id25.25.m25.1.1.1.cmml\" type=\"integer\" xref=\"id25.25.m25.1.1.1\">3</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"id25.25.m25.1c\">{}^{3}</annotation><annotation encoding=\"application/x-llamapun\" id=\"id25.25.m25.1d\">start_FLOATSUPERSCRIPT 3 end_FLOATSUPERSCRIPT</annotation></semantics></math> Carnegie Mellon University \\xe2\\x80\\x83<math alttext=\"{}^{4}\" class=\"ltx_Math\" display=\"inline\" id=\"id26.26.m26.1\"><semantics id=\"id26.26.m26.1a\"><msup id=\"id26.26.m26.1.1\" xref=\"id26.26.m26.1.1.cmml\"><mi id=\"id26.26.m26.1.1a\" xref=\"id26.26.m26.1.1.cmml\"></mi><mn id=\"id26.26.m26.1.1.1\" xref=\"id26.26.m26.1.1.1.cmml\">4</mn></msup><annotation-xml encoding=\"MathML-Content\" id=\"id26.26.m26.1b\"><apply id=\"id26.26.m26.1.1.cmml\" xref=\"id26.26.m26.1.1\"><cn id=\"id26.26.m26.1.1.1.cmml\" type=\"integer\" xref=\"id26.26.m26.1.1.1\">4</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"id26.26.m26.1c\">{}^{4}</annotation><annotation encoding=\"application/x-llamapun\" id=\"id26.26.m26.1d\">start_FLOATSUPERSCRIPT 4 end_FLOATSUPERSCRIPT</annotation></semantics></math> Stanford \\xe2\\x80\\x83<math alttext=\"{}^{5}\" class=\"ltx_Math\" display=\"inline\" id=\"id27.27.m27.1\"><semantics id=\"id27.27.m27.1a\"><msup id=\"id27.27.m27.1.1\" xref=\"id27.27.m27.1.1.cmml\"><mi id=\"id27.27.m27.1.1a\" xref=\"id27.27.m27.1.1.cmml\"></mi><mn id=\"id27.27.m27.1.1.1\" xref=\"id27.27.m27.1.1.1.cmml\">5</mn></msup><annotation-xml encoding=\"MathML-Content\" id=\"id27.27.m27.1b\"><apply id=\"id27.27.m27.1.1.cmml\" xref=\"id27.27.m27.1.1\"><cn id=\"id27.27.m27.1.1.1.cmml\" type=\"integer\" xref=\"id27.27.m27.1.1.1\">5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"id27.27.m27.1c\">{}^{5}</annotation><annotation encoding=\"application/x-llamapun\" id=\"id27.27.m27.1d\">start_FLOATSUPERSCRIPT 5 end_FLOATSUPERSCRIPT</annotation></semantics></math> MBZUAI\\n</span><span class=\"ltx_author_notes\">Joint first authors. This paper is an extended version of our earlier blog post\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib8\" title=\"\">8</a>]</cite>.</span></span>\\n</div>\\n<div class=\"ltx_abstract\">\\n<h6 class=\"ltx_title ltx_title_abstract\">Abstract</h6>\\n<p class=\"ltx_p\" id=\"id30.id1\">Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences.\\nTo address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions.\\nWe examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them.\\nWe then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform.\\nOur results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans.\\nHence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain.\\nAdditionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna.\\nThe MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge\" title=\"\">https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge</a>.</p>\\n</div>\\n<section class=\"ltx_section\" id=\"S1\">\\n<h2 class=\"ltx_title ltx_title_section\">\\n<span class=\"ltx_tag ltx_tag_section\">1 </span>Introduction</h2>\\n<div class=\"ltx_para ltx_noindent\" id=\"S1.p1\">\\n<p class=\"ltx_p\" id=\"S1.p1.1\">There has been a proliferation of LLM-based chat assistants (chatbots) that leverage supervised instruction fine-tuning and reinforcement learning with human feedback (RLHF) to unlock new instruction following and conversational abilities\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib31\" title=\"\">31</a>, <a class=\"ltx_ref\" href=\"#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"#bib.bib30\" title=\"\">30</a>, <a class=\"ltx_ref\" href=\"#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"#bib.bib52\" title=\"\">52</a>, <a class=\"ltx_ref\" href=\"#bib.bib48\" title=\"\">48</a>, <a class=\"ltx_ref\" href=\"#bib.bib14\" title=\"\">14</a>]</cite>.\\nOnce aligned with humans, these chat models are strongly preferred by human users over the original, unaligned models on which they are built.\\nHowever, the heightened user preference does not always correspond to improved scores on traditional LLM benchmarks \\xe2\\x80\\x93 benchmarks like MMLU\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib19\" title=\"\">19</a>]</cite> and HELM\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib24\" title=\"\">24</a>]</cite> cannot effectively tell the difference between these aligned models and the base models.\\nThis phenomenon suggests that there is a fundamental discrepancy between user perceptions of the usefulness of chatbots and the criteria adopted by conventional benchmarks.</p>\\n</div>\\n<div class=\"ltx_para ltx_noindent\" id=\"S1.p2\">\\n<p class=\"ltx_p\" id=\"S1.p2.1\">We argue that this discrepancy primarily arises due to\\nexisting evaluation that only measures LLMs\\xe2\\x80\\x99 core capability on a confined set of tasks (e.g., multi-choice knowledge or retrieval questions), without adequately assessing its alignment with human preference in open-ended tasks, such as the ability to accurately adhere to instructions in multi-turn dialogues.\\nAs a demonstration, we show conversation histories with two models on an MMLU question in <a class=\"ltx_ref\" href=\"#S1.F1\" title=\"Figure 1 \\xe2\\x80\\xa3 1 Introduction \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">1</span></a>.\\nThe two models are LLaMA-13B\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib39\" title=\"\">39</a>]</cite>, a pre-trained base model without fine-tuning, and Vicuna-13B, our fine-tuned model from LLaMA-13B on high-quality conversations (the training details are in <a class=\"ltx_ref\" href=\"#A5\" title=\"Appendix E Training Details of Vicuna Models \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">E</span></a>).\\nDespite the base LLaMA models showing competitive performance on conventional benchmarks (<a class=\"ltx_ref\" href=\"#S4.T9\" title=\"Table 9 \\xe2\\x80\\xa3 4.3 Win rates under different judges \\xe2\\x80\\xa3 4 Agreement Evaluation \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Table</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">9</span></a>), its answers to open-ended questions are often not preferred by humans.\\nThis misalignment of conventional benchmarks underscores the core problem driving this paper:\\n<em class=\"ltx_emph ltx_font_italic\" id=\"S1.p2.1.1\">the need for a robust and scalable automated method to evaluate LLM alignment with human preferences.</em></p>\\n</div>\\n<figure class=\"ltx_figure\" id=\"S1.F1\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"689\" id=\"S1.F1.g1\" src=\"x1.png\" width=\"789\"/>\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 1: </span>Multi-turn dialogues between a user and two AI assistants\\xe2\\x80\\x94LLaMA-13B (Assistant A) and Vicuna-13B (Assistant B)\\xe2\\x80\\x94initiated by a question from the MMLU benchmark and a follow-up instruction. GPT-4 is then presented with the context to determine which assistant answers better.</figcaption>\\n</figure>\\n<div class=\"ltx_para ltx_noindent\" id=\"S1.p3\">\\n<p class=\"ltx_p\" id=\"S1.p3.1\">To study this, we introduce two benchmarks with human ratings as the primary evaluation metric: MT-bench and Chatbot Arena.\\nMT-bench is a series of open-ended questions that evaluate a chatbot\\xe2\\x80\\x99s multi-turn conversational and instruction-following ability \\xe2\\x80\\x93 two critical elements for human preference. MT-bench is also carefully constructed to differentiate chatbots based on their core capabilities, such as reasoning and math.\\nIn addition, we develop Chatbot Arena, a crowdsourced platform featuring anonymous battles between chatbots in real-world scenarios \\xe2\\x80\\x93 Users engage in conversations with two chatbots at the same time and rate their responses based on personal preferences.</p>\\n</div>\\n<div class=\"ltx_para ltx_noindent\" id=\"S1.p4\">\\n<p class=\"ltx_p\" id=\"S1.p4.1\">While human evaluation is the gold standard for assessing human preferences, it is exceptionally slow and costly.\\nTo automate the evaluation, we explore the use of state-of-the-art LLMs, such as GPT-4, as a surrogate for humans.\\nBecause these models are often trained with RLHF, they already exhibit strong human alignment.\\nWe call this approach <em class=\"ltx_emph ltx_font_italic\" id=\"S1.p4.1.1\">\\xe2\\x80\\x9cLLM-as-a-judge\\xe2\\x80\\x9d</em>.\\nThis approach has been tried in our earlier blog post\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib8\" title=\"\">8</a>]</cite> and other concurrent or follow-up work\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"#bib.bib29\" title=\"\">29</a>, <a class=\"ltx_ref\" href=\"#bib.bib14\" title=\"\">14</a>, <a class=\"ltx_ref\" href=\"#bib.bib12\" title=\"\">12</a>, <a class=\"ltx_ref\" href=\"#bib.bib52\" title=\"\">52</a>, <a class=\"ltx_ref\" href=\"#bib.bib18\" title=\"\">18</a>, <a class=\"ltx_ref\" href=\"#bib.bib33\" title=\"\">33</a>, <a class=\"ltx_ref\" href=\"#bib.bib40\" title=\"\">40</a>, <a class=\"ltx_ref\" href=\"#bib.bib7\" title=\"\">7</a>, <a class=\"ltx_ref\" href=\"#bib.bib43\" title=\"\">43</a>]</cite>.\\nHowever, there has not been a systematic study of this approach.</p>\\n</div>\\n<div class=\"ltx_para ltx_noindent\" id=\"S1.p5\">\\n<p class=\"ltx_p\" id=\"S1.p5.1\">In this paper, we study the LLM-as-a-judge approach by comparing it to the gold standard of human evaluation.\\nWe examine several potential limitations of the LLM-as-a-judge approach including position bias, verbosity bias, self-enhancement bias, and limited reasoning ability.\\nWe show that some of the biases are minor or can be mitigated.\\nOnce addressed, our results from 3K controlled expert votes and 3K crowdsourced human votes in the wild verify that GPT-4 judge match human evaluations at an agreement rate exceeding 80%, achieving the same level of human-human agreement (\\xc2\\xa7<a class=\"ltx_ref\" href=\"#S4.SS2\" title=\"4.2 High agreement between GPT-4 and humans \\xe2\\x80\\xa3 4 Agreement Evaluation \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">4.2</span></a>, Table\\xc2\\xa0<a class=\"ltx_ref\" href=\"#S3.T4\" title=\"Table 4 \\xe2\\x80\\xa3 3.3 Limitations of LLM-as-a-Judge \\xe2\\x80\\xa3 3 LLM as a Judge \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>).\\nConsequently, this suggests LLM-as-a-judge is a scalable method to swiftly evaluate human preference, serving as a promising alternative to traditional human evaluations. \\n<br class=\"ltx_break\"/></p>\\n</div>\\n<div class=\"ltx_para ltx_noindent\" id=\"S1.p6\">\\n<p class=\"ltx_p\" id=\"S1.p6.1\">This paper makes two contributions: (1) a systematic study of LLM-as-a-judge; and (2) human preference datasets with high-quality questions and diverse user interactions from MT-bench and Chatbot Arena.\\nIn addition, we argue for the adoption of a hybrid evaluation framework for future LLM benchmarks: by combining the existing capability-based benchmarks and the new preference-based benchmarks with LLM-as-a-judge, one can swiftly and automatically evaluate both the core capabilities and human alignment of models.\\nWe publicly release 80 MT-bench questions, 3K expert votes, and 30K conversations with human preferences for future study.</p>\\n</div>\\n<figure class=\"ltx_table\" id=\"S1.T1\">\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 1: </span>Sample multi-turn questions in MT-bench. </figcaption>\\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S1.T1.4\" style=\"width:390.3pt;height:104.8pt;vertical-align:-0.7pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-69.4pt,18.5pt) scale(0.737779581152346,0.737779581152346) ;\">\\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S1.T1.4.4\">\\n<thead class=\"ltx_thead\">\\n<tr class=\"ltx_tr\" id=\"S1.T1.4.4.5.1\">\\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" id=\"S1.T1.4.4.5.1.1\" style=\"width:43.4pt;\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S1.T1.4.4.5.1.1.1\">Category</p>\\n</th>\\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S1.T1.4.4.5.1.2\">Sample Questions</th>\\n</tr>\\n</thead>\\n<tbody class=\"ltx_tbody\">\\n<tr class=\"ltx_tr\" id=\"S1.T1.4.4.6.1\">\\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S1.T1.4.4.6.1.1\" rowspan=\"2\" style=\"width:43.4pt;\"><span class=\"ltx_text ltx_align_top\" id=\"S1.T1.4.4.6.1.1.1\">Writing</span></th>\\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S1.T1.4.4.6.1.2\" style=\"width:34.7pt;\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S1.T1.4.4.6.1.2.1\">1st Turn</p>\\n</td>\\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S1.T1.4.4.6.1.3\" style=\"width:325.2pt;\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S1.T1.4.4.6.1.3.1\">Compose an engaging travel blog post about a recent trip to Hawaii, highlighting cultural experiences and must-see attractions.</p>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S1.T1.4.4.7.2\">\\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S1.T1.4.4.7.2.1\" style=\"width:34.7pt;\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S1.T1.4.4.7.2.1.1\">2nd Turn</p>\\n</td>\\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S1.T1.4.4.7.2.2\" style=\"width:325.2pt;\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S1.T1.4.4.7.2.2.1\">Rewrite your previous response. Start every sentence with the letter A.</p>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S1.T1.2.2.2\">\\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S1.T1.2.2.2.3\" rowspan=\"2\" style=\"width:43.4pt;\"><span class=\"ltx_text ltx_align_top\" id=\"S1.T1.2.2.2.3.1\">Math</span></th>\\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S1.T1.2.2.2.4\" style=\"width:34.7pt;\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S1.T1.2.2.2.4.1\">1st Turn</p>\\n</td>\\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S1.T1.2.2.2.2\" style=\"width:325.2pt;\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S1.T1.2.2.2.2.2.2\">Given that <math alttext=\"f(x)=4x^{3}-9x-14\" class=\"ltx_Math\" display=\"inline\" id=\"S1.T1.1.1.1.1.1.1.m1.1\"><semantics id=\"S1.T1.1.1.1.1.1.1.m1.1a\"><mrow id=\"S1.T1.1.1.1.1.1.1.m1.1.2\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.cmml\"><mrow id=\"S1.T1.1.1.1.1.1.1.m1.1.2.2\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.2.cmml\"><mi id=\"S1.T1.1.1.1.1.1.1.m1.1.2.2.2\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.2.2.cmml\">f</mi><mo id=\"S1.T1.1.1.1.1.1.1.m1.1.2.2.1\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.2.1.cmml\">\\xe2\\x81\\xa2</mo><mrow id=\"S1.T1.1.1.1.1.1.1.m1.1.2.2.3.2\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.2.cmml\"><mo id=\"S1.T1.1.1.1.1.1.1.m1.1.2.2.3.2.1\" stretchy=\"false\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.2.cmml\">(</mo><mi id=\"S1.T1.1.1.1.1.1.1.m1.1.1\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.1.cmml\">x</mi><mo id=\"S1.T1.1.1.1.1.1.1.m1.1.2.2.3.2.2\" stretchy=\"false\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.2.cmml\">)</mo></mrow></mrow><mo id=\"S1.T1.1.1.1.1.1.1.m1.1.2.1\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.1.cmml\">=</mo><mrow id=\"S1.T1.1.1.1.1.1.1.m1.1.2.3\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.cmml\"><mrow id=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.2\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.2.cmml\"><mn id=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.2.2\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.2.2.cmml\">4</mn><mo id=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.2.1\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.2.1.cmml\">\\xe2\\x81\\xa2</mo><msup id=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.2.3\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.2.3.cmml\"><mi id=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.2.3.2\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.2.3.2.cmml\">x</mi><mn id=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.2.3.3\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.2.3.3.cmml\">3</mn></msup></mrow><mo id=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.1\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.1.cmml\">\\xe2\\x88\\x92</mo><mrow id=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.3\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.3.cmml\"><mn id=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.3.2\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.3.2.cmml\">9</mn><mo id=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.3.1\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.3.1.cmml\">\\xe2\\x81\\xa2</mo><mi id=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.3.3\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.3.3.cmml\">x</mi></mrow><mo id=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.1a\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.1.cmml\">\\xe2\\x88\\x92</mo><mn id=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.4\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.4.cmml\">14</mn></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S1.T1.1.1.1.1.1.1.m1.1b\"><apply id=\"S1.T1.1.1.1.1.1.1.m1.1.2.cmml\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2\"><eq id=\"S1.T1.1.1.1.1.1.1.m1.1.2.1.cmml\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.1\"></eq><apply id=\"S1.T1.1.1.1.1.1.1.m1.1.2.2.cmml\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.2\"><times id=\"S1.T1.1.1.1.1.1.1.m1.1.2.2.1.cmml\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.2.1\"></times><ci id=\"S1.T1.1.1.1.1.1.1.m1.1.2.2.2.cmml\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.2.2\">\\xf0\\x9d\\x91\\x93</ci><ci id=\"S1.T1.1.1.1.1.1.1.m1.1.1.cmml\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.1\">\\xf0\\x9d\\x91\\xa5</ci></apply><apply id=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.cmml\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.3\"><minus id=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.1.cmml\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.1\"></minus><apply id=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.2.cmml\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.2\"><times id=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.2.1.cmml\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.2.1\"></times><cn id=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.2.2.cmml\" type=\"integer\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.2.2\">4</cn><apply id=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.2.3.cmml\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.2.3\"><csymbol cd=\"ambiguous\" id=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.2.3.1.cmml\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.2.3\">superscript</csymbol><ci id=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.2.3.2.cmml\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.2.3.2\">\\xf0\\x9d\\x91\\xa5</ci><cn id=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.2.3.3.cmml\" type=\"integer\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.2.3.3\">3</cn></apply></apply><apply id=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.3.cmml\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.3\"><times id=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.3.1.cmml\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.3.1\"></times><cn id=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.3.2.cmml\" type=\"integer\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.3.2\">9</cn><ci id=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.3.3.cmml\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.3.3\">\\xf0\\x9d\\x91\\xa5</ci></apply><cn id=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.4.cmml\" type=\"integer\" xref=\"S1.T1.1.1.1.1.1.1.m1.1.2.3.4\">14</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S1.T1.1.1.1.1.1.1.m1.1c\">f(x)=4x^{3}-9x-14</annotation><annotation encoding=\"application/x-llamapun\" id=\"S1.T1.1.1.1.1.1.1.m1.1d\">italic_f ( italic_x ) = 4 italic_x start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT - 9 italic_x - 14</annotation></semantics></math>, find the value of <math alttext=\"f(2)\" class=\"ltx_Math\" display=\"inline\" id=\"S1.T1.2.2.2.2.2.2.m2.1\"><semantics id=\"S1.T1.2.2.2.2.2.2.m2.1a\"><mrow id=\"S1.T1.2.2.2.2.2.2.m2.1.2\" xref=\"S1.T1.2.2.2.2.2.2.m2.1.2.cmml\"><mi id=\"S1.T1.2.2.2.2.2.2.m2.1.2.2\" xref=\"S1.T1.2.2.2.2.2.2.m2.1.2.2.cmml\">f</mi><mo id=\"S1.T1.2.2.2.2.2.2.m2.1.2.1\" xref=\"S1.T1.2.2.2.2.2.2.m2.1.2.1.cmml\">\\xe2\\x81\\xa2</mo><mrow id=\"S1.T1.2.2.2.2.2.2.m2.1.2.3.2\" xref=\"S1.T1.2.2.2.2.2.2.m2.1.2.cmml\"><mo id=\"S1.T1.2.2.2.2.2.2.m2.1.2.3.2.1\" stretchy=\"false\" xref=\"S1.T1.2.2.2.2.2.2.m2.1.2.cmml\">(</mo><mn id=\"S1.T1.2.2.2.2.2.2.m2.1.1\" xref=\"S1.T1.2.2.2.2.2.2.m2.1.1.cmml\">2</mn><mo id=\"S1.T1.2.2.2.2.2.2.m2.1.2.3.2.2\" stretchy=\"false\" xref=\"S1.T1.2.2.2.2.2.2.m2.1.2.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S1.T1.2.2.2.2.2.2.m2.1b\"><apply id=\"S1.T1.2.2.2.2.2.2.m2.1.2.cmml\" xref=\"S1.T1.2.2.2.2.2.2.m2.1.2\"><times id=\"S1.T1.2.2.2.2.2.2.m2.1.2.1.cmml\" xref=\"S1.T1.2.2.2.2.2.2.m2.1.2.1\"></times><ci id=\"S1.T1.2.2.2.2.2.2.m2.1.2.2.cmml\" xref=\"S1.T1.2.2.2.2.2.2.m2.1.2.2\">\\xf0\\x9d\\x91\\x93</ci><cn id=\"S1.T1.2.2.2.2.2.2.m2.1.1.cmml\" type=\"integer\" xref=\"S1.T1.2.2.2.2.2.2.m2.1.1\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S1.T1.2.2.2.2.2.2.m2.1c\">f(2)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S1.T1.2.2.2.2.2.2.m2.1d\">italic_f ( 2 )</annotation></semantics></math>.</p>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S1.T1.4.4.4\">\\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S1.T1.4.4.4.3\" style=\"width:34.7pt;\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S1.T1.4.4.4.3.1\">2nd Turn</p>\\n</td>\\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S1.T1.4.4.4.2\" style=\"width:325.2pt;\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S1.T1.4.4.4.2.2.2\">Find <math alttext=\"x\" class=\"ltx_Math\" display=\"inline\" id=\"S1.T1.3.3.3.1.1.1.m1.1\"><semantics id=\"S1.T1.3.3.3.1.1.1.m1.1a\"><mi id=\"S1.T1.3.3.3.1.1.1.m1.1.1\" xref=\"S1.T1.3.3.3.1.1.1.m1.1.1.cmml\">x</mi><annotation-xml encoding=\"MathML-Content\" id=\"S1.T1.3.3.3.1.1.1.m1.1b\"><ci id=\"S1.T1.3.3.3.1.1.1.m1.1.1.cmml\" xref=\"S1.T1.3.3.3.1.1.1.m1.1.1\">\\xf0\\x9d\\x91\\xa5</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S1.T1.3.3.3.1.1.1.m1.1c\">x</annotation><annotation encoding=\"application/x-llamapun\" id=\"S1.T1.3.3.3.1.1.1.m1.1d\">italic_x</annotation></semantics></math> such that <math alttext=\"f(x)=0\" class=\"ltx_Math\" display=\"inline\" id=\"S1.T1.4.4.4.2.2.2.m2.1\"><semantics id=\"S1.T1.4.4.4.2.2.2.m2.1a\"><mrow id=\"S1.T1.4.4.4.2.2.2.m2.1.2\" xref=\"S1.T1.4.4.4.2.2.2.m2.1.2.cmml\"><mrow id=\"S1.T1.4.4.4.2.2.2.m2.1.2.2\" xref=\"S1.T1.4.4.4.2.2.2.m2.1.2.2.cmml\"><mi id=\"S1.T1.4.4.4.2.2.2.m2.1.2.2.2\" xref=\"S1.T1.4.4.4.2.2.2.m2.1.2.2.2.cmml\">f</mi><mo id=\"S1.T1.4.4.4.2.2.2.m2.1.2.2.1\" xref=\"S1.T1.4.4.4.2.2.2.m2.1.2.2.1.cmml\">\\xe2\\x81\\xa2</mo><mrow id=\"S1.T1.4.4.4.2.2.2.m2.1.2.2.3.2\" xref=\"S1.T1.4.4.4.2.2.2.m2.1.2.2.cmml\"><mo id=\"S1.T1.4.4.4.2.2.2.m2.1.2.2.3.2.1\" stretchy=\"false\" xref=\"S1.T1.4.4.4.2.2.2.m2.1.2.2.cmml\">(</mo><mi id=\"S1.T1.4.4.4.2.2.2.m2.1.1\" xref=\"S1.T1.4.4.4.2.2.2.m2.1.1.cmml\">x</mi><mo id=\"S1.T1.4.4.4.2.2.2.m2.1.2.2.3.2.2\" stretchy=\"false\" xref=\"S1.T1.4.4.4.2.2.2.m2.1.2.2.cmml\">)</mo></mrow></mrow><mo id=\"S1.T1.4.4.4.2.2.2.m2.1.2.1\" xref=\"S1.T1.4.4.4.2.2.2.m2.1.2.1.cmml\">=</mo><mn id=\"S1.T1.4.4.4.2.2.2.m2.1.2.3\" xref=\"S1.T1.4.4.4.2.2.2.m2.1.2.3.cmml\">0</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S1.T1.4.4.4.2.2.2.m2.1b\"><apply id=\"S1.T1.4.4.4.2.2.2.m2.1.2.cmml\" xref=\"S1.T1.4.4.4.2.2.2.m2.1.2\"><eq id=\"S1.T1.4.4.4.2.2.2.m2.1.2.1.cmml\" xref=\"S1.T1.4.4.4.2.2.2.m2.1.2.1\"></eq><apply id=\"S1.T1.4.4.4.2.2.2.m2.1.2.2.cmml\" xref=\"S1.T1.4.4.4.2.2.2.m2.1.2.2\"><times id=\"S1.T1.4.4.4.2.2.2.m2.1.2.2.1.cmml\" xref=\"S1.T1.4.4.4.2.2.2.m2.1.2.2.1\"></times><ci id=\"S1.T1.4.4.4.2.2.2.m2.1.2.2.2.cmml\" xref=\"S1.T1.4.4.4.2.2.2.m2.1.2.2.2\">\\xf0\\x9d\\x91\\x93</ci><ci id=\"S1.T1.4.4.4.2.2.2.m2.1.1.cmml\" xref=\"S1.T1.4.4.4.2.2.2.m2.1.1\">\\xf0\\x9d\\x91\\xa5</ci></apply><cn id=\"S1.T1.4.4.4.2.2.2.m2.1.2.3.cmml\" type=\"integer\" xref=\"S1.T1.4.4.4.2.2.2.m2.1.2.3\">0</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S1.T1.4.4.4.2.2.2.m2.1c\">f(x)=0</annotation><annotation encoding=\"application/x-llamapun\" id=\"S1.T1.4.4.4.2.2.2.m2.1d\">italic_f ( italic_x ) = 0</annotation></semantics></math>.</p>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S1.T1.4.4.8.3\">\\n<th class=\"ltx_td ltx_align_justify ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" id=\"S1.T1.4.4.8.3.1\" rowspan=\"2\" style=\"width:43.4pt;\"><span class=\"ltx_text ltx_align_top\" id=\"S1.T1.4.4.8.3.1.1\">Knowledge</span></th>\\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S1.T1.4.4.8.3.2\" style=\"width:34.7pt;\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S1.T1.4.4.8.3.2.1\">1st Turn</p>\\n</td>\\n<td class=\"ltx_td ltx_align_justify ltx_border_t\" id=\"S1.T1.4.4.8.3.3\" style=\"width:325.2pt;\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S1.T1.4.4.8.3.3.1\">Provide insights into the correlation between economic indicators such as GDP, inflation, and unemployment rates. Explain how fiscal and monetary policies \\xe2\\x80\\xa6</p>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S1.T1.4.4.9.4\">\\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_t\" id=\"S1.T1.4.4.9.4.1\" style=\"width:34.7pt;\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S1.T1.4.4.9.4.1.1\">2nd Turn</p>\\n</td>\\n<td class=\"ltx_td ltx_align_justify ltx_border_bb ltx_border_t\" id=\"S1.T1.4.4.9.4.2\" style=\"width:325.2pt;\">\\n<p class=\"ltx_p ltx_align_top\" id=\"S1.T1.4.4.9.4.2.1\">Now, explain them again like I\\xe2\\x80\\x99m five.</p>\\n</td>\\n</tr>\\n</tbody>\\n</table>\\n</span></div>\\n</figure>\\n</section>\\n<section class=\"ltx_section\" id=\"S2\">\\n<h2 class=\"ltx_title ltx_title_section\">\\n<span class=\"ltx_tag ltx_tag_section\">2 </span>MT-Bench and Chatbot Arena</h2>\\n<section class=\"ltx_subsection\" id=\"S2.SS1\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\">2.1 </span>Motivation</h3>\\n<div class=\"ltx_para ltx_noindent\" id=\"S2.SS1.p1\">\\n<p class=\"ltx_p\" id=\"S2.SS1.p1.1\">With the recent advances of LLMs, LLM-based assistants start to exhibit artificial general intelligence across diverse tasks, from writing and chatting to coding\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib5\" title=\"\">5</a>, <a class=\"ltx_ref\" href=\"#bib.bib30\" title=\"\">30</a>, <a class=\"ltx_ref\" href=\"#bib.bib1\" title=\"\">1</a>, <a class=\"ltx_ref\" href=\"#bib.bib37\" title=\"\">37</a>]</cite>. However, evaluating their broad capabilities also becomes more challenging.\\nDespite the availability of numerous benchmarks for language models, they primarily focus on evaluating models on closed-ended questions with short responses.\\nGiven that these chat assistants can now precisely follow user instructions in multi-turn dialogues and answer open-ended questions in a zero-shot manner, current benchmarks are inadequate for assessing such capabilities. Existing benchmarks mostly fall into the following three categories.</p>\\n<ul class=\"ltx_itemize\" id=\"S2.I1\">\\n<li class=\"ltx_item\" id=\"S2.I1.i1\" style=\"list-style-type:none;\">\\n<span class=\"ltx_tag ltx_tag_item\">\\xe2\\x80\\xa2</span>\\n<div class=\"ltx_para\" id=\"S2.I1.i1.p1\">\\n<p class=\"ltx_p\" id=\"S2.I1.i1.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.I1.i1.p1.1.1\">Core-knowledge benchmarks</span>, including MMLU\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib19\" title=\"\">19</a>]</cite>, HellaSwag\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib50\" title=\"\">50</a>]</cite>, ARC\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib9\" title=\"\">9</a>]</cite>, WinoGrande\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib36\" title=\"\">36</a>]</cite>, HumanEval\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib6\" title=\"\">6</a>]</cite>, GSM-8K\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib10\" title=\"\">10</a>]</cite>, and AGIEval\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib51\" title=\"\">51</a>]</cite>, evaluate the core capabilities of pre-trained LLMs using zero-shot and few-shot benchmark sets. They typically require LLMs to generate a short, specific answer to benchmark questions that can be automatically validated.</p>\\n</div>\\n</li>\\n<li class=\"ltx_item\" id=\"S2.I1.i2\" style=\"list-style-type:none;\">\\n<span class=\"ltx_tag ltx_tag_item\">\\xe2\\x80\\xa2</span>\\n<div class=\"ltx_para\" id=\"S2.I1.i2.p1\">\\n<p class=\"ltx_p\" id=\"S2.I1.i2.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.I1.i2.p1.1.1\">Instruction-following benchmarks</span>, such as Flan\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib27\" title=\"\">27</a>, <a class=\"ltx_ref\" href=\"#bib.bib46\" title=\"\">46</a>]</cite>, Self-instruct\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib44\" title=\"\">44</a>]</cite>, NaturalInstructions\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib28\" title=\"\">28</a>]</cite>, Super-NaturalInstructions\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib45\" title=\"\">45</a>]</cite>, expand to slightly more open-ended questions and more diverse tasks and are used to evaluate LLMs after instruction fine-tuning.</p>\\n</div>\\n</li>\\n<li class=\"ltx_item\" id=\"S2.I1.i3\" style=\"list-style-type:none;\">\\n<span class=\"ltx_tag ltx_tag_item\">\\xe2\\x80\\xa2</span>\\n<div class=\"ltx_para ltx_noindent\" id=\"S2.I1.i3.p1\">\\n<p class=\"ltx_p\" id=\"S2.I1.i3.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.I1.i3.p1.1.1\">Conversational benchmarks</span>, like CoQA\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib35\" title=\"\">35</a>]</cite>, MMDialog\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib15\" title=\"\">15</a>]</cite> and OpenAssistant\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib23\" title=\"\">23</a>]</cite>, are closest to our intended use cases. However, the diversity and complexity of their questions often fall short in challenging the capabilities of the latest chatbots.</p>\\n</div>\\n</li>\\n</ul>\\n</div>\\n<div class=\"ltx_para ltx_noindent\" id=\"S2.SS1.p2\">\\n<p class=\"ltx_p\" id=\"S2.SS1.p2.1\">While largely overlooked by existing LLM benchmarks, human preferences serve as a direct measure of a chatbot\\xe2\\x80\\x99s utility in open-ended, multi-turn human-AI interactions. To bridge this gap, we introduce two novel benchmarks expressly tailored to assess human preferences. Simultaneously, these benchmarks are designed to distinguish the core capabilities of state-of-the-art models.</p>\\n</div>\\n</section>\\n<section class=\"ltx_subsection\" id=\"S2.SS2\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\">2.2 </span>MT-Bench</h3>\\n<div class=\"ltx_para ltx_noindent\" id=\"S2.SS2.p1\">\\n<p class=\"ltx_p\" id=\"S2.SS2.p1.1\">We create MT-bench, a benchmark consisting of 80 high-quality multi-turn questions.\\nMT-bench is designed to test multi-turn conversation and instruction-following ability, covering common use cases and focusing on challenging questions to differentiate models.\\nWe identify 8 common categories of user prompts to guide its construction: writing, roleplay, extraction, reasoning, math, coding, knowledge I (STEM), and knowledge II (humanities/social science).\\nFor each category, we then manually designed 10 multi-turn questions.\\n<a class=\"ltx_ref\" href=\"#S1.T1\" title=\"Table 1 \\xe2\\x80\\xa3 1 Introduction \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Table</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">1</span></a> lists several sample questions.</p>\\n</div>\\n</section>\\n<section class=\"ltx_subsection\" id=\"S2.SS3\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\">2.3 </span>Chatbot Arena</h3>\\n<div class=\"ltx_para ltx_noindent\" id=\"S2.SS3.p1\">\\n<p class=\"ltx_p\" id=\"S2.SS3.p1.1\">Our second approach is Chatbot Arena, a crowdsourcing benchmark platform featuring anonymous battles.\\nOn this platform, users can interact with two anonymous models simultaneously, posing the same question to both. They vote for which model provides the preferred response, with the identities of the models disclosed post-voting. After running Chatbot Arena for one month, we have collected around 30K votes. Since the platform does not use pre-defined questions, it allows gathering a wide range of unrestricted use cases and votes in the wild, based on the diverse interests of users. A screenshot of the platform can be found at <a class=\"ltx_ref\" href=\"#A3.SS2\" title=\"C.2 Chatbot Arena \\xe2\\x80\\xa3 Appendix C Data Collection \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Section</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">C.2</span></a>.\\n</p>\\n</div>\\n</section>\\n</section>\\n<section class=\"ltx_section\" id=\"S3\">\\n<h2 class=\"ltx_title ltx_title_section\">\\n<span class=\"ltx_tag ltx_tag_section\">3 </span>LLM as a Judge</h2>\\n<div class=\"ltx_para ltx_noindent\" id=\"S3.p1\">\\n<p class=\"ltx_p\" id=\"S3.p1.1\">While our initial evaluations using MT-bench and Chatbot Arena rely on human ratings, collecting human preferences can be costly and laborious\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib44\" title=\"\">44</a>, <a class=\"ltx_ref\" href=\"#bib.bib38\" title=\"\">38</a>, <a class=\"ltx_ref\" href=\"#bib.bib31\" title=\"\">31</a>, <a class=\"ltx_ref\" href=\"#bib.bib2\" title=\"\">2</a>, <a class=\"ltx_ref\" href=\"#bib.bib13\" title=\"\">13</a>]</cite>. To overcome this, we aim to develop a more scalable and automated approach. Given that most questions in MT-bench and Chatbot Arena are open-ended without reference answers, devising a rule-based program to assess the outputs is extremely challenging.\\nTraditional evaluation metrics based on the similarity between outputs and reference answers (e.g., ROUGE\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib25\" title=\"\">25</a>]</cite>, BLEU\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib32\" title=\"\">32</a>]</cite>) are also ineffective for these questions.</p>\\n</div>\\n<div class=\"ltx_para ltx_noindent\" id=\"S3.p2\">\\n<p class=\"ltx_p\" id=\"S3.p2.1\">As LLMs continue to improve, they show potential in replacing human annotators in many tasks\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib17\" title=\"\">17</a>, <a class=\"ltx_ref\" href=\"#bib.bib20\" title=\"\">20</a>]</cite>. Specifically, we are interested in whether LLMs can effectively evaluate the responses of chat assistants and match human preferences. Next, we discuss the use and limitations of LLM-as-a-judge.</p>\\n</div>\\n<section class=\"ltx_subsection\" id=\"S3.SS1\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\">3.1 </span>Types of LLM-as-a-Judge</h3>\\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS1.p1\">\\n<p class=\"ltx_p\" id=\"S3.SS1.p1.1\">We propose 3 LLM-as-a-judge variations. They can be implemented independently or in combination:</p>\\n<ul class=\"ltx_itemize\" id=\"S3.I1\">\\n<li class=\"ltx_item\" id=\"S3.I1.i1\" style=\"list-style-type:none;\">\\n<span class=\"ltx_tag ltx_tag_item\">\\xe2\\x80\\xa2</span>\\n<div class=\"ltx_para\" id=\"S3.I1.i1.p1\">\\n<p class=\"ltx_p\" id=\"S3.I1.i1.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.I1.i1.p1.1.1\">Pairwise comparison</span>. An LLM judge is presented with a question and two answers, and tasked to determine which one is better or declare a tie. The prompt used is given in <a class=\"ltx_ref\" href=\"#A1.F4\" title=\"Figure 4 \\xe2\\x80\\xa3 Appendix A Prompt templates \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">4</span></a> (Appendix).</p>\\n</div>\\n</li>\\n<li class=\"ltx_item\" id=\"S3.I1.i2\" style=\"list-style-type:none;\">\\n<span class=\"ltx_tag ltx_tag_item\">\\xe2\\x80\\xa2</span>\\n<div class=\"ltx_para\" id=\"S3.I1.i2.p1\">\\n<p class=\"ltx_p\" id=\"S3.I1.i2.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.I1.i2.p1.1.1\">Single answer grading</span>.\\nAlternatively, an LLM judge is asked to directly assign a score to a single answer. The prompt used for this scenario is in <a class=\"ltx_ref\" href=\"#A1.F5\" title=\"Figure 5 \\xe2\\x80\\xa3 Appendix A Prompt templates \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">5</span></a> (Appendix).</p>\\n</div>\\n</li>\\n<li class=\"ltx_item\" id=\"S3.I1.i3\" style=\"list-style-type:none;\">\\n<span class=\"ltx_tag ltx_tag_item\">\\xe2\\x80\\xa2</span>\\n<div class=\"ltx_para ltx_noindent\" id=\"S3.I1.i3.p1\">\\n<p class=\"ltx_p\" id=\"S3.I1.i3.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.I1.i3.p1.1.1\">Reference-guided grading</span>.\\nIn certain cases, it may be beneficial to provide a reference solution if applicable. An example prompt we use for grading math problems is in <a class=\"ltx_ref\" href=\"#A1.F7\" title=\"Figure 7 \\xe2\\x80\\xa3 Appendix A Prompt templates \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">7</span></a> (Appendix).</p>\\n</div>\\n</li>\\n</ul>\\n</div>\\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS1.p2\">\\n<p class=\"ltx_p\" id=\"S3.SS1.p2.1\">These methods have different pros and cons. For example, the pairwise comparison may lack scalability when the number of players increases, given that the number of possible pairs grows quadratically; single answer grading may be unable to discern subtle differences between specific pairs, and its results may become unstable, as absolute scores are likely to fluctuate more than relative pairwise results if the judge model changes.</p>\\n</div>\\n</section>\\n<section class=\"ltx_subsection\" id=\"S3.SS2\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\">3.2 </span>Advantages of LLM-as-a-Judge</h3>\\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS2.p1\">\\n<p class=\"ltx_p\" id=\"S3.SS2.p1.1\">LLM-as-a-judge offers two key benefits: <span class=\"ltx_text ltx_font_italic\" id=\"S3.SS2.p1.1.1\">scalability</span> and <span class=\"ltx_text ltx_font_italic\" id=\"S3.SS2.p1.1.2\">explainability</span>. It reduces the need for human involvement, enabling scalable benchmarks and fast iterations. Additionally, LLM judges provide not only scores but also explanations, making their outputs interpretable, as shown in <a class=\"ltx_ref\" href=\"#S1.F1\" title=\"Figure 1 \\xe2\\x80\\xa3 1 Introduction \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">1</span></a>.</p>\\n</div>\\n</section>\\n<section class=\"ltx_subsection\" id=\"S3.SS3\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\">3.3 </span>Limitations of LLM-as-a-Judge</h3>\\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS3.p1\">\\n<p class=\"ltx_p\" id=\"S3.SS3.p1.1\">We identify certain biases and limitations of LLM judges. However, we will also present solutions later and show the agreement between LLM judges and humans is high despite these limitations.</p>\\n</div>\\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS3.p2\">\\n<p class=\"ltx_p\" id=\"S3.SS3.p2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.SS3.p2.1.1\">Position bias</span> is when an LLM exhibits a propensity to favor certain positions over others. This bias is not unique to our context and has been seen in human decision-making\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"#bib.bib34\" title=\"\">34</a>]</cite> and other ML domains\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib22\" title=\"\">22</a>, <a class=\"ltx_ref\" href=\"#bib.bib41\" title=\"\">41</a>]</cite>.</p>\\n</div>\\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS3.p3\">\\n<p class=\"ltx_p\" id=\"S3.SS3.p3.1\"><a class=\"ltx_ref\" href=\"#A2.F10\" title=\"Figure 10 \\xe2\\x80\\xa3 Appendix B Case Study \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">10</span></a> (Appendix) shows an example of position bias.\\nGPT-4 is tasked to evaluate two responses from GPT-3.5 and Vicuna-13B to an open-ended question. When GPT-3.5\\xe2\\x80\\x99s answer is positioned first, GPT-4 considers GPT-3.5\\xe2\\x80\\x99s answer more detailed and superior. However, upon switching the positions of the two responses, GPT-4\\xe2\\x80\\x99s judgement flips, favoring Vicuna\\xe2\\x80\\x99s answer.</p>\\n</div>\\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS3.p4\">\\n<p class=\"ltx_p\" id=\"S3.SS3.p4.1\">To analyze the position bias, we construct two similar answers to each first-turn question in MT-bench by calling GPT-3.5 twice with a temperature of 0.7.\\nWe then try three LLMs with two different prompts:\\n\\xe2\\x80\\x9cdefault\\xe2\\x80\\x9d is our default prompt in <a class=\"ltx_ref\" href=\"#A1.F4\" title=\"Figure 4 \\xe2\\x80\\xa3 Appendix A Prompt templates \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">4</span></a> (Appendix).\\n\\xe2\\x80\\x9crename\\xe2\\x80\\x9d renames the assistants in our default prompt to see whether the bias is on positions or names.\\nAs in <a class=\"ltx_ref\" href=\"#S3.T2\" title=\"Table 2 \\xe2\\x80\\xa3 3.3 Limitations of LLM-as-a-Judge \\xe2\\x80\\xa3 3 LLM as a Judge \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Table</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">2</span></a>, we found all of them exhibit strong position bias. Most LLM judges favor the first position. Claude-v1 also shows a name bias which makes it favors \"Assistant A\", as illustrated by the \"rename\" prompt. The position bias can be very significant. Only GPT-4 outputs consistent results in more than 60% of cases.</p>\\n</div>\\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS3.p5\">\\n<p class=\"ltx_p\" id=\"S3.SS3.p5.1\">Note that this test is challenging because the answers are very similar and occasionally indistinguishable even to humans. We will show that position bias is less prominent in some cases in <a class=\"ltx_ref\" href=\"#A4.SS1\" title=\"D.1 Position bias \\xe2\\x80\\xa3 Appendix D Additional Experimental Results \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Section</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">D.1</span></a>.\\nAs for the origin of this bias, we suspect that it could be rooted in the training data or inherent to the left-to-right architecture of causal transformers, but leave a deeper study as future work.</p>\\n</div>\\n<figure class=\"ltx_table\" id=\"S3.T2\">\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 2: </span>Position bias of different LLM judges. Consistency is the percentage of cases where a judge gives consistent results when swapping the order of two assistants. \\xe2\\x80\\x9cBiased toward first\\xe2\\x80\\x9d is the percentage of cases when a judge favors the first answer. \\xe2\\x80\\x9cError\\xe2\\x80\\x9d indicates wrong output formats. The two largest numbers in each column are in bold.</figcaption>\\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S3.T2.1\">\\n<tbody class=\"ltx_tbody\">\\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.1\">\\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S3.T2.1.1.1.1\">Judge</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S3.T2.1.1.1.2\">Prompt</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S3.T2.1.1.1.3\">Consistency</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S3.T2.1.1.1.4\">Biased toward first</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S3.T2.1.1.1.5\">Biased toward second</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S3.T2.1.1.1.6\">Error</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S3.T2.1.2.2\">\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.2.2.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S3.T2.1.2.2.1.1\">Claude-v1</span></td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.2.2.2\">default</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.2.2.3\">23.8%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.2.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.2.2.4.1\">75.0%</span></td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.2.2.5\">0.0%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.2.2.6\">1.2%</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S3.T2.1.3.3\">\\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.3.3.1\">rename</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.3.3.2\">56.2%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.3.3.3\">11.2%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.3.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.3.3.4.1\">28.7%</span></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.3.3.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.3.3.5.1\">3.8%</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S3.T2.1.4.4\">\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.4.4.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S3.T2.1.4.4.1.1\">GPT-3.5</span></td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.4.4.2\">default</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.4.4.3\">46.2%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.4.4.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.4.4.4.1\">50.0%</span></td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.4.4.5\">1.2%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.4.4.6\">2.5%</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S3.T2.1.5.5\">\\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.5.5.1\">rename</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.5.5.2\">51.2%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.5.5.3\">38.8%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.5.5.4\">6.2%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T2.1.5.5.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.5.5.5.1\">3.8%</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S3.T2.1.6.6\">\\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S3.T2.1.6.6.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"S3.T2.1.6.6.1.1\">GPT-4</span></td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.6.6.2\">default</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.6.6.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.6.6.3.1\">65.0%</span></td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.6.6.4\">30.0%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.6.6.5\">5.0%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.6.6.6\">0.0%</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S3.T2.1.7.7\">\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T2.1.7.7.1\">rename</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T2.1.7.7.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.7.7.2.1\">66.2%</span></td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T2.1.7.7.3\">28.7%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T2.1.7.7.4\">5.0%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S3.T2.1.7.7.5\">0.0%</td>\\n</tr>\\n</tbody>\\n</table>\\n</figure>\\n<figure class=\"ltx_table\" id=\"S3.T4\">\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 3: </span>Failure rate under \\xe2\\x80\\x9crepetitive list\\xe2\\x80\\x9d attack for different LLM judges on 23 answers.</figcaption>\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>Judge failure rate on 10 math questions with different prompts. We test LLaMA-13B vs. Vicuna-13B and swap positions. A failure means when GPT-4 says an incorrect answer is correct.</figcaption><div class=\"ltx_flex_figure ltx_flex_table\">\\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\\n<table class=\"ltx_tabular ltx_centering ltx_flex_size_2 ltx_align_middle\" id=\"S3.T4.1\">\\n<tbody class=\"ltx_tbody\">\\n<tr class=\"ltx_tr\" id=\"S3.T4.1.1.1\">\\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S3.T4.1.1.1.1\">Judge</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S3.T4.1.1.1.2\">Claude-v1</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S3.T4.1.1.1.3\">GPT-3.5</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S3.T4.1.1.1.4\">GPT-4</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S3.T4.1.2.2\">\\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S3.T4.1.2.2.1\">Failure rate</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S3.T4.1.2.2.2\">91.3%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S3.T4.1.2.2.3\">91.3%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S3.T4.1.2.2.4\">8.7%</td>\\n</tr>\\n</tbody>\\n</table>\\n</div>\\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\\n<table class=\"ltx_tabular ltx_centering ltx_flex_size_2 ltx_align_middle\" id=\"S3.T4.2\">\\n<tbody class=\"ltx_tbody\">\\n<tr class=\"ltx_tr\" id=\"S3.T4.2.1.1\">\\n<td class=\"ltx_td ltx_border_tt\" id=\"S3.T4.2.1.1.1\"></td>\\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S3.T4.2.1.1.2\">Default</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S3.T4.2.1.1.3\">CoT</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S3.T4.2.1.1.4\">Reference</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S3.T4.2.2.2\">\\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S3.T4.2.2.2.1\">Failure rate</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S3.T4.2.2.2.2\">14/20</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S3.T4.2.2.2.3\">6/20</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S3.T4.2.2.2.4\">3/20</td>\\n</tr>\\n</tbody>\\n</table>\\n</div>\\n</div>\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 4: </span>Judge failure rate on 10 math questions with different prompts. We test LLaMA-13B vs. Vicuna-13B and swap positions. A failure means when GPT-4 says an incorrect answer is correct.</figcaption>\\n</figure>\\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS3.p6\">\\n<p class=\"ltx_p\" id=\"S3.SS3.p6.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.SS3.p6.1.1\">Verbosity bias</span> is when an LLM judge favors longer, verbose responses, even if they are not as clear, high-quality, or accurate as shorter alternatives.</p>\\n</div>\\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS3.p7\">\\n<p class=\"ltx_p\" id=\"S3.SS3.p7.1\">To examine this bias, we design a \\xe2\\x80\\x9crepetitive list\\xe2\\x80\\x9d attack with model answers from MT-bench.\\nWe first select 23 model answers from MT-bench that contain a numbered list.\\nWe then make them unnecessarily verbose by asking GPT-4 to rephrase the list without adding any new information and insert the rephrased new list to the beginning of the original list.\\nFor example, if the original response contains 5 items, then the new response will contain 10 items but the first 5 items are rephrased from the original 5 items. An example is shown in <a class=\"ltx_ref\" href=\"#A2.F11\" title=\"Figure 11 \\xe2\\x80\\xa3 Appendix B Case Study \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">11</span></a> (Appendix). We define the attack is successful if an LLM judge thinks the new response is better than the old response. <a class=\"ltx_ref\" href=\"#S3.T4\" title=\"Table 4 \\xe2\\x80\\xa3 3.3 Limitations of LLM-as-a-Judge \\xe2\\x80\\xa3 3 LLM as a Judge \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Table</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">4</span></a> shows the failure rate of LLM judges under this attack, demonstrating that all LLMs may be prone to verbosity bias though GPT-4 defends significantly better than others. As a calibration, we find LLM judges are able to correctly judge identical answers (i.e., they always return a tie for two identical answers) but cannot pass the more advanced \\xe2\\x80\\x9crepetitive list\\xe2\\x80\\x9d attack.</p>\\n</div>\\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS3.p8\">\\n<p class=\"ltx_p\" id=\"S3.SS3.p8.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.SS3.p8.1.1\">Self-enhancement bias.</span>\\nWe adopt the term \\xe2\\x80\\x9cself-enhancement bias\\xe2\\x80\\x9d from social cognition literature\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib4\" title=\"\">4</a>]</cite> to describe the effect that LLM judges may favor the answers generated by themselves.</p>\\n</div>\\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS3.p9\">\\n<p class=\"ltx_p\" id=\"S3.SS3.p9.1\">We examine this effect statistically. <a class=\"ltx_ref\" href=\"#S4.F2\" title=\"Figure 2 \\xe2\\x80\\xa3 4.2 High agreement between GPT-4 and humans \\xe2\\x80\\xa3 4 Agreement Evaluation \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">2</span></a>(b) shows the win rate (w/o tie) of six models under different LLM judges and humans.\\nCompared to humans, we do observe that some judges favor certain models. For example, GPT-4 favors itself with a 10% higher win rate; Claude-v1 favors itself with a 25% higher win rate. However, they also favor other models and GPT-3.5 does not favor itself.\\nDue to limited data and small differences, our study cannot determine whether the models exhibit a self-enhancement bias.\\nConducting a controlled study is challenging because we cannot easily rephrase a response to fit the style of another model without changing the quality.</p>\\n</div>\\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS3.p10\">\\n<p class=\"ltx_p\" id=\"S3.SS3.p10.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.SS3.p10.1.1\">Limited capability in grading math and reasoning questions.</span>\\nLLMs are known to have limited math and reasoning capability\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib10\" title=\"\">10</a>]</cite>, which results in its failure of grading such questions because they do not know the correct answers.\\nHowever, what is more intriguing is that it also shows limitations in grading basic math problems which it is capable of solving.\\nFor instance, in Figure\\xc2\\xa0<a class=\"ltx_ref\" href=\"#A2.F12\" title=\"Figure 12 \\xe2\\x80\\xa3 Appendix B Case Study \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">12</span></a> (Appendix), we present an example of an elementary math question in which GPT-4 makes an incorrect judgment.\\nIt\\xe2\\x80\\x99s worth noting that although GPT-4 can solve the problem (when asked separately), it was misled by the provided answers, ultimately resulting in incorrect judgment.\\nThis pattern can also be seen in a reasoning question example in Figure\\xc2\\xa0<a class=\"ltx_ref\" href=\"#A2.F13\" title=\"Figure 13 \\xe2\\x80\\xa3 Appendix B Case Study \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">13</span></a> (Appendix).\\nBoth GPT-3.5 and Claude-v1 show a similar weakness.\\nIn <a class=\"ltx_ref\" href=\"#S3.SS4\" title=\"3.4 Addressing limitations \\xe2\\x80\\xa3 3 LLM as a Judge \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Section</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>, we will introduce a reference-guided method to mitigate such issues.\\n</p>\\n</div>\\n</section>\\n<section class=\"ltx_subsection\" id=\"S3.SS4\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\">3.4 </span>Addressing limitations</h3>\\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS4.p1\">\\n<p class=\"ltx_p\" id=\"S3.SS4.p1.1\">We present a few methods to address position bias and the limited grading ability for math questions.</p>\\n</div>\\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS4.p2\">\\n<p class=\"ltx_p\" id=\"S3.SS4.p2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.SS4.p2.1.1\">Swapping positions.</span>\\nThe position bias can be addressed by simple solutions.\\nA conservative approach is to call a judge twice by swapping the order of two answers and only declare a win when an answer is preferred in both orders. If the results are inconsistent after swapping, we can call it a tie.\\nAnother more aggressive approach is to assign positions randomly, which can be effective at a large scale with the correct expectations. In the following experiments, we use the conservative one.</p>\\n</div>\\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS4.p3\">\\n<p class=\"ltx_p\" id=\"S3.SS4.p3.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.SS4.p3.1.1\">Few-shot judge.</span>\\nWe assess whether few-shot examples can improve consistency in the position bias benchmark. We select three good judgment examples using MT-bench-like questions, GPT-3.5 and Vicuna for generating answers, and GPT-4 for generating judgments.\\nThe examples cover three cases: A is better, B is better, and tie.\\nAs shown in <a class=\"ltx_ref\" href=\"#A4.T13\" title=\"Table 13 \\xe2\\x80\\xa3 D.2 Few-shot judge \\xe2\\x80\\xa3 Appendix D Additional Experimental Results \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Table</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">13</span></a> (Appendix), the few-shot judge can significantly increase the consistency of GPT-4 from 65.0% to 77.5%.\\nHowever, high consistency may not imply high accuracy and we are not sure whether the few-shot examples will introduce new biases. Besides, the longer prompts make API calls <math alttext=\"4\\\\times\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S3.SS4.p3.1.m1.1\"><semantics id=\"S3.SS4.p3.1.m1.1a\"><mrow id=\"S3.SS4.p3.1.m1.1b\"><mn id=\"S3.SS4.p3.1.m1.1.1\">4</mn><mo id=\"S3.SS4.p3.1.m1.1.2\" lspace=\"0.222em\">\\xc3\\x97</mo></mrow><annotation encoding=\"application/x-tex\" id=\"S3.SS4.p3.1.m1.1c\">4\\\\times</annotation><annotation encoding=\"application/x-llamapun\" id=\"S3.SS4.p3.1.m1.1d\">4 \\xc3\\x97</annotation></semantics></math> more expensive. We use the zero-shot prompt by default in our following experiments but leave an additional study in <a class=\"ltx_ref\" href=\"#A4.SS2\" title=\"D.2 Few-shot judge \\xe2\\x80\\xa3 Appendix D Additional Experimental Results \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Section</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">D.2</span></a>.</p>\\n</div>\\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS4.p4\">\\n<p class=\"ltx_p\" id=\"S3.SS4.p4.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.SS4.p4.1.1\">Chain-of-thought and reference-guided judge.</span>\\n\\nIn <a class=\"ltx_ref\" href=\"#S3.SS3\" title=\"3.3 Limitations of LLM-as-a-Judge \\xe2\\x80\\xa3 3 LLM as a Judge \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Section</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">3.3</span></a>, we have shown LLM\\xe2\\x80\\x99s limited capability in grading math and reasoning questions.\\nWe propose two simple methods to mitigate this issue: chain-of-thought judge and reference-guided judge.\\nChain-of-thought is a widely used technique to improve LLM\\xe2\\x80\\x99s reasoning capability\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib47\" title=\"\">47</a>]</cite>.\\nWe propose a similar technique to prompt an LLM judge to begin with answering the question independently and then start grading.\\nDetailed prompt in Figure\\xc2\\xa0<a class=\"ltx_ref\" href=\"#A1.F6\" title=\"Figure 6 \\xe2\\x80\\xa3 Appendix A Prompt templates \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">6</span></a> (Appendix).\\nHowever, even with the CoT prompt, we find that in many cases LLM makes exactly the same mistake as the given answers in its problem-solving process (See example in Figure\\xc2\\xa0<a class=\"ltx_ref\" href=\"#A2.F14\" title=\"Figure 14 \\xe2\\x80\\xa3 Appendix B Case Study \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">14</span></a> (Appendix), suggesting that LLM judge may still be misled by the context.\\nHence, we propose a reference-guided method, in which we first generate LLM judge\\xe2\\x80\\x99s answer independently, and then display it as a reference answer in the judge prompt.\\nIn Table\\xc2\\xa0<a class=\"ltx_ref\" href=\"#S3.T4\" title=\"Table 4 \\xe2\\x80\\xa3 3.3 Limitations of LLM-as-a-Judge \\xe2\\x80\\xa3 3 LLM as a Judge \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">4</span></a>, we see a significant improvement in failure rate (from 70% to 15%) over the default prompt.</p>\\n</div>\\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS4.p5\">\\n<p class=\"ltx_p\" id=\"S3.SS4.p5.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.SS4.p5.1.1\">Fine-tuning a judge model.</span>\\n\\nWe try fine-tuning a Vicuna-13B on arena data to act as a judge and show some promising preliminary results in <a class=\"ltx_ref\" href=\"#A6\" title=\"Appendix F Exploring Vicuna as a judge \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">F</span></a>.</p>\\n</div>\\n</section>\\n<section class=\"ltx_subsection\" id=\"S3.SS5\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\">3.5 </span>Multi-turn judge</h3>\\n<div class=\"ltx_para ltx_noindent\" id=\"S3.SS5.p1\">\\n<p class=\"ltx_p\" id=\"S3.SS5.p1.1\">In MT-bench, every question involves two turns to evaluate conversational abilities.\\nTherefore, when comparing two assistants, it becomes necessary to present a total of two questions and four responses, complicating the prompt design.\\nWe explore two possible designs, (1) breaking the two turns into two prompts or (2) displaying complete conversations in a single prompt.\\nOur finding is the former one can cause the LLM judge struggling to locate the assistant\\xe2\\x80\\x99s previous response precisely.\\nWe illustrate a case in Figure\\xc2\\xa0<a class=\"ltx_ref\" href=\"#A2.F15\" title=\"Figure 15 \\xe2\\x80\\xa3 Appendix B Case Study \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">15</span></a> (Appendix) where GPT-4 makes an inaccurate judgment due to a faulty reference.\\nThis suggests the necessity of displaying a complete conversation to enable the LLM judge to better grasp the context.\\nWe then consider the alternative design that presents two full conversations in a single prompt in which we ask the LLM judge to focus on the second question (Figure\\xc2\\xa0<a class=\"ltx_ref\" href=\"#A1.F8\" title=\"Figure 8 \\xe2\\x80\\xa3 Appendix A Prompt templates \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">8</span></a> (Appendix)).\\nThis approach has been found to significantly alleviate the aforementioned referencing issue.</p>\\n</div>\\n</section>\\n</section>\\n<section class=\"ltx_section\" id=\"S4\">\\n<h2 class=\"ltx_title ltx_title_section\">\\n<span class=\"ltx_tag ltx_tag_section\">4 </span>Agreement Evaluation</h2>\\n<div class=\"ltx_para ltx_noindent\" id=\"S4.p1\">\\n<p class=\"ltx_p\" id=\"S4.p1.1\">We study the agreement between different LLM judges and humans on MT-bench and Chatbot Arena datasets. On MT-bench, we also study the agreement among humans. MT-bench represents a small-scale study with controlled human evaluation, while Chatbot Arena represents a larger-scale study with crowdsourced human evaluation in the wild.</p>\\n</div>\\n<section class=\"ltx_subsection\" id=\"S4.SS1\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\">4.1 </span>Setup</h3>\\n<div class=\"ltx_para ltx_noindent\" id=\"S4.SS1.p1\">\\n<p class=\"ltx_p\" id=\"S4.SS1.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.SS1.p1.1.1\">MT-bench.</span>\\nWe generate answers for all 80 questions with 6 models: GPT-4, GPT-3.5, Claude-V1, Vicuna-13B, Alpaca-13B\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib38\" title=\"\">38</a>]</cite>, and LLaMA-13B\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib39\" title=\"\">39</a>]</cite>. We then use 2 kinds of judges: LLM judges and 58 expert-level human labelers. The labelers are mostly graduate students so they are considered experts and more skilled than average crowd workers.\\nWe let LLM judges evaluate all pairs and let each human evaluate at least 20 random multi-turn questions. This resulted in around 3K votes for all questions.\\nThe detailed data collection process is in <a class=\"ltx_ref\" href=\"#A3\" title=\"Appendix C Data Collection \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">C</span></a>.</p>\\n</div>\\n<div class=\"ltx_para ltx_noindent\" id=\"S4.SS1.p2\">\\n<p class=\"ltx_p\" id=\"S4.SS1.p2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.SS1.p2.1.1\">Chatbot Arena.</span>\\nWe randomly sample 3K single-turn votes from 30K arena data, which covers models including GPT-4, GPT-3.5, Claude, Vicuna-7B/13B, Koala-13B\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib16\" title=\"\">16</a>]</cite>, Alpaca-13B, LLaMA-13B, and Dolly-12B. We use two kinds of judges: LLM judges and collected crowd judges (2114 unique IPs).</p>\\n</div>\\n<div class=\"ltx_para ltx_noindent\" id=\"S4.SS1.p3\">\\n<p class=\"ltx_p\" id=\"S4.SS1.p3.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.SS1.p3.1.1\">Metrics.</span>\\nWe define the <span class=\"ltx_text ltx_font_italic\" id=\"S4.SS1.p3.1.2\">agreement</span> between two types of judges as the probability of randomly selected individuals (but not identical) of each type agreeing on a randomly selected question.\\nSee more explanation in <a class=\"ltx_ref\" href=\"#A4.SS3\" title=\"D.3 Agreement Evaluation \\xe2\\x80\\xa3 Appendix D Additional Experimental Results \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Section</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">D.3</span></a>.\\n<span class=\"ltx_text ltx_font_italic\" id=\"S4.SS1.p3.1.3\">Average win rate</span> is the average of win rates against all other players.\\nThese metrics can be computed with or without including tie votes.</p>\\n</div>\\n</section>\\n<section class=\"ltx_subsection\" id=\"S4.SS2\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\">4.2 </span>High agreement between GPT-4 and humans</h3>\\n<div class=\"ltx_para ltx_noindent\" id=\"S4.SS2.p1\">\\n<p class=\"ltx_p\" id=\"S4.SS2.p1.1\">We compute agreement on MT-bench data.\\nIn <a class=\"ltx_ref\" href=\"#S4.T5.st2\" title=\"5(b) \\xe2\\x80\\xa3 Table 5 \\xe2\\x80\\xa3 4.2 High agreement between GPT-4 and humans \\xe2\\x80\\xa3 4 Agreement Evaluation \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Table</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">5(b)</span></a>, GPT-4 with both pairwise comparison and single answer grading show very high agreements with human experts.\\nThe agreement under setup S2 (w/o tie) between GPT-4 and humans reaches 85%, which is even higher than the agreement among humans (81%). This means GPT-4\\xe2\\x80\\x99s judgments closely align with the majority of humans.\\nWe also show that GPT-4\\xe2\\x80\\x99s judgments may help humans make better judgments. During our data collection, when a human\\xe2\\x80\\x99s choice deviated from GPT-4, we presented GPT-4\\xe2\\x80\\x99s judgments to humans and ask if they are reasonable (details in <a class=\"ltx_ref\" href=\"#A3.SS1\" title=\"C.1 MT-bench human evaluation \\xe2\\x80\\xa3 Appendix C Data Collection \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Section</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">C.1</span></a>). Despite different views, humans deemed GPT-4\\xe2\\x80\\x99s judgments reasonable in 75% of cases and are even willing to change their choices in 34% of cases.</p>\\n</div>\\n<div class=\"ltx_para ltx_noindent\" id=\"S4.SS2.p2\">\\n<p class=\"ltx_p\" id=\"S4.SS2.p2.1\">The data from Arena shows a similar trend, as illustrated by <a class=\"ltx_ref\" href=\"#S4.T7\" title=\"Table 7 \\xe2\\x80\\xa3 4.2 High agreement between GPT-4 and humans \\xe2\\x80\\xa3 4 Agreement Evaluation \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Table</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">7</span></a>. Comparing GPT-4 and other LLM judges, we find they reach a similar non-tie agreement ratio between humans but the number of non-tied votes from GPT-4 is much larger. This means that GPT-4 is more affirmative and less suffered from position bias but other models also perform well when they give an affirmative answer.</p>\\n</div>\\n<div class=\"ltx_para ltx_noindent\" id=\"S4.SS2.p3\">\\n<p class=\"ltx_p\" id=\"S4.SS2.p3.1\">In both tables, GPT-4 with single-answer grading matches both pairwise GPT-4 and human preferences very well.\\nThis means GPT-4 has a relatively stable internal rubric.\\nAlthough it may sometimes perform slightly worse than pairwise comparison and give more tie votes, it is a more scalable method.</p>\\n</div>\\n<div class=\"ltx_para ltx_noindent\" id=\"S4.SS2.p4\">\\n<p class=\"ltx_p\" id=\"S4.SS2.p4.1\">We then perform a breakdown analysis by computing agreement on different model pairs and categories.\\nWe only include non-tied votes.\\nIn <a class=\"ltx_ref\" href=\"#S4.T7\" title=\"Table 7 \\xe2\\x80\\xa3 4.2 High agreement between GPT-4 and humans \\xe2\\x80\\xa3 4 Agreement Evaluation \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Table</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">7</span></a>, we observe the agreement between GPT-4 and human progressively increases in line with the performance disparity of the model pairs (i.e., larger win rate difference), from 70% to nearly 100%. This suggests that GPT-4 aligns with humans better when significant performance differences exist between the models.</p>\\n</div>\\n<figure class=\"ltx_table\" id=\"S4.T5\">\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 5: </span>Agreement between two types of judges on MT-bench. \\xe2\\x80\\x9cG4-Pair\\xe2\\x80\\x9d and \\xe2\\x80\\x9cG4-Single\\xe2\\x80\\x9d denote GPT-4 with pairwise comparison and single-answer grading respectively.\\nThe single-answer grading can be converted into pairwise comparison results for calculating the agreement.\\nWe report two setups: \\xe2\\x80\\x9cS1\\xe2\\x80\\x9d includes non-tie, tie, and inconsistent (due to position bias) votes and counts inconsistent as tie; \\xe2\\x80\\x9cS2\\xe2\\x80\\x9d only includes non-tie votes.\\nThe agreement between two random judges under each setup is denoted as \\xe2\\x80\\x9cR=\\xe2\\x80\\x9d.\\nThe top value in each cell is the agreement, and the bottom gray value is #votes.\\n</figcaption><div class=\"ltx_flex_figure ltx_flex_table\">\\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\\n<figure class=\"ltx_table ltx_flex_size_2 ltx_align_center\" id=\"S4.T5.st1\">\\n<div class=\"ltx_inline-block ltx_transformed_outer\" id=\"S4.T5.st1.1\" style=\"width:433.6pt;height:230pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(118.1pt,-62.6pt) scale(2.19631194027036,2.19631194027036) ;\">\\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_top\" id=\"S4.T5.st1.1.1\">\\n<tbody class=\"ltx_tbody\">\\n<tr class=\"ltx_tr\" id=\"S4.T5.st1.1.1.1.1\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" id=\"S4.T5.st1.1.1.1.1.1\">Setup</th>\\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S4.T5.st1.1.1.1.1.2\">S1 (R = 33%)</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S4.T5.st1.1.1.1.1.3\">S2 (R = 50%)</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S4.T5.st1.1.1.2.2\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T5.st1.1.1.2.2.1\">Judge</th>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T5.st1.1.1.2.2.2\">G4-Single</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T5.st1.1.1.2.2.3\">Human</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T5.st1.1.1.2.2.4\">G4-Single</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T5.st1.1.1.2.2.5\">Human</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S4.T5.st1.1.1.3.3\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T5.st1.1.1.3.3.1\">G4-Pair</th>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T5.st1.1.1.3.3.2\">\\n<span class=\"ltx_inline-block\" id=\"S4.T5.st1.1.1.3.3.2.1\">\\n<span class=\"ltx_p\" id=\"S4.T5.st1.1.1.3.3.2.1.1\">70%</span>\\n<span class=\"ltx_p\" id=\"S4.T5.st1.1.1.3.3.2.1.2\"><span class=\"ltx_text\" id=\"S4.T5.st1.1.1.3.3.2.1.2.1\" style=\"color:#808080;\"> 1138</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T5.st1.1.1.3.3.3\">\\n<span class=\"ltx_inline-block\" id=\"S4.T5.st1.1.1.3.3.3.1\">\\n<span class=\"ltx_p\" id=\"S4.T5.st1.1.1.3.3.3.1.1\">66%</span>\\n<span class=\"ltx_p\" id=\"S4.T5.st1.1.1.3.3.3.1.2\"><span class=\"ltx_text\" id=\"S4.T5.st1.1.1.3.3.3.1.2.1\" style=\"color:#808080;\"> 1343</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T5.st1.1.1.3.3.4\">\\n<span class=\"ltx_inline-block\" id=\"S4.T5.st1.1.1.3.3.4.1\">\\n<span class=\"ltx_p\" id=\"S4.T5.st1.1.1.3.3.4.1.1\">97%</span>\\n<span class=\"ltx_p\" id=\"S4.T5.st1.1.1.3.3.4.1.2\"><span class=\"ltx_text\" id=\"S4.T5.st1.1.1.3.3.4.1.2.1\" style=\"color:#808080;\"> 662</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T5.st1.1.1.3.3.5\">\\n<span class=\"ltx_inline-block\" id=\"S4.T5.st1.1.1.3.3.5.1\">\\n<span class=\"ltx_p\" id=\"S4.T5.st1.1.1.3.3.5.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.st1.1.1.3.3.5.1.1.1\">85%</span></span>\\n<span class=\"ltx_p\" id=\"S4.T5.st1.1.1.3.3.5.1.2\"><span class=\"ltx_text\" id=\"S4.T5.st1.1.1.3.3.5.1.2.1\" style=\"color:#808080;\"> 859</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S4.T5.st1.1.1.4.4\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T5.st1.1.1.4.4.1\">G4-Single</th>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T5.st1.1.1.4.4.2\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T5.st1.1.1.4.4.3\">\\n<span class=\"ltx_inline-block\" id=\"S4.T5.st1.1.1.4.4.3.1\">\\n<span class=\"ltx_p\" id=\"S4.T5.st1.1.1.4.4.3.1.1\">60%</span>\\n<span class=\"ltx_p\" id=\"S4.T5.st1.1.1.4.4.3.1.2\"><span class=\"ltx_text\" id=\"S4.T5.st1.1.1.4.4.3.1.2.1\" style=\"color:#808080;\"> 1280</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T5.st1.1.1.4.4.4\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T5.st1.1.1.4.4.5\">\\n<span class=\"ltx_inline-block\" id=\"S4.T5.st1.1.1.4.4.5.1\">\\n<span class=\"ltx_p\" id=\"S4.T5.st1.1.1.4.4.5.1.1\">85%</span>\\n<span class=\"ltx_p\" id=\"S4.T5.st1.1.1.4.4.5.1.2\"><span class=\"ltx_text\" id=\"S4.T5.st1.1.1.4.4.5.1.2.1\" style=\"color:#808080;\"> 739</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S4.T5.st1.1.1.5.5\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" id=\"S4.T5.st1.1.1.5.5.1\">Human</th>\\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" id=\"S4.T5.st1.1.1.5.5.2\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" id=\"S4.T5.st1.1.1.5.5.3\">\\n<span class=\"ltx_inline-block\" id=\"S4.T5.st1.1.1.5.5.3.1\">\\n<span class=\"ltx_p\" id=\"S4.T5.st1.1.1.5.5.3.1.1\">63%</span>\\n<span class=\"ltx_p\" id=\"S4.T5.st1.1.1.5.5.3.1.2\"><span class=\"ltx_text\" id=\"S4.T5.st1.1.1.5.5.3.1.2.1\" style=\"color:#808080;\"> 721</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" id=\"S4.T5.st1.1.1.5.5.4\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" id=\"S4.T5.st1.1.1.5.5.5\">\\n<span class=\"ltx_inline-block\" id=\"S4.T5.st1.1.1.5.5.5.1\">\\n<span class=\"ltx_p\" id=\"S4.T5.st1.1.1.5.5.5.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.st1.1.1.5.5.5.1.1.1\">81%</span></span>\\n<span class=\"ltx_p\" id=\"S4.T5.st1.1.1.5.5.5.1.2\"><span class=\"ltx_text\" id=\"S4.T5.st1.1.1.5.5.5.1.2.1\" style=\"color:#808080;\"> 479</span></span>\\n</span>\\n</td>\\n</tr>\\n</tbody>\\n</table>\\n</span></div>\\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">(a) </span>First Turn</figcaption>\\n</figure>\\n</div>\\n<div class=\"ltx_flex_cell ltx_flex_size_2\">\\n<figure class=\"ltx_table ltx_flex_size_2 ltx_align_center\" id=\"S4.T5.st2\">\\n<div class=\"ltx_inline-block ltx_transformed_outer\" id=\"S4.T5.st2.1\" style=\"width:433.6pt;height:230pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(118.1pt,-62.6pt) scale(2.19631194027036,2.19631194027036) ;\">\\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_top\" id=\"S4.T5.st2.1.1\">\\n<tbody class=\"ltx_tbody\">\\n<tr class=\"ltx_tr\" id=\"S4.T5.st2.1.1.1.1\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" id=\"S4.T5.st2.1.1.1.1.1\">Setup</th>\\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S4.T5.st2.1.1.1.1.2\">S1 (R = 33%)</td>\\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S4.T5.st2.1.1.1.1.3\">S2 (R = 50%)</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S4.T5.st2.1.1.2.2\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T5.st2.1.1.2.2.1\">Judge</th>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T5.st2.1.1.2.2.2\">G4-Single</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T5.st2.1.1.2.2.3\">Human</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T5.st2.1.1.2.2.4\">G4-Single</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T5.st2.1.1.2.2.5\">Human</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S4.T5.st2.1.1.3.3\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T5.st2.1.1.3.3.1\">G4-Pair</th>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T5.st2.1.1.3.3.2\">\\n<span class=\"ltx_inline-block\" id=\"S4.T5.st2.1.1.3.3.2.1\">\\n<span class=\"ltx_p\" id=\"S4.T5.st2.1.1.3.3.2.1.1\">70%</span>\\n<span class=\"ltx_p\" id=\"S4.T5.st2.1.1.3.3.2.1.2\"><span class=\"ltx_text\" id=\"S4.T5.st2.1.1.3.3.2.1.2.1\" style=\"color:#808080;\"> 1161</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T5.st2.1.1.3.3.3\">\\n<span class=\"ltx_inline-block\" id=\"S4.T5.st2.1.1.3.3.3.1\">\\n<span class=\"ltx_p\" id=\"S4.T5.st2.1.1.3.3.3.1.1\">66%</span>\\n<span class=\"ltx_p\" id=\"S4.T5.st2.1.1.3.3.3.1.2\"><span class=\"ltx_text\" id=\"S4.T5.st2.1.1.3.3.3.1.2.1\" style=\"color:#808080;\"> 1325</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T5.st2.1.1.3.3.4\">\\n<span class=\"ltx_inline-block\" id=\"S4.T5.st2.1.1.3.3.4.1\">\\n<span class=\"ltx_p\" id=\"S4.T5.st2.1.1.3.3.4.1.1\">95%</span>\\n<span class=\"ltx_p\" id=\"S4.T5.st2.1.1.3.3.4.1.2\"><span class=\"ltx_text\" id=\"S4.T5.st2.1.1.3.3.4.1.2.1\" style=\"color:#808080;\"> 727</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T5.st2.1.1.3.3.5\">\\n<span class=\"ltx_inline-block\" id=\"S4.T5.st2.1.1.3.3.5.1\">\\n<span class=\"ltx_p\" id=\"S4.T5.st2.1.1.3.3.5.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.st2.1.1.3.3.5.1.1.1\">85%</span></span>\\n<span class=\"ltx_p\" id=\"S4.T5.st2.1.1.3.3.5.1.2\"><span class=\"ltx_text\" id=\"S4.T5.st2.1.1.3.3.5.1.2.1\" style=\"color:#808080;\"> 864</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S4.T5.st2.1.1.4.4\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T5.st2.1.1.4.4.1\">G4-Single</th>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T5.st2.1.1.4.4.2\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T5.st2.1.1.4.4.3\">\\n<span class=\"ltx_inline-block\" id=\"S4.T5.st2.1.1.4.4.3.1\">\\n<span class=\"ltx_p\" id=\"S4.T5.st2.1.1.4.4.3.1.1\">59%</span>\\n<span class=\"ltx_p\" id=\"S4.T5.st2.1.1.4.4.3.1.2\"><span class=\"ltx_text\" id=\"S4.T5.st2.1.1.4.4.3.1.2.1\" style=\"color:#808080;\"> 1285</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T5.st2.1.1.4.4.4\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T5.st2.1.1.4.4.5\">\\n<span class=\"ltx_inline-block\" id=\"S4.T5.st2.1.1.4.4.5.1\">\\n<span class=\"ltx_p\" id=\"S4.T5.st2.1.1.4.4.5.1.1\">84%</span>\\n<span class=\"ltx_p\" id=\"S4.T5.st2.1.1.4.4.5.1.2\"><span class=\"ltx_text\" id=\"S4.T5.st2.1.1.4.4.5.1.2.1\" style=\"color:#808080;\"> 776</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S4.T5.st2.1.1.5.5\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t\" id=\"S4.T5.st2.1.1.5.5.1\">Human</th>\\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_t\" id=\"S4.T5.st2.1.1.5.5.2\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_t\" id=\"S4.T5.st2.1.1.5.5.3\">\\n<span class=\"ltx_inline-block\" id=\"S4.T5.st2.1.1.5.5.3.1\">\\n<span class=\"ltx_p\" id=\"S4.T5.st2.1.1.5.5.3.1.1\">67%</span>\\n<span class=\"ltx_p\" id=\"S4.T5.st2.1.1.5.5.3.1.2\"><span class=\"ltx_text\" id=\"S4.T5.st2.1.1.5.5.3.1.2.1\" style=\"color:#808080;\"> 707</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_t\" id=\"S4.T5.st2.1.1.5.5.4\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_t\" id=\"S4.T5.st2.1.1.5.5.5\">\\n<span class=\"ltx_inline-block\" id=\"S4.T5.st2.1.1.5.5.5.1\">\\n<span class=\"ltx_p\" id=\"S4.T5.st2.1.1.5.5.5.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.st2.1.1.5.5.5.1.1.1\">82%</span></span>\\n<span class=\"ltx_p\" id=\"S4.T5.st2.1.1.5.5.5.1.2\"><span class=\"ltx_text\" id=\"S4.T5.st2.1.1.5.5.5.1.2.1\" style=\"color:#808080;\"> 474</span></span>\\n</span>\\n</td>\\n</tr>\\n</tbody>\\n</table>\\n</span></div>\\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">(b) </span>Second Turn</figcaption>\\n</figure>\\n</div>\\n</div>\\n</figure>\\n<figure class=\"ltx_table\" id=\"S4.T7\">\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 6: </span>Agreement between two types of judges on Chatbot Arena.\\n\\xe2\\x80\\x9cG4-S\\xe2\\x80\\x9d denotes GPT-4 with single-answer grading.\\n\\xe2\\x80\\x9cG4\\xe2\\x80\\x9d, \\xe2\\x80\\x9cG3.5\\xe2\\x80\\x9d and \\xe2\\x80\\x9cC\\xe2\\x80\\x9d denote GPT-4, GPT-3.5, and Claude with pairwise comparison, respectively.\\n\\xe2\\x80\\x9cH\\xe2\\x80\\x9d denotes human.\\nThe remaining of table follows the same format as <a class=\"ltx_ref\" href=\"#S4.T5.st2\" title=\"5(b) \\xe2\\x80\\xa3 Table 5 \\xe2\\x80\\xa3 4.2 High agreement between GPT-4 and humans \\xe2\\x80\\xa3 4 Agreement Evaluation \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Table</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">5(b)</span></a>.\\n</figcaption><div class=\"ltx_flex_figure ltx_flex_table\">\\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\\n<div class=\"ltx_inline-block ltx_flex_size_1 ltx_align_center ltx_transformed_outer\" id=\"S4.T7.2\" style=\"width:433.6pt;height:252.6pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(107.3pt,-62.5pt) scale(1.97974437791434,1.97974437791434) ;\">\\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T7.2.1\">\\n<thead class=\"ltx_thead\">\\n<tr class=\"ltx_tr\" id=\"S4.T7.2.1.1.1\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S4.T7.2.1.1.1.1\">Setup</th>\\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"4\" id=\"S4.T7.2.1.1.1.2\">S1 (Random = 33%)</th>\\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"4\" id=\"S4.T7.2.1.1.1.3\">S2 (Random = 50%)</th>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S4.T7.2.1.2.2\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\" id=\"S4.T7.2.1.2.2.1\">Judge</th>\\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"S4.T7.2.1.2.2.2\">G4-S</th>\\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"S4.T7.2.1.2.2.3\">G3.5</th>\\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"S4.T7.2.1.2.2.4\">C</th>\\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"S4.T7.2.1.2.2.5\">H</th>\\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"S4.T7.2.1.2.2.6\">G4-S</th>\\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"S4.T7.2.1.2.2.7\">G3.5</th>\\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"S4.T7.2.1.2.2.8\">C</th>\\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"S4.T7.2.1.2.2.9\">H</th>\\n</tr>\\n</thead>\\n<tbody class=\"ltx_tbody\">\\n<tr class=\"ltx_tr\" id=\"S4.T7.2.1.3.1\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T7.2.1.3.1.1\">G4</th>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T7.2.1.3.1.2\">\\n<span class=\"ltx_inline-block\" id=\"S4.T7.2.1.3.1.2.1\">\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.3.1.2.1.1\">72%</span>\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.3.1.2.1.2\"><span class=\"ltx_text\" id=\"S4.T7.2.1.3.1.2.1.2.1\" style=\"color:#808080;\">2968</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T7.2.1.3.1.3\">\\n<span class=\"ltx_inline-block\" id=\"S4.T7.2.1.3.1.3.1\">\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.3.1.3.1.1\">66%</span>\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.3.1.3.1.2\"><span class=\"ltx_text\" id=\"S4.T7.2.1.3.1.3.1.2.1\" style=\"color:#808080;\">3061</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T7.2.1.3.1.4\">\\n<span class=\"ltx_inline-block\" id=\"S4.T7.2.1.3.1.4.1\">\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.3.1.4.1.1\">66%</span>\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.3.1.4.1.2\"><span class=\"ltx_text\" id=\"S4.T7.2.1.3.1.4.1.2.1\" style=\"color:#808080;\">3062</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T7.2.1.3.1.5\">\\n<span class=\"ltx_inline-block\" id=\"S4.T7.2.1.3.1.5.1\">\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.3.1.5.1.1\">64%</span>\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.3.1.5.1.2\"><span class=\"ltx_text\" id=\"S4.T7.2.1.3.1.5.1.2.1\" style=\"color:#808080;\">3066</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T7.2.1.3.1.6\">\\n<span class=\"ltx_inline-block\" id=\"S4.T7.2.1.3.1.6.1\">\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.3.1.6.1.1\">95%</span>\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.3.1.6.1.2\"><span class=\"ltx_text\" id=\"S4.T7.2.1.3.1.6.1.2.1\" style=\"color:#808080;\">1967</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T7.2.1.3.1.7\">\\n<span class=\"ltx_inline-block\" id=\"S4.T7.2.1.3.1.7.1\">\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.3.1.7.1.1\">94%</span>\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.3.1.7.1.2\"><span class=\"ltx_text\" id=\"S4.T7.2.1.3.1.7.1.2.1\" style=\"color:#808080;\">1788</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T7.2.1.3.1.8\">\\n<span class=\"ltx_inline-block\" id=\"S4.T7.2.1.3.1.8.1\">\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.3.1.8.1.1\">95%</span>\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.3.1.8.1.2\"><span class=\"ltx_text\" id=\"S4.T7.2.1.3.1.8.1.2.1\" style=\"color:#808080;\">1712</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T7.2.1.3.1.9\">\\n<span class=\"ltx_inline-block\" id=\"S4.T7.2.1.3.1.9.1\">\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.3.1.9.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T7.2.1.3.1.9.1.1.1\">87%</span></span>\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.3.1.9.1.2\"><span class=\"ltx_text\" id=\"S4.T7.2.1.3.1.9.1.2.1\" style=\"color:#808080;\">1944</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S4.T7.2.1.4.2\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T7.2.1.4.2.1\">G4-S</th>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T7.2.1.4.2.2\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T7.2.1.4.2.3\">\\n<span class=\"ltx_inline-block\" id=\"S4.T7.2.1.4.2.3.1\">\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.4.2.3.1.1\">60%</span>\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.4.2.3.1.2\"><span class=\"ltx_text\" id=\"S4.T7.2.1.4.2.3.1.2.1\" style=\"color:#808080;\">2964</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T7.2.1.4.2.4\">\\n<span class=\"ltx_inline-block\" id=\"S4.T7.2.1.4.2.4.1\">\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.4.2.4.1.1\">62%</span>\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.4.2.4.1.2\"><span class=\"ltx_text\" id=\"S4.T7.2.1.4.2.4.1.2.1\" style=\"color:#808080;\">2964</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T7.2.1.4.2.5\">\\n<span class=\"ltx_inline-block\" id=\"S4.T7.2.1.4.2.5.1\">\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.4.2.5.1.1\">60%</span>\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.4.2.5.1.2\"><span class=\"ltx_text\" id=\"S4.T7.2.1.4.2.5.1.2.1\" style=\"color:#808080;\">2968</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T7.2.1.4.2.6\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T7.2.1.4.2.7\">\\n<span class=\"ltx_inline-block\" id=\"S4.T7.2.1.4.2.7.1\">\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.4.2.7.1.1\">89%</span>\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.4.2.7.1.2\"><span class=\"ltx_text\" id=\"S4.T7.2.1.4.2.7.1.2.1\" style=\"color:#808080;\">1593</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T7.2.1.4.2.8\">\\n<span class=\"ltx_inline-block\" id=\"S4.T7.2.1.4.2.8.1\">\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.4.2.8.1.1\">91%</span>\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.4.2.8.1.2\"><span class=\"ltx_text\" id=\"S4.T7.2.1.4.2.8.1.2.1\" style=\"color:#808080;\">1538</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T7.2.1.4.2.9\">\\n<span class=\"ltx_inline-block\" id=\"S4.T7.2.1.4.2.9.1\">\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.4.2.9.1.1\">85%</span>\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.4.2.9.1.2\"><span class=\"ltx_text\" id=\"S4.T7.2.1.4.2.9.1.2.1\" style=\"color:#808080;\">1761</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S4.T7.2.1.5.3\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T7.2.1.5.3.1\">G3.5</th>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T7.2.1.5.3.2\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T7.2.1.5.3.3\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T7.2.1.5.3.4\">\\n<span class=\"ltx_inline-block\" id=\"S4.T7.2.1.5.3.4.1\">\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.5.3.4.1.1\">68%</span>\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.5.3.4.1.2\"><span class=\"ltx_text\" id=\"S4.T7.2.1.5.3.4.1.2.1\" style=\"color:#808080;\">3057</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T7.2.1.5.3.5\">\\n<span class=\"ltx_inline-block\" id=\"S4.T7.2.1.5.3.5.1\">\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.5.3.5.1.1\">54%</span>\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.5.3.5.1.2\"><span class=\"ltx_text\" id=\"S4.T7.2.1.5.3.5.1.2.1\" style=\"color:#808080;\">3061</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T7.2.1.5.3.6\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T7.2.1.5.3.7\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T7.2.1.5.3.8\">\\n<span class=\"ltx_inline-block\" id=\"S4.T7.2.1.5.3.8.1\">\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.5.3.8.1.1\">96%</span>\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.5.3.8.1.2\"><span class=\"ltx_text\" id=\"S4.T7.2.1.5.3.8.1.2.1\" style=\"color:#808080;\">1497</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"S4.T7.2.1.5.3.9\">\\n<span class=\"ltx_inline-block\" id=\"S4.T7.2.1.5.3.9.1\">\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.5.3.9.1.1\">83%</span>\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.5.3.9.1.2\"><span class=\"ltx_text\" id=\"S4.T7.2.1.5.3.9.1.2.1\" style=\"color:#808080;\">1567</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S4.T7.2.1.6.4\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" id=\"S4.T7.2.1.6.4.1\">C</th>\\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" id=\"S4.T7.2.1.6.4.2\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" id=\"S4.T7.2.1.6.4.3\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" id=\"S4.T7.2.1.6.4.4\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" id=\"S4.T7.2.1.6.4.5\">\\n<span class=\"ltx_inline-block\" id=\"S4.T7.2.1.6.4.5.1\">\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.6.4.5.1.1\">53%</span>\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.6.4.5.1.2\"><span class=\"ltx_text\" id=\"S4.T7.2.1.6.4.5.1.2.1\" style=\"color:#808080;\">3062</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" id=\"S4.T7.2.1.6.4.6\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" id=\"S4.T7.2.1.6.4.7\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" id=\"S4.T7.2.1.6.4.8\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\" id=\"S4.T7.2.1.6.4.9\">\\n<span class=\"ltx_inline-block\" id=\"S4.T7.2.1.6.4.9.1\">\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.6.4.9.1.1\">84%</span>\\n<span class=\"ltx_p\" id=\"S4.T7.2.1.6.4.9.1.2\"><span class=\"ltx_text\" id=\"S4.T7.2.1.6.4.9.1.2.1\" style=\"color:#808080;\">1475</span></span>\\n</span>\\n</td>\\n</tr>\\n</tbody>\\n</table>\\n</span></div>\\n</div>\\n<div class=\"ltx_flex_break\"></div>\\n<div class=\"ltx_flex_cell ltx_flex_size_1\"><span class=\"ltx_inline-para ltx_minipage ltx_flex_size_1 ltx_align_center ltx_align_middle\" id=\"S4.T7.1\" style=\"width:169.1pt;\">\\n<span class=\"ltx_para ltx_align_center\" id=\"S4.T7.1.p1\"><img alt=\"[Uncaptioned image]\" class=\"ltx_graphics ltx_img_landscape\" height=\"416\" id=\"S4.T7.1.p1.g1\" src=\"x2.png\" width=\"549\"/>\\n</span></span></div>\\n</div>\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 7: </span>Agreement and win rate difference. Each point corresponds to a model pair and counts only the non-tie votes between the two models. The x-axis value is the win rate difference between the two models. The y-axis value is the GPT-4 and human agreement.</figcaption>\\n</figure>\\n<figure class=\"ltx_figure\" id=\"S4.F2\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"211\" id=\"S4.F2.g1\" src=\"x3.png\" width=\"830\"/>\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 2: </span>Average win rate of six models under different judges on MT-bench.</figcaption>\\n</figure>\\n<figure class=\"ltx_figure\" id=\"S4.F3\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"211\" id=\"S4.F3.g1\" src=\"x4.png\" width=\"830\"/>\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 3: </span>Average win rate of nine models under different judges on Chatbot Arena.</figcaption>\\n</figure>\\n<figure class=\"ltx_table\" id=\"S4.T8\">\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 8: </span>Category-wise win rate of models.</figcaption>\\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T8.1\" style=\"width:411.9pt;height:103.1pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(26.1pt,-6.5pt) scale(1.14506381340065,1.14506381340065) ;\">\\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T8.1.1\">\\n<thead class=\"ltx_thead\">\\n<tr class=\"ltx_tr\" id=\"S4.T8.1.1.1.1\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T8.1.1.1.1.1\">Model</th>\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T8.1.1.1.1.2\">Writing</th>\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T8.1.1.1.1.3\">Roleplay</th>\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T8.1.1.1.1.4\">Reasoning</th>\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T8.1.1.1.1.5\">Math</th>\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T8.1.1.1.1.6\">Coding</th>\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T8.1.1.1.1.7\">Extraction</th>\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T8.1.1.1.1.8\">STEM</th>\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T8.1.1.1.1.9\">Humanities</th>\\n</tr>\\n</thead>\\n<tbody class=\"ltx_tbody\">\\n<tr class=\"ltx_tr\" id=\"S4.T8.1.1.2.1\">\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T8.1.1.2.1.1\">GPT-4</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T8.1.1.2.1.2\">61.2%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T8.1.1.2.1.3\">67.9%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T8.1.1.2.1.4\">49.3%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T8.1.1.2.1.5\">66.1%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T8.1.1.2.1.6\">56.3%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T8.1.1.2.1.7\">66.2%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T8.1.1.2.1.8\">76.6%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T8.1.1.2.1.9\">72.2%</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S4.T8.1.1.3.2\">\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T8.1.1.3.2.1\">GPT-3.5</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T8.1.1.3.2.2\">50.9%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T8.1.1.3.2.3\">60.6%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T8.1.1.3.2.4\">32.6%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T8.1.1.3.2.5\">63.8%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T8.1.1.3.2.6\">55.0%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T8.1.1.3.2.7\">48.8%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T8.1.1.3.2.8\">52.8%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T8.1.1.3.2.9\">53.8%</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S4.T8.1.1.4.3\">\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T8.1.1.4.3.1\">Vicuna-13B</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T8.1.1.4.3.2\">39.7%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T8.1.1.4.3.3\">39.2%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T8.1.1.4.3.4\">20.1%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T8.1.1.4.3.5\">18.0%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T8.1.1.4.3.6\">36.9%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T8.1.1.4.3.7\">29.2%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T8.1.1.4.3.8\">47.0%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T8.1.1.4.3.9\">47.5%</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S4.T8.1.1.5.4\">\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T8.1.1.5.4.1\">LLaMA-13B</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T8.1.1.5.4.2\">15.1%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T8.1.1.5.4.3\">15.1%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T8.1.1.5.4.4\">7.8%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T8.1.1.5.4.5\">7.5%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T8.1.1.5.4.6\">2.1%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T8.1.1.5.4.7\">9.3%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T8.1.1.5.4.8\">6.8%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T8.1.1.5.4.9\">10.1%</td>\\n</tr>\\n</tbody>\\n</table>\\n</span></div>\\n</figure>\\n</section>\\n<section class=\"ltx_subsection\" id=\"S4.SS3\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\">4.3 </span>Win rates under different judges</h3>\\n<div class=\"ltx_para\" id=\"S4.SS3.p1\">\\n<p class=\"ltx_p\" id=\"S4.SS3.p1.1\">We plot the average win rate of models under different judges on MT-bench and Chatbot Arena in <a class=\"ltx_ref\" href=\"#S4.F2\" title=\"Figure 2 \\xe2\\x80\\xa3 4.2 High agreement between GPT-4 and humans \\xe2\\x80\\xa3 4 Agreement Evaluation \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">2</span></a> and <a class=\"ltx_ref\" href=\"#S4.F3\" title=\"Figure 3 \\xe2\\x80\\xa3 4.2 High agreement between GPT-4 and humans \\xe2\\x80\\xa3 4 Agreement Evaluation \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">3</span></a>, respectively. The win rate curves from LLM judges closely match the curves from humans. On MT-bench second turn, proprietary models like Claude and GPT-3.5 are more preferred by the humans compared to the first turn, meaning that a multi-turn benchmark can better differentiate some advanced abilities of models. We also list the per-category win rate of representative models in <a class=\"ltx_ref\" href=\"#S4.T8\" title=\"Table 8 \\xe2\\x80\\xa3 4.2 High agreement between GPT-4 and humans \\xe2\\x80\\xa3 4 Agreement Evaluation \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Table</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">8</span></a> to show how MT-bench differentiates models, in which we see GPT-4 is significantly better than others.\\nVicuna-13B is noticeably worse than GPT-3.5/4 in reasoning, math, and coding categories.\\nNote that in math/coding category, GPT-3.5 and GPT-4 have similar overall win-rate because they both failed to answer some hard questions, but GPT-4 is still significantly better than GPT-3 in the direct pairwise comparison or single-answer grading.\\nPlease see a performance breakdown of MT-bench score for each category in <a class=\"ltx_ref\" href=\"#A4.SS4\" title=\"D.4 Category-wise scores with single-answer grading \\xe2\\x80\\xa3 Appendix D Additional Experimental Results \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Section</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">D.4</span></a>.</p>\\n</div>\\n<figure class=\"ltx_table\" id=\"S4.T9\">\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 9: </span>Evaluation results of several model variants.</figcaption>\\n<div class=\"ltx_inline-block ltx_align_center ltx_transformed_outer\" id=\"S4.T9.1\" style=\"width:411.9pt;height:184.6pt;vertical-align:-0.9pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(-16.0pt,7.1pt) scale(0.927930980838804,0.927930980838804) ;\">\\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T9.1.1\">\\n<tbody class=\"ltx_tbody\">\\n<tr class=\"ltx_tr\" id=\"S4.T9.1.1.1.1\">\\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S4.T9.1.1.1.1.1\">Model</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S4.T9.1.1.1.1.2\">#Training Token</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S4.T9.1.1.1.1.3\">MMLU (5-shot)</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S4.T9.1.1.1.1.4\">TruthfulQA (0-shot)</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S4.T9.1.1.1.1.5\">MT-Bench Score (GPT-4)</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S4.T9.1.1.2.2\">\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T9.1.1.2.2.1\">LLaMA-7B</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T9.1.1.2.2.2\">1T</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T9.1.1.2.2.3\">35.2</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T9.1.1.2.2.4\">0.22</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T9.1.1.2.2.5\">2.74</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S4.T9.1.1.3.3\">\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T9.1.1.3.3.1\">LLaMA-13B</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T9.1.1.3.3.2\">1T</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T9.1.1.3.3.3\">47.0</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T9.1.1.3.3.4\">0.26</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T9.1.1.3.3.5\">2.61</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S4.T9.1.1.4.4\">\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T9.1.1.4.4.1\">Alpaca-7B</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T9.1.1.4.4.2\">4.4M</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T9.1.1.4.4.3\">40.1</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T9.1.1.4.4.4\">0.26</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T9.1.1.4.4.5\">4.54</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S4.T9.1.1.5.5\">\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T9.1.1.5.5.1\">Alpaca-13B</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T9.1.1.5.5.2\">4.4M</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T9.1.1.5.5.3\">48.1</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T9.1.1.5.5.4\">0.30</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T9.1.1.5.5.5\">4.53</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S4.T9.1.1.6.6\">\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T9.1.1.6.6.1\">Vicuna-7B (selected)</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T9.1.1.6.6.2\">4.8M</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T9.1.1.6.6.3\">37.3</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T9.1.1.6.6.4\">0.32</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T9.1.1.6.6.5\">5.95</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S4.T9.1.1.7.7\">\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T9.1.1.7.7.1\">Vicuna-7B (single)</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T9.1.1.7.7.2\">184M</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T9.1.1.7.7.3\">44.1</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T9.1.1.7.7.4\">0.30</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T9.1.1.7.7.5\">6.04</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S4.T9.1.1.8.8\">\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T9.1.1.8.8.1\">Vicuna-7B (all)</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T9.1.1.8.8.2\">370M</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T9.1.1.8.8.3\">47.1</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T9.1.1.8.8.4\">0.32</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T9.1.1.8.8.5\">6.00</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S4.T9.1.1.9.9\">\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T9.1.1.9.9.1\">Vicuna-13B (all)</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T9.1.1.9.9.2\">370M</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T9.1.1.9.9.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T9.1.1.9.9.3.1\">52.1</span></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T9.1.1.9.9.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T9.1.1.9.9.4.1\">0.35</span></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T9.1.1.9.9.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T9.1.1.9.9.5.1\">6.39</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S4.T9.1.1.10.10\">\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T9.1.1.10.10.1\">GPT-3.5</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T9.1.1.10.10.2\">-</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T9.1.1.10.10.3\">70.0</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T9.1.1.10.10.4\">-</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T9.1.1.10.10.5\">7.94</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"S4.T9.1.1.11.11\">\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T9.1.1.11.11.1\">GPT-4</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T9.1.1.11.11.2\">-</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T9.1.1.11.11.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T9.1.1.11.11.3.1\">86.4</span></td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T9.1.1.11.11.4\">-</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T9.1.1.11.11.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T9.1.1.11.11.5.1\">8.99</span></td>\\n</tr>\\n</tbody>\\n</table>\\n</span></div>\\n</figure>\\n</section>\\n</section>\\n<section class=\"ltx_section\" id=\"S5\">\\n<h2 class=\"ltx_title ltx_title_section\">\\n<span class=\"ltx_tag ltx_tag_section\">5 </span>Human Preference Benchmark and Standardized Benchmark</h2>\\n<div class=\"ltx_para ltx_noindent\" id=\"S5.p1\">\\n<p class=\"ltx_p\" id=\"S5.p1.1\">Human preference benchmarks such as MT-bench and Chatbot Arena serve as valuable additions to the current standardized LLM benchmarks. They focus on different aspects of a model and the recommended way is to comprehensively evaluate models with both kinds of benchmarks.</p>\\n</div>\\n<div class=\"ltx_para ltx_noindent\" id=\"S5.p2\">\\n<p class=\"ltx_p\" id=\"S5.p2.1\">We evaluate several model variants derived from LLaMA on MMLU\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib19\" title=\"\">19</a>]</cite>, Truthful QA\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib26\" title=\"\">26</a>]</cite> (MC1), and MT-bench (GPT-4 judge). The training details are in <a class=\"ltx_ref\" href=\"#A5\" title=\"Appendix E Training Details of Vicuna Models \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">E</span></a>.\\nSince we have shown that GPT-4 single-answer grading also performs well in <a class=\"ltx_ref\" href=\"#S4.SS2\" title=\"4.2 High agreement between GPT-4 and humans \\xe2\\x80\\xa3 4 Agreement Evaluation \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Section</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">4.2</span></a>, we use GPT-4 single-answer grading for MT-bench in favor of its scalability and simplicity.\\nWe ask GPT-4 to give a score for each turn on a scale of 10 by using our prompt templates (<a class=\"ltx_ref\" href=\"#A1.F5\" title=\"Figure 5 \\xe2\\x80\\xa3 Appendix A Prompt templates \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">5</span></a>, <a class=\"ltx_ref\" href=\"#A1.F9\" title=\"Figure 9 \\xe2\\x80\\xa3 Appendix A Prompt templates \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">9</span></a>) and report an average score of <math alttext=\"160=80\\\\times 2\" class=\"ltx_Math\" display=\"inline\" id=\"S5.p2.1.m1.1\"><semantics id=\"S5.p2.1.m1.1a\"><mrow id=\"S5.p2.1.m1.1.1\" xref=\"S5.p2.1.m1.1.1.cmml\"><mn id=\"S5.p2.1.m1.1.1.2\" xref=\"S5.p2.1.m1.1.1.2.cmml\">160</mn><mo id=\"S5.p2.1.m1.1.1.1\" xref=\"S5.p2.1.m1.1.1.1.cmml\">=</mo><mrow id=\"S5.p2.1.m1.1.1.3\" xref=\"S5.p2.1.m1.1.1.3.cmml\"><mn id=\"S5.p2.1.m1.1.1.3.2\" xref=\"S5.p2.1.m1.1.1.3.2.cmml\">80</mn><mo id=\"S5.p2.1.m1.1.1.3.1\" lspace=\"0.222em\" rspace=\"0.222em\" xref=\"S5.p2.1.m1.1.1.3.1.cmml\">\\xc3\\x97</mo><mn id=\"S5.p2.1.m1.1.1.3.3\" xref=\"S5.p2.1.m1.1.1.3.3.cmml\">2</mn></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.p2.1.m1.1b\"><apply id=\"S5.p2.1.m1.1.1.cmml\" xref=\"S5.p2.1.m1.1.1\"><eq id=\"S5.p2.1.m1.1.1.1.cmml\" xref=\"S5.p2.1.m1.1.1.1\"></eq><cn id=\"S5.p2.1.m1.1.1.2.cmml\" type=\"integer\" xref=\"S5.p2.1.m1.1.1.2\">160</cn><apply id=\"S5.p2.1.m1.1.1.3.cmml\" xref=\"S5.p2.1.m1.1.1.3\"><times id=\"S5.p2.1.m1.1.1.3.1.cmml\" xref=\"S5.p2.1.m1.1.1.3.1\"></times><cn id=\"S5.p2.1.m1.1.1.3.2.cmml\" type=\"integer\" xref=\"S5.p2.1.m1.1.1.3.2\">80</cn><cn id=\"S5.p2.1.m1.1.1.3.3.cmml\" type=\"integer\" xref=\"S5.p2.1.m1.1.1.3.3\">2</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.p2.1.m1.1c\">160=80\\\\times 2</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.p2.1.m1.1d\">160 = 80 \\xc3\\x97 2</annotation></semantics></math> turns.\\n<a class=\"ltx_ref\" href=\"#S4.T9\" title=\"Table 9 \\xe2\\x80\\xa3 4.3 Win rates under different judges \\xe2\\x80\\xa3 4 Agreement Evaluation \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Table</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">9</span></a> shows the results.\\nWe find that fine-tuning on high-quality dialog datasets (i.e., ShareGPT) can consistently improve the model performance on MMLU and the improvement scales with fine-tuning data size.\\nOn the other hand, a small high-quality conversation dataset can quickly teach the model a style preferred by GPT-4 (or approximately human) but cannot improve MMLU significantly, as shown by the Vicuna-7B (selected) which is trained with only 4.8M tokens or 3K conversations. In <a class=\"ltx_ref\" href=\"#S4.T9\" title=\"Table 9 \\xe2\\x80\\xa3 4.3 Win rates under different judges \\xe2\\x80\\xa3 4 Agreement Evaluation \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Table</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">9</span></a>, no single benchmark can determine model quality, meaning that a comprehensive evaluation is needed. Our results indicate that using LLM-as-a-judge to approximate human preferences is highly feasible and could become a new standard in future benchmarks. We are also hosting a regularly updated leaderboard with more models <span class=\"ltx_note ltx_role_footnote\" id=\"footnote1\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">1</sup><span class=\"ltx_tag ltx_tag_note\">1</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard\" title=\"\">https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard</a></span></span></span>.\\nNotably, DynaBench\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib21\" title=\"\">21</a>]</cite>, a research platform dedicated to dynamic data collection and benchmarking, aligns with our spirit. DynaBench addresses the challenges posed by static standardized benchmarks, such as saturation and overfitting, by emphasizing dynamic data with human-in-the-loop. Our LLM-as-a-judge approach can automate and scale platforms of this nature.</p>\\n</div>\\n</section>\\n<section class=\"ltx_section\" id=\"S6\">\\n<h2 class=\"ltx_title ltx_title_section\">\\n<span class=\"ltx_tag ltx_tag_section\">6 </span>Discussion</h2>\\n<div class=\"ltx_para ltx_noindent\" id=\"S6.p1\">\\n<p class=\"ltx_p\" id=\"S6.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.p1.1.1\">Limitations.</span>\\nThis paper emphasizes helpfulness but largely neglects safety. Honesty and harmlessness are crucial for a chat assistant as well\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib2\" title=\"\">2</a>]</cite>. We anticipate similar methods can be used to evaluate these metrics by modifying the default prompt. Additionally, within helpfulness, there are multiple dimensions like accuracy, relevance, and creativity, but they are all combined into a single metric in this study. A more comprehensive evaluation can be developed by analyzing and separating these dimensions.\\nWe propose preliminary solutions to address the limitations and biases of LLM-as-a-judge in <a class=\"ltx_ref\" href=\"#S3.SS4\" title=\"3.4 Addressing limitations \\xe2\\x80\\xa3 3 LLM as a Judge \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Section</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">3.4</span></a>, but we anticipate more advanced methods can be developed.</p>\\n</div>\\n<div class=\"ltx_para ltx_noindent\" id=\"S6.p2\">\\n<p class=\"ltx_p\" id=\"S6.p2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.p2.1.1\">Data collection and release.</span>\\n<a class=\"ltx_ref\" href=\"#A3\" title=\"Appendix C Data Collection \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Appendix</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">C</span></a> describes the detailed data collection and release processes, which include the instructions we give to users, the screenshots of the data collection interface, the information about participated users, and the content of the released data.</p>\\n</div>\\n<div class=\"ltx_para ltx_noindent\" id=\"S6.p3\">\\n<p class=\"ltx_p\" id=\"S6.p3.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.p3.1.1\">Societal impacts.</span>\\nThe societal impact of this study is multi-faceted. Our evaluation methods can help enhance chatbot quality and user experiences. However, addressing biases in these methods is crucial. Our dataset enables better studies of human preferences and model behavior. Advanced chat assistants may replace certain human tasks, resulting in job displacements and new opportunities.</p>\\n</div>\\n<div class=\"ltx_para ltx_noindent\" id=\"S6.p4\">\\n<p class=\"ltx_p\" id=\"S6.p4.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S6.p4.1.1\">Future directions.</span>\\n1) Benchmarking chatbots at scale with a broader set of categories 2) Open-source LLM judge aligned with human preference 3) Enhancing open models\\xe2\\x80\\x99 math/reasoning capability.</p>\\n</div>\\n</section>\\n<section class=\"ltx_section\" id=\"S7\">\\n<h2 class=\"ltx_title ltx_title_section\">\\n<span class=\"ltx_tag ltx_tag_section\">7 </span>Conclusion</h2>\\n<div class=\"ltx_para ltx_noindent\" id=\"S7.p1\">\\n<p class=\"ltx_p\" id=\"S7.p1.1\">In this paper, we propose LLM-as-a-judge for chatbot evaluation and systematically examine its efficacy using human preference data from 58 experts on MT-bench, as well as thousands of crowd-users on Chatbot Arena.\\nOur results reveal that strong LLMs can achieve an agreement rate of over 80%, on par with the level of agreement among human experts, establishing a foundation for an LLM-based evaluation framework.</p>\\n</div>\\n</section>\\n<section class=\"ltx_section\" id=\"Sx1\">\\n<h2 class=\"ltx_title ltx_title_section\">Acknowledgement</h2>\\n<div class=\"ltx_para ltx_noindent\" id=\"Sx1.p1\">\\n<p class=\"ltx_p\" id=\"Sx1.p1.1\">This project is partly supported by gifts from Anyscale, Astronomer, Google, IBM, Intel, Lacework, Microsoft, MBZUAI, Samsung SDS, Uber, and VMware. Lianmin Zheng is supported by a Meta Ph.D. Fellowship.\\nWe extend our thanks to Xinyang Geng, Hao Liu, Eric Wallace, Xuecheng Li, Tianyi Zhang, Qirong Ho, and Kevin Lin for their insightful discussions.</p>\\n</div>\\n</section>\\n<section class=\"ltx_bibliography\" id=\"bib\">\\n<h2 class=\"ltx_title ltx_title_bibliography\">References</h2>\\n<ul class=\"ltx_biblist\">\\n<li class=\"ltx_bibitem\" id=\"bib.bib1\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[1]</span>\\n<span class=\"ltx_bibblock\">\\nRohan Anil, Andrew\\xc2\\xa0M Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin,\\nAlexandre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen,\\net\\xc2\\xa0al.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Palm 2 technical report.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib1.1.1\">arXiv preprint arXiv:2305.10403</span>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib2\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[2]</span>\\n<span class=\"ltx_bibblock\">\\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma,\\nDawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et\\xc2\\xa0al.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Training a helpful and harmless assistant with reinforcement learning\\nfrom human feedback.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib2.1.1\">arXiv preprint arXiv:2204.05862</span>, 2022.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib3\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[3]</span>\\n<span class=\"ltx_bibblock\">\\nNiels\\xc2\\xa0J Blunch.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Position bias in multiple-choice questions.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib3.1.1\">Journal of Marketing Research</span>, 21(2):216\\xe2\\x80\\x93220, 1984.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib4\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[4]</span>\\n<span class=\"ltx_bibblock\">\\nJonathon\\xc2\\xa0D Brown.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Evaluations of self and others: Self-enhancement biases in social\\njudgments.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib4.1.1\">Social cognition</span>, 4(4):353\\xe2\\x80\\x93376, 1986.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib5\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[5]</span>\\n<span class=\"ltx_bibblock\">\\nS\\xc3\\xa9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric\\nHorvitz, Ece Kamar, Peter Lee, Yin\\xc2\\xa0Tat Lee, Yuanzhi Li, Scott Lundberg,\\net\\xc2\\xa0al.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Sparks of artificial general intelligence: Early experiments with\\ngpt-4.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib5.1.1\">arXiv preprint arXiv:2303.12712</span>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib6\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[6]</span>\\n<span class=\"ltx_bibblock\">\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de\\xc2\\xa0Oliveira\\nPinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg\\nBrockman, et\\xc2\\xa0al.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Evaluating large language models trained on code.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib6.1.1\">arXiv preprint arXiv:2107.03374</span>, 2021.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib7\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[7]</span>\\n<span class=\"ltx_bibblock\">\\nCheng-Han Chiang and Hung-yi Lee.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Can large language models be an alternative to human evaluations?\\n\\n</span>\\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib7.1.1\">arXiv preprint arXiv:2305.01937</span>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib8\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[8]</span>\\n<span class=\"ltx_bibblock\">\\nWei-Lin Chiang, Zhuohan Li, Zi\\xc2\\xa0Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin\\nZheng, Siyuan Zhuang, Yonghao Zhuang, Joseph\\xc2\\xa0E. Gonzalez, Ion Stoica, and\\nEric\\xc2\\xa0P. Xing.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt\\nquality, March 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib9\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[9]</span>\\n<span class=\"ltx_bibblock\">\\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa\\nSchoenick, and Oyvind Tafjord.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Think you have solved question answering? try arc, the ai2 reasoning\\nchallenge.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib9.1.1\">arXiv preprint arXiv:1803.05457</span>, 2018.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib10\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[10]</span>\\n<span class=\"ltx_bibblock\">\\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz\\nKaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano,\\net\\xc2\\xa0al.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Training verifiers to solve math word problems.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib10.1.1\">arXiv preprint arXiv:2110.14168</span>, 2021.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib11\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[11]</span>\\n<span class=\"ltx_bibblock\">\\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\\xc3\\xa9.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Flashattention: Fast and memory-efficient exact attention with\\nio-awareness.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib11.1.1\">Advances in Neural Information Processing Systems</span>,\\n35:16344\\xe2\\x80\\x9316359, 2022.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib12\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[12]</span>\\n<span class=\"ltx_bibblock\">\\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Qlora: Efficient finetuning of quantized llms.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib12.1.1\">arXiv preprint arXiv:2305.14314</span>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib13\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[13]</span>\\n<span class=\"ltx_bibblock\">\\nShizhe Diao, Rui Pan, Hanze Dong, Ka\\xc2\\xa0Shun Shum, Jipeng Zhang, Wei Xiong, and\\nTong Zhang.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Lmflow: An extensible toolkit for finetuning and inference of large\\nfoundation models.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib13.1.1\">arXiv preprint arXiv:2306.12420</span>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib14\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[14]</span>\\n<span class=\"ltx_bibblock\">\\nYann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba,\\nCarlos Guestrin, Percy Liang, and Tatsunori\\xc2\\xa0B Hashimoto.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Alpacafarm: A simulation framework for methods that learn from human\\nfeedback.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib14.1.1\">arXiv preprint arXiv:2305.14387</span>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib15\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[15]</span>\\n<span class=\"ltx_bibblock\">\\nJiazhan Feng, Qingfeng Sun, Can Xu, Pu\\xc2\\xa0Zhao, Yaming Yang, Chongyang Tao,\\nDongyan Zhao, and Qingwei Lin.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Mmdialog: A large-scale multi-turn dialogue dataset towards\\nmulti-modal open-domain conversation.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib15.1.1\">arXiv preprint arXiv:2211.05719</span>, 2022.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib16\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[16]</span>\\n<span class=\"ltx_bibblock\">\\nXinyang Geng, Arnav Gudibande, Hao Liu, Eric Wallace, Pieter Abbeel, Sergey\\nLevine, and Dawn Song.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Koala: A dialogue model for academic research.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Blog post, April 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib17\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[17]</span>\\n<span class=\"ltx_bibblock\">\\nFabrizio Gilardi, Meysam Alizadeh, and Ma\\xc3\\xabl Kubli.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Chatgpt outperforms crowd-workers for text-annotation tasks.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib17.1.1\">arXiv preprint arXiv:2303.15056</span>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib18\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[18]</span>\\n<span class=\"ltx_bibblock\">\\nArnav Gudibande, Eric Wallace, Charlie Snell, Xinyang Geng, Hao Liu, Pieter\\nAbbeel, Sergey Levine, and Dawn Song.\\n\\n</span>\\n<span class=\"ltx_bibblock\">The false promise of imitating proprietary llms.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib18.1.1\">arXiv preprint arXiv:2305.15717</span>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib19\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[19]</span>\\n<span class=\"ltx_bibblock\">\\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn\\nSong, and Jacob Steinhardt.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Measuring massive multitask language understanding.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib19.1.1\">arXiv preprint arXiv:2009.03300</span>, 2020.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib20\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[20]</span>\\n<span class=\"ltx_bibblock\">\\nFan Huang, Haewoon Kwak, and Jisun An.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Is chatgpt better than human annotators? potential and limitations of\\nchatgpt in explaining implicit hate speech.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib20.1.1\">arXiv preprint arXiv:2302.07736</span>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib21\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[21]</span>\\n<span class=\"ltx_bibblock\">\\nDouwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger,\\nZhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia,\\net\\xc2\\xa0al.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Dynabench: Rethinking benchmarking in nlp.\\n\\n</span>\\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib21.1.1\">Proceedings of the 2021 Conference of the North American\\nChapter of the Association for Computational Linguistics: Human Language\\nTechnologies</span>, pages 4110\\xe2\\x80\\x934124, 2021.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib22\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[22]</span>\\n<span class=\"ltx_bibblock\">\\nMiyoung Ko, Jinhyuk Lee, Hyunjae Kim, Gangwoo Kim, and Jaewoo Kang.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Look at the first sentence: Position bias in question answering.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib22.1.1\">arXiv preprint arXiv:2004.14602</span>, 2020.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib23\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[23]</span>\\n<span class=\"ltx_bibblock\">\\nAndreas K\\xc3\\xb6pf, Yannic Kilcher, Dimitri von R\\xc3\\xbctte, Sotiris Anagnostidis,\\nZhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen\\xc2\\xa0Minh Duc, Oliver\\nStanley, Rich\\xc3\\xa1rd Nagyfi, et\\xc2\\xa0al.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Openassistant conversations\\xe2\\x80\\x93democratizing large language model\\nalignment.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib23.1.1\">arXiv preprint arXiv:2304.07327</span>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib24\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[24]</span>\\n<span class=\"ltx_bibblock\">\\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu,\\nMichihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar,\\net\\xc2\\xa0al.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Holistic evaluation of language models.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib24.1.1\">arXiv preprint arXiv:2211.09110</span>, 2022.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib25\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[25]</span>\\n<span class=\"ltx_bibblock\">\\nChin-Yew Lin.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Rouge: A package for automatic evaluation of summaries.\\n\\n</span>\\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib25.1.1\">Text summarization branches out</span>, pages 74\\xe2\\x80\\x9381, 2004.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib26\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[26]</span>\\n<span class=\"ltx_bibblock\">\\nStephanie Lin, Jacob Hilton, and Owain Evans.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Truthfulqa: Measuring how models mimic human falsehoods.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib26.1.1\">arXiv preprint arXiv:2109.07958</span>, 2021.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib27\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[27]</span>\\n<span class=\"ltx_bibblock\">\\nShayne Longpre, Le\\xc2\\xa0Hou, Tu\\xc2\\xa0Vu, Albert Webson, Hyung\\xc2\\xa0Won Chung, Yi\\xc2\\xa0Tay, Denny\\nZhou, Quoc\\xc2\\xa0V Le, Barret Zoph, Jason Wei, et\\xc2\\xa0al.\\n\\n</span>\\n<span class=\"ltx_bibblock\">The flan collection: Designing data and methods for effective\\ninstruction tuning.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib27.1.1\">arXiv preprint arXiv:2301.13688</span>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib28\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[28]</span>\\n<span class=\"ltx_bibblock\">\\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Cross-task generalization via natural language crowdsourcing\\ninstructions.\\n\\n</span>\\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib28.1.1\">ACL</span>, 2022.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib29\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[29]</span>\\n<span class=\"ltx_bibblock\">\\nOpenAI.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Evals is a framework for evaluating llms and llm systems, and an\\nopen-source registry of benchmarks.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/openai/evals\" title=\"\">https://github.com/openai/evals</a>.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib30\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[30]</span>\\n<span class=\"ltx_bibblock\">\\nOpenAI.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Gpt-4 technical report, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib31\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[31]</span>\\n<span class=\"ltx_bibblock\">\\nLong Ouyang, Jeffrey Wu, Xu\\xc2\\xa0Jiang, Diogo Almeida, Carroll Wainwright, Pamela\\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et\\xc2\\xa0al.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Training language models to follow instructions with human feedback.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib31.1.1\">Advances in Neural Information Processing Systems</span>,\\n35:27730\\xe2\\x80\\x9327744, 2022.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib32\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[32]</span>\\n<span class=\"ltx_bibblock\">\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Bleu: a method for automatic evaluation of machine translation.\\n\\n</span>\\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib32.1.1\">Proceedings of the 40th annual meeting of the Association for\\nComputational Linguistics</span>, pages 311\\xe2\\x80\\x93318, 2002.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib33\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[33]</span>\\n<span class=\"ltx_bibblock\">\\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Instruction tuning with gpt-4.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib33.1.1\">arXiv preprint arXiv:2304.03277</span>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib34\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[34]</span>\\n<span class=\"ltx_bibblock\">\\nPriya Raghubir and Ana Valenzuela.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Center-of-inattention: Position biases in decision-making.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib34.1.1\">Organizational Behavior and Human Decision Processes</span>,\\n99(1):66\\xe2\\x80\\x9380, 2006.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib35\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[35]</span>\\n<span class=\"ltx_bibblock\">\\nSiva Reddy, Danqi Chen, and Christopher\\xc2\\xa0D Manning.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Coqa: A conversational question answering challenge.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib35.1.1\">Transactions of the Association for Computational Linguistics</span>,\\n7:249\\xe2\\x80\\x93266, 2019.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib36\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[36]</span>\\n<span class=\"ltx_bibblock\">\\nKeisuke Sakaguchi, Ronan\\xc2\\xa0Le Bras, Chandra Bhagavatula, and Yejin Choi.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Winogrande: An adversarial winograd schema challenge at scale.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib36.1.1\">Communications of the ACM</span>, 64(9):99\\xe2\\x80\\x93106, 2021.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib37\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[37]</span>\\n<span class=\"ltx_bibblock\">\\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal\\xc2\\xa0Md Shoeb, Abubakar\\nAbid, Adam Fisch, Adam\\xc2\\xa0R Brown, Adam Santoro, Aditya Gupta, Adri\\xc3\\xa0\\nGarriga-Alonso, et\\xc2\\xa0al.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Beyond the imitation game: Quantifying and extrapolating the\\ncapabilities of language models.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib37.1.1\">arXiv preprint arXiv:2206.04615</span>, 2022.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib38\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[38]</span>\\n<span class=\"ltx_bibblock\">\\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos\\nGuestrin, Percy Liang, and Tatsunori\\xc2\\xa0B. Hashimoto.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Stanford alpaca: An instruction-following llama model.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/tatsu-lab/stanford_alpaca\" title=\"\">https://github.com/tatsu-lab/stanford_alpaca</a>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib39\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[39]</span>\\n<span class=\"ltx_bibblock\">\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne\\nLachaux, Timoth\\xc3\\xa9e Lacroix, Baptiste Rozi\\xc3\\xa8re, Naman Goyal, Eric\\nHambro, Faisal Azhar, et\\xc2\\xa0al.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Llama: Open and efficient foundation language models.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib39.1.1\">arXiv preprint arXiv:2302.13971</span>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib40\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[40]</span>\\n<span class=\"ltx_bibblock\">\\nPeiyi Wang, Lei Li, Liang Chen, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi\\xc2\\xa0Liu,\\nTianyu Liu, and Zhifang Sui.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Large language models are not fair evaluators.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib40.1.1\">arXiv preprint arXiv:2305.17926</span>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib41\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[41]</span>\\n<span class=\"ltx_bibblock\">\\nXuanhui Wang, Nadav Golbandi, Michael Bendersky, Donald Metzler, and Marc\\nNajork.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Position bias estimation for unbiased learning to rank in personal\\nsearch.\\n\\n</span>\\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib41.1.1\">Proceedings of the Eleventh ACM International Conference on\\nWeb Search and Data Mining</span>, pages 610\\xe2\\x80\\x93618, 2018.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib42\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[42]</span>\\n<span class=\"ltx_bibblock\">\\nYidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen,\\nChaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, and Yue\\nZhang.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Pandalm: An automatic evaluation benchmark for llm instruction tuning\\noptimization, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib43\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[43]</span>\\n<span class=\"ltx_bibblock\">\\nYizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot,\\nKhyathi\\xc2\\xa0Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah\\xc2\\xa0A Smith,\\nIz\\xc2\\xa0Beltagy, et\\xc2\\xa0al.\\n\\n</span>\\n<span class=\"ltx_bibblock\">How far can camels go? exploring the state of instruction tuning on\\nopen resources.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib43.1.1\">arXiv preprint arXiv:2306.04751</span>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib44\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[44]</span>\\n<span class=\"ltx_bibblock\">\\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah\\xc2\\xa0A. Smith, Daniel\\nKhashabi, and Hannaneh Hajishirzi.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Self-instruct: Aligning language model with self generated\\ninstructions, 2022.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib45\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[45]</span>\\n<span class=\"ltx_bibblock\">\\nYizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza\\nMirzaei, Anjana Arunkumar, Arjun Ashok, Arut\\xc2\\xa0Selvan Dhanasekaran, Atharva\\nNaik, David Stap, et\\xc2\\xa0al.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Super-naturalinstructions:generalization via declarative instructions\\non 1600+ tasks.\\n\\n</span>\\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib45.1.1\">EMNLP</span>, 2022.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib46\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[46]</span>\\n<span class=\"ltx_bibblock\">\\nJason Wei, Maarten Bosma, Vincent\\xc2\\xa0Y Zhao, Kelvin Guu, Adams\\xc2\\xa0Wei Yu, Brian\\nLester, Nan Du, Andrew\\xc2\\xa0M Dai, and Quoc\\xc2\\xa0V Le.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Finetuned language models are zero-shot learners.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib46.1.1\">arXiv preprint arXiv:2109.01652</span>, 2021.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib47\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[47]</span>\\n<span class=\"ltx_bibblock\">\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed\\xc2\\xa0Chi, Quoc Le, and\\nDenny Zhou.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Chain of thought prompting elicits reasoning in large language\\nmodels.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib47.1.1\">arXiv preprint arXiv:2201.11903</span>, 2022.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib48\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[48]</span>\\n<span class=\"ltx_bibblock\">\\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu\\xc2\\xa0Zhao, Jiazhan Feng, Chongyang\\nTao, and Daxin Jiang.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Wizardlm: Empowering large language models to follow complex\\ninstructions.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib48.1.1\">arXiv preprint arXiv:2304.12244</span>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib49\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[49]</span>\\n<span class=\"ltx_bibblock\">\\nZongheng Yang, Zhanghao Wu, Michael Luo, Wei-Lin Chiang, Romil Bhardwaj, Woosuk\\nKwon, Siyuan Zhuang, Frank\\xc2\\xa0Sifei Luan, Gautam Mittal, Scott Shenker, and Ion\\nStoica.\\n\\n</span>\\n<span class=\"ltx_bibblock\">SkyPilot: An intercloud broker for sky computing.\\n\\n</span>\\n<span class=\"ltx_bibblock\">In <span class=\"ltx_text ltx_font_italic\" id=\"bib.bib49.1.1\">20th USENIX Symposium on Networked Systems Design and\\nImplementation (NSDI 23)</span>, pages 437\\xe2\\x80\\x93455, Boston, MA, April 2023. USENIX\\nAssociation.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib50\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[50]</span>\\n<span class=\"ltx_bibblock\">\\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Hellaswag: Can a machine really finish your sentence?\\n\\n</span>\\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib50.1.1\">arXiv preprint arXiv:1905.07830</span>, 2019.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib51\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[51]</span>\\n<span class=\"ltx_bibblock\">\\nWanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin\\nSaied, Weizhu Chen, and Nan Duan.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Agieval: A human-centric benchmark for evaluating foundation models.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib51.1.1\">arXiv preprint arXiv:2304.06364</span>, 2023.\\n\\n</span>\\n</li>\\n<li class=\"ltx_bibitem\" id=\"bib.bib52\">\\n<span class=\"ltx_tag ltx_tag_bibitem\">[52]</span>\\n<span class=\"ltx_bibblock\">\\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe\\nMa, Avia Efrat, Ping Yu, Lili Yu, et\\xc2\\xa0al.\\n\\n</span>\\n<span class=\"ltx_bibblock\">Lima: Less is more for alignment.\\n\\n</span>\\n<span class=\"ltx_bibblock\"><span class=\"ltx_text ltx_font_italic\" id=\"bib.bib52.1.1\">arXiv preprint arXiv:2305.11206</span>, 2023.\\n\\n</span>\\n</li>\\n</ul>\\n</section>\\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\\n<section class=\"ltx_appendix\" id=\"A1\">\\n<h2 class=\"ltx_title ltx_title_appendix\">\\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix A </span>Prompt templates</h2>\\n<div class=\"ltx_para ltx_noindent\" id=\"A1.p1\">\\n<p class=\"ltx_p\" id=\"A1.p1.1\">We list the prompt templates for LLM judges.\\nPlease refer to our github repository\\xc2\\xa0<span class=\"ltx_note ltx_role_footnote\" id=\"footnote2\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span><a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge\" title=\"\">https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge</a></span></span></span> for full details.</p>\\n</div>\\n<figure class=\"ltx_figure\" id=\"A1.F4\">\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"471\" id=\"A1.F4.g1\" src=\"x5.png\" width=\"830\"/>\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 4: </span>The default prompt for pairwise comparison.</figcaption>\\n</figure>\\n<figure class=\"ltx_figure\" id=\"A1.F5\">\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"296\" id=\"A1.F5.g1\" src=\"x6.png\" width=\"830\"/>\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 5: </span>The default prompt for single answer grading.</figcaption>\\n</figure>\\n<figure class=\"ltx_figure\" id=\"A1.F6\">\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"475\" id=\"A1.F6.g1\" src=\"x7.png\" width=\"830\"/>\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 6: </span>The chain-of-thought prompt for math and reasoning questions.</figcaption>\\n</figure>\\n<figure class=\"ltx_figure\" id=\"A1.F7\">\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"552\" id=\"A1.F7.g1\" src=\"x8.png\" width=\"830\"/>\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 7: </span>The prompt for reference-guided pairwise comparison.</figcaption>\\n</figure>\\n<figure class=\"ltx_figure\" id=\"A1.F8\">\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/>\\n<br class=\"ltx_break ltx_centering\"/><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"805\" id=\"A1.F8.g1\" src=\"x9.png\" width=\"830\"/>\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 8: </span>The prompt for multi-turn pairwise comparison.</figcaption>\\n</figure>\\n<figure class=\"ltx_figure\" id=\"A1.F9\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"762\" id=\"A1.F9.g1\" src=\"x10.png\" width=\"830\"/>\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 9: </span>The prompt for reference-guided multi-turn single-answer grading.</figcaption>\\n</figure>\\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\\n</section>\\n<section class=\"ltx_appendix\" id=\"A2\">\\n<h2 class=\"ltx_title ltx_title_appendix\">\\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix B </span>Case Study</h2>\\n<div class=\"ltx_para ltx_noindent\" id=\"A2.p1\">\\n<p class=\"ltx_p\" id=\"A2.p1.1\">We list several case studies. The examples are generated by <span class=\"ltx_text ltx_font_typewriter\" id=\"A2.p1.1.1\">gpt-4-0314</span>. They may not be fully reproducible with future GPT-4 versions.</p>\\n</div>\\n<figure class=\"ltx_figure\" id=\"A2.F10\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"926\" id=\"A2.F10.g1\" src=\"x11.png\" width=\"830\"/>\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 10: </span>An example of position bias. When Assistant A is placed in the first position, GPT-4 thinks A is better, but its verdict changes when we swap the position of A and B. We observe similar pattern from other LLM judges such as Claude/GPT-3.5.</figcaption>\\n</figure>\\n<figure class=\"ltx_figure\" id=\"A2.F11\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"841\" id=\"A2.F11.g1\" src=\"x12.png\" width=\"830\"/>\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 11: </span>An example of \\xe2\\x80\\x9crepetitive list\\xe2\\x80\\x9d attack to examine verbosity bias. Except for the two rephrased items (highlighted in red), Assistant A\\xe2\\x80\\x99s answer is exactly the same as Assistant B. Both GPT-3.5 and Claude-v1 show a verbosity bias towards the longer and repetitive answer. Only GPT-4 successfully detected this attack.</figcaption>\\n</figure>\\n<figure class=\"ltx_figure\" id=\"A2.F12\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"424\" id=\"A2.F12.g1\" src=\"x13.png\" width=\"830\"/>\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 12: </span>With a default prompt, GPT-4 shows limited capability in grading math questions. Despite being able to answer the question itself, its judgment was influenced by the given answers, leading to arithmetic mistakes highlighted in yellow.</figcaption>\\n</figure>\\n<figure class=\"ltx_figure\" id=\"A2.F13\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"595\" id=\"A2.F13.g1\" src=\"x14.png\" width=\"830\"/>\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 13: </span>An example of GPT-4\\xe2\\x80\\x99s limited capability in grading reasoning question. Despite GPT-4 knows how to solve the question (if asked separately), it made a wrong judgement saying both assistants\\xe2\\x80\\x99 wrong answers are correct.</figcaption>\\n</figure>\\n<figure class=\"ltx_figure\" id=\"A2.F14\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"909\" id=\"A2.F14.g1\" src=\"x15.png\" width=\"830\"/>\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 14: </span>An example of GPT-4\\xe2\\x80\\x99s wrong judgment with chain-of-thought prompt. We can see GPT-4 exactly copied Assistant B\\xe2\\x80\\x99s answer (which contains arithmetic errors) and determined Assistant A\\xe2\\x80\\x99s answer is incorrect. This suggest GPT-4\\xe2\\x80\\x99s chain-of-thought process can be significantly influenced by the given answers despite we ask it to think independently.</figcaption>\\n</figure>\\n<figure class=\"ltx_figure\" id=\"A2.F15\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_square\" height=\"737\" id=\"A2.F15.g1\" src=\"x16.png\" width=\"830\"/>\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 15: </span>In this example, despite Assistant A correctly followed user\\xe2\\x80\\x99s instruction to generate a concrete plan for the second example of its previous response, GPT-4 wrongly referred to the second example in Assistant B\\xe2\\x80\\x99s response, resulting in a wrong judgment. This suggests the prompt design that breaks the questions into two prompts may cause LLM judge struggle to locate assistants\\xe2\\x80\\x99 previous responses.</figcaption>\\n</figure>\\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\\n</section>\\n<section class=\"ltx_appendix\" id=\"A3\">\\n<h2 class=\"ltx_title ltx_title_appendix\">\\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix C </span>Data Collection</h2>\\n<div class=\"ltx_para ltx_noindent\" id=\"A3.p1\">\\n<p class=\"ltx_p\" id=\"A3.p1.1\">We describe our data collection process for both MT-bench and Chatbot Arena.</p>\\n</div>\\n<section class=\"ltx_subsection\" id=\"A3.SS1\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\">C.1 </span>MT-bench human evaluation</h3>\\n<div class=\"ltx_para ltx_noindent\" id=\"A3.SS1.p1\">\\n<p class=\"ltx_p\" id=\"A3.SS1.p1.1\"><a class=\"ltx_ref\" href=\"#A3.F16\" title=\"Figure 16 \\xe2\\x80\\xa3 C.1 MT-bench human evaluation \\xe2\\x80\\xa3 Appendix C Data Collection \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">16</span></a> shows the normal voting interface. <a class=\"ltx_ref\" href=\"#A3.F17\" title=\"Figure 17 \\xe2\\x80\\xa3 C.1 MT-bench human evaluation \\xe2\\x80\\xa3 Appendix C Data Collection \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">17</span></a> shows that we additionally show GPT-4\\xe2\\x80\\x99s judgment to users and ask if it is reasonable when a human differs from GPT-4.</p>\\n</div>\\n<figure class=\"ltx_figure\" id=\"A3.F16\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"300\" id=\"A3.F16.g1\" src=\"extracted/5314631/figures/screenshot_mt_bench.png\" width=\"598\"/>\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 16: </span>The screenshot of MT-bench data collection. We show an instruction similar to the prompt we give to GPT-4. We present questions from MT-bench and answers from two random anonymous assistants and ask which one is better. We present the first-turn conversation and ask humans to vote, then repeat the same procedure for the second-turn. A user can skip up to 5 questions if they are not confident. For some questions (e.g., math, reasoning), they can also see a reference solution.</figcaption>\\n</figure>\\n<figure class=\"ltx_figure\" id=\"A3.F17\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"345\" id=\"A3.F17.g1\" src=\"extracted/5314631/figures/screenshot_mt_bench_agree.png\" width=\"598\"/>\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 17: </span>The screenshot of MT-bench data collection. When human\\xe2\\x80\\x99s vote differs from GPT-4, we additionally show GPT-4\\xe2\\x80\\x99s judgment (red region in the screenshot) and ask the user to click one of the three buttons to decide whether GPT-4\\xe2\\x80\\x99s judgment is reasonable.</figcaption>\\n</figure>\\n<div class=\"ltx_para ltx_noindent\" id=\"A3.SS1.p2\">\\n<p class=\"ltx_p\" id=\"A3.SS1.p2.1\">To invite participants, we obtained their consent by letting them sign an application form. We pay them $20 for judging 20 questions, which corresponds to an hourly rate of around $35. The participants are mostly graduate students from more than ten universities.</p>\\n</div>\\n</section>\\n<section class=\"ltx_subsection\" id=\"A3.SS2\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\">C.2 </span>Chatbot Arena</h3>\\n<div class=\"ltx_para ltx_noindent\" id=\"A3.SS2.p1\">\\n<p class=\"ltx_p\" id=\"A3.SS2.p1.1\"><a class=\"ltx_ref\" href=\"#A3.F18\" title=\"Figure 18 \\xe2\\x80\\xa3 C.2 Chatbot Arena \\xe2\\x80\\xa3 Appendix C Data Collection \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">18</span></a> shows a screenshot of Chatbot Arena. Users are required to accept the terms of use, which obtain their consent and give us the right to release the conversation data.\\nThe instructions are shown at the top of the interface. This is a free website. We do not pay users and any user can use this platform without registration. More introductions and analyses can be found at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://lmsys.org/blog/2023-05-03-arena/\" title=\"\">https://lmsys.org/blog/2023-05-03-arena/</a>.</p>\\n</div>\\n<figure class=\"ltx_figure\" id=\"A3.F18\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"468\" id=\"A3.F18.g1\" src=\"extracted/5314631/figures/screenshot_arena.png\" width=\"598\"/>\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 18: </span>The screenshot of Chatbot Arena.</figcaption>\\n</figure>\\n</section>\\n<section class=\"ltx_subsection\" id=\"A3.SS3\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\">C.3 </span>Data Release</h3>\\n<div class=\"ltx_para ltx_noindent\" id=\"A3.SS3.p1\">\\n<p class=\"ltx_p\" id=\"A3.SS3.p1.1\">We will clean the Personal Identifiable Information (PII) and tag toxic conversations with OpenAI moderation APIs for our dataset release.</p>\\n</div>\\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\\n</section>\\n</section>\\n<section class=\"ltx_appendix\" id=\"A4\">\\n<h2 class=\"ltx_title ltx_title_appendix\">\\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix D </span>Additional Experimental Results</h2>\\n<div class=\"ltx_para ltx_noindent\" id=\"A4.p1\">\\n<p class=\"ltx_p\" id=\"A4.p1.1\">We present some additional experimental results.</p>\\n</div>\\n<section class=\"ltx_subsection\" id=\"A4.SS1\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\">D.1 </span>Position bias</h3>\\n<div class=\"ltx_para ltx_noindent\" id=\"A4.SS1.p1\">\\n<p class=\"ltx_p\" id=\"A4.SS1.p1.1\">We test two more prompts and present the full results in <a class=\"ltx_ref\" href=\"#A4.T10\" title=\"Table 10 \\xe2\\x80\\xa3 D.1 Position bias \\xe2\\x80\\xa3 Appendix D Additional Experimental Results \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Table</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">10</span></a>\\n\\xe2\\x80\\x9cscore\\xe2\\x80\\x9d changes the default prompt to let the model output two absolute scores instead of which one is better.\\n\\xe2\\x80\\x9cshort\\xe2\\x80\\x9d is a simplified version of our default prompt by removing instructions like \\xe2\\x80\\x9cAvoid any position bias..\\xe2\\x80\\x9d, \\xe2\\x80\\x9cBegin your evaluation \\xe2\\x80\\xa6 and provide a short explanation\\xe2\\x80\\x9d.\\nWe can find different prompts have different effects on different models.\\nFor example, the \"score\" prompt can increase the consistency of GPT-3.5 but decreases it for Claude-v1 and GPT-4.</p>\\n</div>\\n<figure class=\"ltx_table\" id=\"A4.T10\">\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 10: </span>Position bias on different models and prompts. Consistency is the percentage of cases where a judge gives consistent results when swapping the order of two assistants. \\xe2\\x80\\x9cBiased toward first\\xe2\\x80\\x9d is the percentage of cases when a judge favors the first answer. \\xe2\\x80\\x9cError\\xe2\\x80\\x9d indicates wrong output formats. The two largest numbers in each column are in bold.</figcaption>\\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A4.T10.1\">\\n<thead class=\"ltx_thead\">\\n<tr class=\"ltx_tr\" id=\"A4.T10.1.1.1\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A4.T10.1.1.1.1\">Judge</th>\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A4.T10.1.1.1.2\">Prompt</th>\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A4.T10.1.1.1.3\">Consistency</th>\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A4.T10.1.1.1.4\">Biased toward first</th>\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A4.T10.1.1.1.5\">Biased toward second</th>\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A4.T10.1.1.1.6\">Error</th>\\n</tr>\\n</thead>\\n<tbody class=\"ltx_tbody\">\\n<tr class=\"ltx_tr\" id=\"A4.T10.1.2.1\">\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T10.1.2.1.1\" rowspan=\"4\"><span class=\"ltx_text\" id=\"A4.T10.1.2.1.1.1\">claude-v1</span></td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T10.1.2.1.2\">default</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T10.1.2.1.3\">23.8%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T10.1.2.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A4.T10.1.2.1.4.1\">75.0%</span></td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T10.1.2.1.5\">0.0%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T10.1.2.1.6\">1.2%</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A4.T10.1.3.2\">\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.3.2.1\">rename</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.3.2.2\">56.2%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.3.2.3\">11.2%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.3.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A4.T10.1.3.2.4.1\">28.7%</span></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.3.2.5\"><span class=\"ltx_text ltx_font_bold\" id=\"A4.T10.1.3.2.5.1\">3.8%</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A4.T10.1.4.3\">\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.4.3.1\">score</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.4.3.2\">20.0%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.4.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A4.T10.1.4.3.3.1\">80.0%</span></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.4.3.4\">0.0%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.4.3.5\">0.0%</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A4.T10.1.5.4\">\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.5.4.1\">short</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.5.4.2\">22.5%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.5.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A4.T10.1.5.4.3.1\">75.0%</span></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.5.4.4\">2.5%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.5.4.5\">0.0%</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A4.T10.1.6.5\">\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T10.1.6.5.1\" rowspan=\"4\"><span class=\"ltx_text\" id=\"A4.T10.1.6.5.1.1\">gpt-3.5-turbo</span></td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T10.1.6.5.2\">default</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T10.1.6.5.3\">46.2%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T10.1.6.5.4\">50.0%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T10.1.6.5.5\">1.2%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T10.1.6.5.6\">2.5%</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A4.T10.1.7.6\">\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.7.6.1\">rename</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.7.6.2\">51.2%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.7.6.3\">38.8%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.7.6.4\">6.2%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.7.6.5\"><span class=\"ltx_text ltx_font_bold\" id=\"A4.T10.1.7.6.5.1\">3.8%</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A4.T10.1.8.7\">\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.8.7.1\">score</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.8.7.2\">55.0%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.8.7.3\">33.8%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.8.7.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A4.T10.1.8.7.4.1\">11.2%</span></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.8.7.5\">0.0%</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A4.T10.1.9.8\">\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.9.8.1\">short</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.9.8.2\">38.8%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.9.8.3\">57.5%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.9.8.4\">3.8%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.9.8.5\">0.0%</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A4.T10.1.10.9\">\\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A4.T10.1.10.9.1\" rowspan=\"4\"><span class=\"ltx_text\" id=\"A4.T10.1.10.9.1.1\">gpt-4</span></td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T10.1.10.9.2\">default</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T10.1.10.9.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A4.T10.1.10.9.3.1\">65.0%</span></td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T10.1.10.9.4\">30.0%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T10.1.10.9.5\">5.0%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T10.1.10.9.6\">0.0%</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A4.T10.1.11.10\">\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.11.10.1\">rename</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.11.10.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A4.T10.1.11.10.2.1\">66.2%</span></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.11.10.3\">28.7%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.11.10.4\">5.0%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.11.10.5\">0.0%</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A4.T10.1.12.11\">\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.12.11.1\">score</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.12.11.2\">51.2%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.12.11.3\">46.2%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.12.11.4\">2.5%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T10.1.12.11.5\">0.0%</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A4.T10.1.13.12\">\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A4.T10.1.13.12.1\">short</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A4.T10.1.13.12.2\">62.5%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A4.T10.1.13.12.3\">35.0%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A4.T10.1.13.12.4\">2.5%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A4.T10.1.13.12.5\">0.0%</td>\\n</tr>\\n</tbody>\\n</table>\\n</figure>\\n<div class=\"ltx_para ltx_noindent\" id=\"A4.SS1.p2\">\\n<p class=\"ltx_p\" id=\"A4.SS1.p2.1\">As shown in <a class=\"ltx_ref\" href=\"#A4.T11\" title=\"Table 11 \\xe2\\x80\\xa3 D.1 Position bias \\xe2\\x80\\xa3 Appendix D Additional Experimental Results \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Table</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">11</span></a>, position bias is more noticeable on open questions like writing and stem/humanity knowledge questions.\\nOn math and coding questions, LLM judges are more confident even though their judgments can often be wrong, as we show in <a class=\"ltx_ref\" href=\"#S3.SS3\" title=\"3.3 Limitations of LLM-as-a-Judge \\xe2\\x80\\xa3 3 LLM as a Judge \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Section</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">3.3</span></a>. Finally, we study how the model pairs influence position bias by using GPT-4 and the default prompt to judge three different model pairs. As shown in <a class=\"ltx_ref\" href=\"#A4.T12\" title=\"Table 12 \\xe2\\x80\\xa3 D.1 Position bias \\xe2\\x80\\xa3 Appendix D Additional Experimental Results \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Table</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">12</span></a>, the position bias is more noticeable for models with close performance and can almost disappear when the performance of the two models differs a lot.</p>\\n</div>\\n<figure class=\"ltx_table\" id=\"A4.T11\">\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 11: </span>Position bias on different categories. The two largest numbers in each column are in bold.</figcaption>\\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A4.T11.1\">\\n<thead class=\"ltx_thead\">\\n<tr class=\"ltx_tr\" id=\"A4.T11.1.1.1\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A4.T11.1.1.1.1\">Category</th>\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A4.T11.1.1.1.2\">Consistent</th>\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A4.T11.1.1.1.3\">Biased toward first</th>\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A4.T11.1.1.1.4\">Biased toward second</th>\\n</tr>\\n</thead>\\n<tbody class=\"ltx_tbody\">\\n<tr class=\"ltx_tr\" id=\"A4.T11.1.2.1\">\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T11.1.2.1.1\">writing</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T11.1.2.1.2\">42.0%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T11.1.2.1.3\">46.0%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T11.1.2.1.4\">12.0%</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A4.T11.1.3.2\">\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T11.1.3.2.1\">roleplay</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T11.1.3.2.2\">68.0%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T11.1.3.2.3\">30.0%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T11.1.3.2.4\">2.0%</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A4.T11.1.4.3\">\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T11.1.4.3.1\">reasoning</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T11.1.4.3.2\">76.0%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T11.1.4.3.3\">20.0%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T11.1.4.3.4\">4.0%</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A4.T11.1.5.4\">\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T11.1.5.4.1\">math</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T11.1.5.4.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A4.T11.1.5.4.2.1\">86.0%</span></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T11.1.5.4.3\">4.0%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T11.1.5.4.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A4.T11.1.5.4.4.1\">10.0%</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A4.T11.1.6.5\">\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T11.1.6.5.1\">coding</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T11.1.6.5.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A4.T11.1.6.5.2.1\">86.0%</span></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T11.1.6.5.3\">14.0%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T11.1.6.5.4\">0.0%</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A4.T11.1.7.6\">\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T11.1.7.6.1\">extraction</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T11.1.7.6.2\">78.0%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T11.1.7.6.3\">12.0%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T11.1.7.6.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A4.T11.1.7.6.4.1\">10.0%</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A4.T11.1.8.7\">\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T11.1.8.7.1\">stem</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T11.1.8.7.2\">44.0%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T11.1.8.7.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A4.T11.1.8.7.3.1\">54.0%</span></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T11.1.8.7.4\">2.0%</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A4.T11.1.9.8\">\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A4.T11.1.9.8.1\">humanities</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A4.T11.1.9.8.2\">36.0%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A4.T11.1.9.8.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A4.T11.1.9.8.3.1\">60.0%</span></td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A4.T11.1.9.8.4\">4.0%</td>\\n</tr>\\n</tbody>\\n</table>\\n</figure>\\n<figure class=\"ltx_table\" id=\"A4.T12\">\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 12: </span>Position bias on different model pairs.</figcaption>\\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A4.T12.1\">\\n<thead class=\"ltx_thead\">\\n<tr class=\"ltx_tr\" id=\"A4.T12.1.1.1\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A4.T12.1.1.1.1\">Pair</th>\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A4.T12.1.1.1.2\">Consistent</th>\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A4.T12.1.1.1.3\">Biased toward first</th>\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A4.T12.1.1.1.4\">Biased toward second</th>\\n</tr>\\n</thead>\\n<tbody class=\"ltx_tbody\">\\n<tr class=\"ltx_tr\" id=\"A4.T12.1.2.1\">\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T12.1.2.1.1\">GPT-3.5 vs Claude-V1</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T12.1.2.1.2\">67.5%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T12.1.2.1.3\">23.8%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T12.1.2.1.4\">8.8%</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A4.T12.1.3.2\">\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T12.1.3.2.1\">GPT-3.5 vs Vicuna-13B</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T12.1.3.2.2\">73.8%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T12.1.3.2.3\">23.8%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T12.1.3.2.4\">2.5%</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A4.T12.1.4.3\">\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A4.T12.1.4.3.1\">GPT-3.5 vs LLaMA-13B</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A4.T12.1.4.3.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A4.T12.1.4.3.2.1\">98.8%</span></td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A4.T12.1.4.3.3\">1.2%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A4.T12.1.4.3.4\">0.0%</td>\\n</tr>\\n</tbody>\\n</table>\\n</figure>\\n</section>\\n<section class=\"ltx_subsection\" id=\"A4.SS2\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\">D.2 </span>Few-shot judge</h3>\\n<div class=\"ltx_para ltx_noindent\" id=\"A4.SS2.p1\">\\n<p class=\"ltx_p\" id=\"A4.SS2.p1.1\">We examine how few-shot examples improve LLM judges. As shown in <a class=\"ltx_ref\" href=\"#A4.T13\" title=\"Table 13 \\xe2\\x80\\xa3 D.2 Few-shot judge \\xe2\\x80\\xa3 Appendix D Additional Experimental Results \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Table</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">13</span></a>, they improve the consistency of all three LLM judges significantly. It almost alleviates the position bias of GPT-4, but moves the position bias of GPT-3.5 from the first position to the second position. We then measure the agreement between few-shot GPT-4 pairwise comparison and humans on MT-bench, but found it performs similarly to zero-shot GPT-4 pairwise comparison.</p>\\n</div>\\n<figure class=\"ltx_table\" id=\"A4.T13\">\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 13: </span>Improvements of the few-shot judge on consistency for position bias.</figcaption>\\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A4.T13.1\">\\n<thead class=\"ltx_thead\">\\n<tr class=\"ltx_tr\" id=\"A4.T13.1.1.1\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A4.T13.1.1.1.1\">Model</th>\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A4.T13.1.1.1.2\">Prompt</th>\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A4.T13.1.1.1.3\">Consistency</th>\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A4.T13.1.1.1.4\">Biased toward first</th>\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A4.T13.1.1.1.5\">Biased toward second</th>\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A4.T13.1.1.1.6\">Error</th>\\n</tr>\\n</thead>\\n<tbody class=\"ltx_tbody\">\\n<tr class=\"ltx_tr\" id=\"A4.T13.1.2.1\">\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T13.1.2.1.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"A4.T13.1.2.1.1.1\">Claude-v1</span></td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T13.1.2.1.2\">zero-shot</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T13.1.2.1.3\">23.8%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T13.1.2.1.4\">75.0%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T13.1.2.1.5\">0.0%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T13.1.2.1.6\">1.2%</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A4.T13.1.3.2\">\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T13.1.3.2.1\">few-shot</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T13.1.3.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A4.T13.1.3.2.2.1\">63.7%</span></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T13.1.3.2.3\">21.2%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T13.1.3.2.4\">11.2%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T13.1.3.2.5\">3.8%</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A4.T13.1.4.3\">\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T13.1.4.3.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"A4.T13.1.4.3.1.1\">GPT-3.5</span></td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T13.1.4.3.2\">zero-shot</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T13.1.4.3.3\">46.2%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T13.1.4.3.4\">50.0%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T13.1.4.3.5\">1.2%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T13.1.4.3.6\">2.5%</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A4.T13.1.5.4\">\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T13.1.5.4.1\">few-shot</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T13.1.5.4.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A4.T13.1.5.4.2.1\">55.0%</span></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T13.1.5.4.3\">16.2%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T13.1.5.4.4\">28.7%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A4.T13.1.5.4.5\">0.0%</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A4.T13.1.6.5\">\\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A4.T13.1.6.5.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"A4.T13.1.6.5.1.1\">GPT-4</span></td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T13.1.6.5.2\">zero-shot</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T13.1.6.5.3\">65.0%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T13.1.6.5.4\">30.0%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T13.1.6.5.5\">5.0%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A4.T13.1.6.5.6\">0.0%</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A4.T13.1.7.6\">\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A4.T13.1.7.6.1\">few-shot</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A4.T13.1.7.6.2\"><span class=\"ltx_text ltx_font_bold\" id=\"A4.T13.1.7.6.2.1\">77.5%</span></td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A4.T13.1.7.6.3\">10.0%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A4.T13.1.7.6.4\">12.5%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A4.T13.1.7.6.5\">0.0%</td>\\n</tr>\\n</tbody>\\n</table>\\n</figure>\\n</section>\\n<section class=\"ltx_subsection\" id=\"A4.SS3\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\">D.3 </span>Agreement Evaluation</h3>\\n<div class=\"ltx_para ltx_noindent\" id=\"A4.SS3.p1\">\\n<p class=\"ltx_p\" id=\"A4.SS3.p1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A4.SS3.p1.1.1\">Agreement calculation.</span>\\nWe define the agreement between two types of judges as the probability of randomly selected individuals (but not identical) of each type agreeing on a randomly selected question.\\nFor example, if we are comparing GPT-4 and Claude, the agreement is the probability of GPT-4 and Claude agreeing on the vote for a randomly selected question.\\nIf we are comparing GPT-4 and humans, the agreement is the probability of GPT-4 and a randomly selected human agreeing on the vote for a randomly selected question.\\nThe agreement among humans themselves is the probability of two randomly selected but not identical humans agreeing on the vote for a randomly selected question.</p>\\n</div>\\n<div class=\"ltx_para ltx_noindent\" id=\"A4.SS3.p2\">\\n<p class=\"ltx_p\" id=\"A4.SS3.p2.3\">Note that the agreement among humans could be a lower estimation compared to the agreement of GPT4 and humans.\\nConsider three humans who voted \\xe2\\x80\\x9cA\\xe2\\x80\\x9d, \\xe2\\x80\\x9cA\\xe2\\x80\\x9d, and \\xe2\\x80\\x9cB\\xe2\\x80\\x9d for a question, respectively.\\nThe agreement among them is only <math alttext=\"\\\\frac{1}{3}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS3.p2.1.m1.1\"><semantics id=\"A4.SS3.p2.1.m1.1a\"><mfrac id=\"A4.SS3.p2.1.m1.1.1\" xref=\"A4.SS3.p2.1.m1.1.1.cmml\"><mn id=\"A4.SS3.p2.1.m1.1.1.2\" xref=\"A4.SS3.p2.1.m1.1.1.2.cmml\">1</mn><mn id=\"A4.SS3.p2.1.m1.1.1.3\" xref=\"A4.SS3.p2.1.m1.1.1.3.cmml\">3</mn></mfrac><annotation-xml encoding=\"MathML-Content\" id=\"A4.SS3.p2.1.m1.1b\"><apply id=\"A4.SS3.p2.1.m1.1.1.cmml\" xref=\"A4.SS3.p2.1.m1.1.1\"><divide id=\"A4.SS3.p2.1.m1.1.1.1.cmml\" xref=\"A4.SS3.p2.1.m1.1.1\"></divide><cn id=\"A4.SS3.p2.1.m1.1.1.2.cmml\" type=\"integer\" xref=\"A4.SS3.p2.1.m1.1.1.2\">1</cn><cn id=\"A4.SS3.p2.1.m1.1.1.3.cmml\" type=\"integer\" xref=\"A4.SS3.p2.1.m1.1.1.3\">3</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.SS3.p2.1.m1.1c\">\\\\frac{1}{3}</annotation><annotation encoding=\"application/x-llamapun\" id=\"A4.SS3.p2.1.m1.1d\">divide start_ARG 1 end_ARG start_ARG 3 end_ARG</annotation></semantics></math>, as there are three pairs \\xe2\\x80\\x9c(A, A)\\xe2\\x80\\x9d, \\xe2\\x80\\x9c(A, B)\\xe2\\x80\\x9d, and \\xe2\\x80\\x9c(A, B)\\xe2\\x80\\x9d.\\nBut the agreement between GPT4 and those three is <math alttext=\"\\\\frac{2}{3}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS3.p2.2.m2.1\"><semantics id=\"A4.SS3.p2.2.m2.1a\"><mfrac id=\"A4.SS3.p2.2.m2.1.1\" xref=\"A4.SS3.p2.2.m2.1.1.cmml\"><mn id=\"A4.SS3.p2.2.m2.1.1.2\" xref=\"A4.SS3.p2.2.m2.1.1.2.cmml\">2</mn><mn id=\"A4.SS3.p2.2.m2.1.1.3\" xref=\"A4.SS3.p2.2.m2.1.1.3.cmml\">3</mn></mfrac><annotation-xml encoding=\"MathML-Content\" id=\"A4.SS3.p2.2.m2.1b\"><apply id=\"A4.SS3.p2.2.m2.1.1.cmml\" xref=\"A4.SS3.p2.2.m2.1.1\"><divide id=\"A4.SS3.p2.2.m2.1.1.1.cmml\" xref=\"A4.SS3.p2.2.m2.1.1\"></divide><cn id=\"A4.SS3.p2.2.m2.1.1.2.cmml\" type=\"integer\" xref=\"A4.SS3.p2.2.m2.1.1.2\">2</cn><cn id=\"A4.SS3.p2.2.m2.1.1.3.cmml\" type=\"integer\" xref=\"A4.SS3.p2.2.m2.1.1.3\">3</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.SS3.p2.2.m2.1c\">\\\\frac{2}{3}</annotation><annotation encoding=\"application/x-llamapun\" id=\"A4.SS3.p2.2.m2.1d\">divide start_ARG 2 end_ARG start_ARG 3 end_ARG</annotation></semantics></math> if GPT4 voted \\xe2\\x80\\x9cfirst\\xe2\\x80\\x9d and <math alttext=\"\\\\frac{1}{3}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS3.p2.3.m3.1\"><semantics id=\"A4.SS3.p2.3.m3.1a\"><mfrac id=\"A4.SS3.p2.3.m3.1.1\" xref=\"A4.SS3.p2.3.m3.1.1.cmml\"><mn id=\"A4.SS3.p2.3.m3.1.1.2\" xref=\"A4.SS3.p2.3.m3.1.1.2.cmml\">1</mn><mn id=\"A4.SS3.p2.3.m3.1.1.3\" xref=\"A4.SS3.p2.3.m3.1.1.3.cmml\">3</mn></mfrac><annotation-xml encoding=\"MathML-Content\" id=\"A4.SS3.p2.3.m3.1b\"><apply id=\"A4.SS3.p2.3.m3.1.1.cmml\" xref=\"A4.SS3.p2.3.m3.1.1\"><divide id=\"A4.SS3.p2.3.m3.1.1.1.cmml\" xref=\"A4.SS3.p2.3.m3.1.1\"></divide><cn id=\"A4.SS3.p2.3.m3.1.1.2.cmml\" type=\"integer\" xref=\"A4.SS3.p2.3.m3.1.1.2\">1</cn><cn id=\"A4.SS3.p2.3.m3.1.1.3.cmml\" type=\"integer\" xref=\"A4.SS3.p2.3.m3.1.1.3\">3</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.SS3.p2.3.m3.1c\">\\\\frac{1}{3}</annotation><annotation encoding=\"application/x-llamapun\" id=\"A4.SS3.p2.3.m3.1d\">divide start_ARG 1 end_ARG start_ARG 3 end_ARG</annotation></semantics></math> otherwise.</p>\\n</div>\\n<div class=\"ltx_para ltx_noindent\" id=\"A4.SS3.p3\">\\n<p class=\"ltx_p\" id=\"A4.SS3.p3.1\">Therefore, to have a more comprehensive understanding of what happened, we introduce a new judge type called human-majority, which considers the majority of human votes for each question.\\nThe agreement between GPT4 and human-majority is then calculated as the probability of GPT4 agreeing with the majority of human votes on a randomly selected question.\\n<span class=\"ltx_text ltx_font_italic\" id=\"A4.SS3.p3.1.1\">The upper bound of the agreement between GPT-4 and humans is the agreement between human-majority and human.</span>\\nWhen there is no majority vote for a question, the agreement is counted by an even split.\\nFor example, if there are an equal number of \\xe2\\x80\\x9cA\\xe2\\x80\\x9d and \\xe2\\x80\\x9cB\\xe2\\x80\\x9d human votes for a question, and GPT4 votes \\xe2\\x80\\x9cA\\xe2\\x80\\x9d, the agreement is counted as <math alttext=\"\\\\frac{1}{2}\" class=\"ltx_Math\" display=\"inline\" id=\"A4.SS3.p3.1.m1.1\"><semantics id=\"A4.SS3.p3.1.m1.1a\"><mfrac id=\"A4.SS3.p3.1.m1.1.1\" xref=\"A4.SS3.p3.1.m1.1.1.cmml\"><mn id=\"A4.SS3.p3.1.m1.1.1.2\" xref=\"A4.SS3.p3.1.m1.1.1.2.cmml\">1</mn><mn id=\"A4.SS3.p3.1.m1.1.1.3\" xref=\"A4.SS3.p3.1.m1.1.1.3.cmml\">2</mn></mfrac><annotation-xml encoding=\"MathML-Content\" id=\"A4.SS3.p3.1.m1.1b\"><apply id=\"A4.SS3.p3.1.m1.1.1.cmml\" xref=\"A4.SS3.p3.1.m1.1.1\"><divide id=\"A4.SS3.p3.1.m1.1.1.1.cmml\" xref=\"A4.SS3.p3.1.m1.1.1\"></divide><cn id=\"A4.SS3.p3.1.m1.1.1.2.cmml\" type=\"integer\" xref=\"A4.SS3.p3.1.m1.1.1.2\">1</cn><cn id=\"A4.SS3.p3.1.m1.1.1.3.cmml\" type=\"integer\" xref=\"A4.SS3.p3.1.m1.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.SS3.p3.1.m1.1c\">\\\\frac{1}{2}</annotation><annotation encoding=\"application/x-llamapun\" id=\"A4.SS3.p3.1.m1.1d\">divide start_ARG 1 end_ARG start_ARG 2 end_ARG</annotation></semantics></math> on this question.</p>\\n</div>\\n<div class=\"ltx_para ltx_noindent\" id=\"A4.SS3.p4\">\\n<p class=\"ltx_p\" id=\"A4.SS3.p4.1\"><span class=\"ltx_text ltx_font_bold\" id=\"A4.SS3.p4.1.1\">More results.</span> <a class=\"ltx_ref\" href=\"#A4.T14.st2\" title=\"14(b) \\xe2\\x80\\xa3 Table 14 \\xe2\\x80\\xa3 D.3 Agreement Evaluation \\xe2\\x80\\xa3 Appendix D Additional Experimental Results \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Table</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">14(b)</span></a> shows more agreement results on MT-bench. In addition to expert labelers (denoted as \\xe2\\x80\\x9cHuman\\xe2\\x80\\x9d), we also include author votes (denoted as \\xe2\\x80\\x9cAuthor\\xe2\\x80\\x9d).</p>\\n</div>\\n<figure class=\"ltx_table\" id=\"A4.T14\">\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 14: </span>Agreement between two types of judges on MT-bench. \\xe2\\x80\\x9cG4-P\\xe2\\x80\\x9d and \\xe2\\x80\\x9cG4-S\\xe2\\x80\\x9d denote GPT-4 with pairwise comparison and single-answer grading, respectively.\\n\\xe2\\x80\\x9cC\\xe2\\x80\\x9d denotes Claude.\\n\\xe2\\x80\\x9cHuman\\xe2\\x80\\x9d denotes expert labelers (excluding authors). \\xe2\\x80\\x98Human-M\\xe2\\x80\\x9d denotes the majority vote of humans.\\nThe single-answer grading can be converted into pairwise comparison results for calculating the agreement.\\nWe report two setups: \\xe2\\x80\\x9cS1\\xe2\\x80\\x9d includes non-tie, tie, and inconsistent (due to position bias) votes and counts inconsistent as a tie; \\xe2\\x80\\x9cS2\\xe2\\x80\\x9d only includes non-tie votes.\\nThe agreement between two random judges under each setup is denoted as \\xe2\\x80\\x9cR=\\xe2\\x80\\x9d.\\nThe top value in each cell is the agreement, and the bottom gray value is #votes.\\n</figcaption><div class=\"ltx_flex_figure ltx_flex_table\">\\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\\n<figure class=\"ltx_table ltx_flex_size_1 ltx_align_center\" id=\"A4.T14.st1\">\\n<div class=\"ltx_inline-block ltx_transformed_outer\" id=\"A4.T14.st1.1\" style=\"width:433.6pt;height:179.2pt;vertical-align:-0.0pt;\"><span class=\"ltx_transformed_inner\" style=\"transform:translate(34.7pt,-14.3pt) scale(1.19053433347917,1.19053433347917) ;\">\\n<table class=\"ltx_tabular ltx_guessed_headers ltx_align_top\" id=\"A4.T14.st1.1.1\">\\n<thead class=\"ltx_thead\">\\n<tr class=\"ltx_tr\" id=\"A4.T14.st1.1.1.1.1\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"A4.T14.st1.1.1.1.1.1\">Setup</th>\\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"5\" id=\"A4.T14.st1.1.1.1.1.2\">S1 (R = 33%)</th>\\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"5\" id=\"A4.T14.st1.1.1.1.1.3\">S2 (R = 50%)</th>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A4.T14.st1.1.1.2.2\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\" id=\"A4.T14.st1.1.1.2.2.1\">Judge</th>\\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"A4.T14.st1.1.1.2.2.2\">G4-S</th>\\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"A4.T14.st1.1.1.2.2.3\">C</th>\\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"A4.T14.st1.1.1.2.2.4\">Author</th>\\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"A4.T14.st1.1.1.2.2.5\">Human</th>\\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"A4.T14.st1.1.1.2.2.6\">Human-M</th>\\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"A4.T14.st1.1.1.2.2.7\">G4-S</th>\\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"A4.T14.st1.1.1.2.2.8\">C</th>\\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"A4.T14.st1.1.1.2.2.9\">Author</th>\\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"A4.T14.st1.1.1.2.2.10\">Human</th>\\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"A4.T14.st1.1.1.2.2.11\">Human-M</th>\\n</tr>\\n</thead>\\n<tbody class=\"ltx_tbody\">\\n<tr class=\"ltx_tr\" id=\"A4.T14.st1.1.1.3.1\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A4.T14.st1.1.1.3.1.1\">G4-P</th>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.3.1.2\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st1.1.1.3.1.2.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.3.1.2.1.1\">70%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.3.1.2.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st1.1.1.3.1.2.1.2.1\" style=\"color:#808080;\"> 1138</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.3.1.3\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st1.1.1.3.1.3.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.3.1.3.1.1\">63%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.3.1.3.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st1.1.1.3.1.3.1.2.1\" style=\"color:#808080;\"> 1198</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.3.1.4\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st1.1.1.3.1.4.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.3.1.4.1.1\">69%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.3.1.4.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st1.1.1.3.1.4.1.2.1\" style=\"color:#808080;\"> 345</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.3.1.5\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st1.1.1.3.1.5.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.3.1.5.1.1\">66%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.3.1.5.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st1.1.1.3.1.5.1.2.1\" style=\"color:#808080;\"> 1343</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.3.1.6\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st1.1.1.3.1.6.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.3.1.6.1.1\">67%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.3.1.6.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st1.1.1.3.1.6.1.2.1\" style=\"color:#808080;\"> 821</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.3.1.7\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st1.1.1.3.1.7.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.3.1.7.1.1\">97%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.3.1.7.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st1.1.1.3.1.7.1.2.1\" style=\"color:#808080;\"> 662</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.3.1.8\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st1.1.1.3.1.8.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.3.1.8.1.1\">94%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.3.1.8.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st1.1.1.3.1.8.1.2.1\" style=\"color:#808080;\"> 582</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.3.1.9\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st1.1.1.3.1.9.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.3.1.9.1.1\">92%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.3.1.9.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st1.1.1.3.1.9.1.2.1\" style=\"color:#808080;\"> 201</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.3.1.10\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st1.1.1.3.1.10.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.3.1.10.1.1\">85%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.3.1.10.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st1.1.1.3.1.10.1.2.1\" style=\"color:#808080;\"> 859</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.3.1.11\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st1.1.1.3.1.11.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.3.1.11.1.1\">85%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.3.1.11.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st1.1.1.3.1.11.1.2.1\" style=\"color:#808080;\"> 546</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A4.T14.st1.1.1.4.2\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A4.T14.st1.1.1.4.2.1\">G4-S</th>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.4.2.2\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.4.2.3\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st1.1.1.4.2.3.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.4.2.3.1.1\">66%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.4.2.3.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st1.1.1.4.2.3.1.2.1\" style=\"color:#808080;\"> 1136</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.4.2.4\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st1.1.1.4.2.4.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.4.2.4.1.1\">67%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.4.2.4.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st1.1.1.4.2.4.1.2.1\" style=\"color:#808080;\"> 324</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.4.2.5\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st1.1.1.4.2.5.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.4.2.5.1.1\">60%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.4.2.5.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st1.1.1.4.2.5.1.2.1\" style=\"color:#808080;\"> 1280</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.4.2.6\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st1.1.1.4.2.6.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.4.2.6.1.1\">60%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.4.2.6.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st1.1.1.4.2.6.1.2.1\" style=\"color:#808080;\"> 781</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.4.2.7\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.4.2.8\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st1.1.1.4.2.8.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.4.2.8.1.1\">90%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.4.2.8.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st1.1.1.4.2.8.1.2.1\" style=\"color:#808080;\"> 563</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.4.2.9\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st1.1.1.4.2.9.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.4.2.9.1.1\">94%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.4.2.9.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st1.1.1.4.2.9.1.2.1\" style=\"color:#808080;\"> 175</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.4.2.10\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st1.1.1.4.2.10.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.4.2.10.1.1\">85%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.4.2.10.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st1.1.1.4.2.10.1.2.1\" style=\"color:#808080;\"> 739</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.4.2.11\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st1.1.1.4.2.11.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.4.2.11.1.1\">85%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.4.2.11.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st1.1.1.4.2.11.1.2.1\" style=\"color:#808080;\"> 473</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A4.T14.st1.1.1.5.3\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A4.T14.st1.1.1.5.3.1\">C</th>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.5.3.2\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.5.3.3\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.5.3.4\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st1.1.1.5.3.4.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.5.3.4.1.1\">58%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.5.3.4.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st1.1.1.5.3.4.1.2.1\" style=\"color:#808080;\"> 343</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.5.3.5\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st1.1.1.5.3.5.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.5.3.5.1.1\">54%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.5.3.5.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st1.1.1.5.3.5.1.2.1\" style=\"color:#808080;\"> 1341</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.5.3.6\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st1.1.1.5.3.6.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.5.3.6.1.1\">55%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.5.3.6.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st1.1.1.5.3.6.1.2.1\" style=\"color:#808080;\"> 820</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.5.3.7\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.5.3.8\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.5.3.9\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st1.1.1.5.3.9.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.5.3.9.1.1\">89%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.5.3.9.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st1.1.1.5.3.9.1.2.1\" style=\"color:#808080;\"> 141</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.5.3.10\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st1.1.1.5.3.10.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.5.3.10.1.1\">85%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.5.3.10.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st1.1.1.5.3.10.1.2.1\" style=\"color:#808080;\"> 648</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.5.3.11\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st1.1.1.5.3.11.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.5.3.11.1.1\">86%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.5.3.11.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st1.1.1.5.3.11.1.2.1\" style=\"color:#808080;\"> 414</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A4.T14.st1.1.1.6.4\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A4.T14.st1.1.1.6.4.1\">Author</th>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.6.4.2\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.6.4.3\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.6.4.4\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st1.1.1.6.4.4.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.6.4.4.1.1\">69%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.6.4.4.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st1.1.1.6.4.4.1.2.1\" style=\"color:#808080;\"> 49</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.6.4.5\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st1.1.1.6.4.5.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.6.4.5.1.1\">65%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.6.4.5.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st1.1.1.6.4.5.1.2.1\" style=\"color:#808080;\"> 428</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.6.4.6\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st1.1.1.6.4.6.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.6.4.6.1.1\">55%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.6.4.6.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st1.1.1.6.4.6.1.2.1\" style=\"color:#808080;\"> 93</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.6.4.7\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.6.4.8\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.6.4.9\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st1.1.1.6.4.9.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.6.4.9.1.1\">87%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.6.4.9.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st1.1.1.6.4.9.1.2.1\" style=\"color:#808080;\"> 31</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.6.4.10\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st1.1.1.6.4.10.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.6.4.10.1.1\">83%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.6.4.10.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st1.1.1.6.4.10.1.2.1\" style=\"color:#808080;\"> 262</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st1.1.1.6.4.11\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st1.1.1.6.4.11.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.6.4.11.1.1\">76%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.6.4.11.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st1.1.1.6.4.11.1.2.1\" style=\"color:#808080;\"> 46</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A4.T14.st1.1.1.7.5\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t\" id=\"A4.T14.st1.1.1.7.5.1\">Human</th>\\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_t\" id=\"A4.T14.st1.1.1.7.5.2\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_t\" id=\"A4.T14.st1.1.1.7.5.3\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_t\" id=\"A4.T14.st1.1.1.7.5.4\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_t\" id=\"A4.T14.st1.1.1.7.5.5\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st1.1.1.7.5.5.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.7.5.5.1.1\">63%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.7.5.5.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st1.1.1.7.5.5.1.2.1\" style=\"color:#808080;\"> 721</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_t\" id=\"A4.T14.st1.1.1.7.5.6\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st1.1.1.7.5.6.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.7.5.6.1.1\">81%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.7.5.6.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st1.1.1.7.5.6.1.2.1\" style=\"color:#808080;\"> 892</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_t\" id=\"A4.T14.st1.1.1.7.5.7\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_t\" id=\"A4.T14.st1.1.1.7.5.8\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_t\" id=\"A4.T14.st1.1.1.7.5.9\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_t\" id=\"A4.T14.st1.1.1.7.5.10\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st1.1.1.7.5.10.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.7.5.10.1.1\">81%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.7.5.10.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st1.1.1.7.5.10.1.2.1\" style=\"color:#808080;\"> 479</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_t\" id=\"A4.T14.st1.1.1.7.5.11\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st1.1.1.7.5.11.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.7.5.11.1.1\">90%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st1.1.1.7.5.11.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st1.1.1.7.5.11.1.2.1\" style=\"color:#808080;\"> 631</span></span>\\n</span>\\n</td>\\n</tr>\\n</tbody>\\n</table>\\n</span></div>\\n<figcaption class=\"ltx_caption\"><span class=\"ltx_tag ltx_tag_table\">(a) </span>First Turn</figcaption>\\n</figure>\\n</div>\\n<div class=\"ltx_flex_break\"></div>\\n<div class=\"ltx_flex_cell ltx_flex_size_1\">\\n<figure class=\"ltx_table ltx_flex_size_1 ltx_align_center\" id=\"A4.T14.st2\">\\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_top\" id=\"A4.T14.st2.1\">\\n<thead class=\"ltx_thead\">\\n<tr class=\"ltx_tr\" id=\"A4.T14.st2.1.1.1\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"A4.T14.st2.1.1.1.1\">Setup</th>\\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"4\" id=\"A4.T14.st2.1.1.1.2\">S1 (R = 33%)</th>\\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"4\" id=\"A4.T14.st2.1.1.1.3\">S2 (R = 50%)</th>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A4.T14.st2.1.2.2\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\" id=\"A4.T14.st2.1.2.2.1\">Judge</th>\\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"A4.T14.st2.1.2.2.2\">G4-S</th>\\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"A4.T14.st2.1.2.2.3\">Author</th>\\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"A4.T14.st2.1.2.2.4\">Human</th>\\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"A4.T14.st2.1.2.2.5\">Human-M</th>\\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"A4.T14.st2.1.2.2.6\">G4-S</th>\\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"A4.T14.st2.1.2.2.7\">Author</th>\\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"A4.T14.st2.1.2.2.8\">Human</th>\\n<th class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_t\" id=\"A4.T14.st2.1.2.2.9\">Human-M</th>\\n</tr>\\n</thead>\\n<tbody class=\"ltx_tbody\">\\n<tr class=\"ltx_tr\" id=\"A4.T14.st2.1.3.1\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A4.T14.st2.1.3.1.1\">G4-P</th>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st2.1.3.1.2\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st2.1.3.1.2.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.3.1.2.1.1\">70%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.3.1.2.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st2.1.3.1.2.1.2.1\" style=\"color:#808080;\"> 1161</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st2.1.3.1.3\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st2.1.3.1.3.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.3.1.3.1.1\">66%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.3.1.3.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st2.1.3.1.3.1.2.1\" style=\"color:#808080;\"> 341</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st2.1.3.1.4\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st2.1.3.1.4.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.3.1.4.1.1\">66%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.3.1.4.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st2.1.3.1.4.1.2.1\" style=\"color:#808080;\"> 1325</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st2.1.3.1.5\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st2.1.3.1.5.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.3.1.5.1.1\">68%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.3.1.5.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st2.1.3.1.5.1.2.1\" style=\"color:#808080;\"> 812</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st2.1.3.1.6\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st2.1.3.1.6.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.3.1.6.1.1\">95%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.3.1.6.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st2.1.3.1.6.1.2.1\" style=\"color:#808080;\"> 727</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st2.1.3.1.7\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st2.1.3.1.7.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.3.1.7.1.1\">88%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.3.1.7.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st2.1.3.1.7.1.2.1\" style=\"color:#808080;\"> 205</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st2.1.3.1.8\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st2.1.3.1.8.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.3.1.8.1.1\">85%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.3.1.8.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st2.1.3.1.8.1.2.1\" style=\"color:#808080;\"> 864</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st2.1.3.1.9\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st2.1.3.1.9.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.3.1.9.1.1\">85%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.3.1.9.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st2.1.3.1.9.1.2.1\" style=\"color:#808080;\"> 557</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A4.T14.st2.1.4.2\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A4.T14.st2.1.4.2.1\">G4-S</th>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st2.1.4.2.2\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st2.1.4.2.3\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st2.1.4.2.3.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.4.2.3.1.1\">65%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.4.2.3.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st2.1.4.2.3.1.2.1\" style=\"color:#808080;\"> 331</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st2.1.4.2.4\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st2.1.4.2.4.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.4.2.4.1.1\">59%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.4.2.4.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st2.1.4.2.4.1.2.1\" style=\"color:#808080;\"> 1285</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st2.1.4.2.5\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st2.1.4.2.5.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.4.2.5.1.1\">61%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.4.2.5.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st2.1.4.2.5.1.2.1\" style=\"color:#808080;\"> 783</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st2.1.4.2.6\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st2.1.4.2.7\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st2.1.4.2.7.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.4.2.7.1.1\">89%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.4.2.7.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st2.1.4.2.7.1.2.1\" style=\"color:#808080;\"> 193</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st2.1.4.2.8\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st2.1.4.2.8.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.4.2.8.1.1\">84%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.4.2.8.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st2.1.4.2.8.1.2.1\" style=\"color:#808080;\"> 776</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st2.1.4.2.9\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st2.1.4.2.9.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.4.2.9.1.1\">85%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.4.2.9.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st2.1.4.2.9.1.2.1\" style=\"color:#808080;\"> 506</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A4.T14.st2.1.5.3\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"A4.T14.st2.1.5.3.1\">Author</th>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st2.1.5.3.2\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st2.1.5.3.3\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st2.1.5.3.3.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.5.3.3.1.1\">67%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.5.3.3.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st2.1.5.3.3.1.2.1\" style=\"color:#808080;\"> 49</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st2.1.5.3.4\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st2.1.5.3.4.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.5.3.4.1.1\">68%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.5.3.4.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st2.1.5.3.4.1.2.1\" style=\"color:#808080;\"> 413</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st2.1.5.3.5\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st2.1.5.3.5.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.5.3.5.1.1\">63%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.5.3.5.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st2.1.5.3.5.1.2.1\" style=\"color:#808080;\"> 87</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st2.1.5.3.6\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st2.1.5.3.7\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st2.1.5.3.7.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.5.3.7.1.1\">87%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.5.3.7.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st2.1.5.3.7.1.2.1\" style=\"color:#808080;\"> 31</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st2.1.5.3.8\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st2.1.5.3.8.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.5.3.8.1.1\">86%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.5.3.8.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st2.1.5.3.8.1.2.1\" style=\"color:#808080;\"> 273</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_t\" id=\"A4.T14.st2.1.5.3.9\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st2.1.5.3.9.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.5.3.9.1.1\">84%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.5.3.9.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st2.1.5.3.9.1.2.1\" style=\"color:#808080;\"> 54</span></span>\\n</span>\\n</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A4.T14.st2.1.6.4\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t\" id=\"A4.T14.st2.1.6.4.1\">Human</th>\\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_t\" id=\"A4.T14.st2.1.6.4.2\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_t\" id=\"A4.T14.st2.1.6.4.3\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_t\" id=\"A4.T14.st2.1.6.4.4\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st2.1.6.4.4.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.6.4.4.1.1\">67%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.6.4.4.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st2.1.6.4.4.1.2.1\" style=\"color:#808080;\"> 707</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_t\" id=\"A4.T14.st2.1.6.4.5\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st2.1.6.4.5.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.6.4.5.1.1\">83%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.6.4.5.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st2.1.6.4.5.1.2.1\" style=\"color:#808080;\"> 877</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_t\" id=\"A4.T14.st2.1.6.4.6\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_t\" id=\"A4.T14.st2.1.6.4.7\">-</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_t\" id=\"A4.T14.st2.1.6.4.8\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st2.1.6.4.8.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.6.4.8.1.1\">82%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.6.4.8.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st2.1.6.4.8.1.2.1\" style=\"color:#808080;\"> 474</span></span>\\n</span>\\n</td>\\n<td class=\"ltx_td ltx_align_right ltx_border_b ltx_border_t\" id=\"A4.T14.st2.1.6.4.9\">\\n<span class=\"ltx_inline-block\" id=\"A4.T14.st2.1.6.4.9.1\">\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.6.4.9.1.1\">91%</span>\\n<span class=\"ltx_p\" id=\"A4.T14.st2.1.6.4.9.1.2\"><span class=\"ltx_text\" id=\"A4.T14.st2.1.6.4.9.1.2.1\" style=\"color:#808080;\"> 629</span></span>\\n</span>\\n</td>\\n</tr>\\n</tbody>\\n</table>\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">(b) </span>Second Turn</figcaption>\\n</figure>\\n</div>\\n</div>\\n</figure>\\n</section>\\n<section class=\"ltx_subsection\" id=\"A4.SS4\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\">D.4 </span>Category-wise scores with single-answer grading</h3>\\n<div class=\"ltx_para ltx_noindent\" id=\"A4.SS4.p1\">\\n<p class=\"ltx_p\" id=\"A4.SS4.p1.1\">We use single-answer grading to evaluate 6 models on MT-bench and plot the category-wise scores in <a class=\"ltx_ref\" href=\"#A4.F19\" title=\"Figure 19 \\xe2\\x80\\xa3 D.4 Category-wise scores with single-answer grading \\xe2\\x80\\xa3 Appendix D Additional Experimental Results \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Figure</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">19</span></a>.</p>\\n</div>\\n<figure class=\"ltx_figure\" id=\"A4.F19\"><img alt=\"Refer to caption\" class=\"ltx_graphics ltx_centering ltx_img_landscape\" height=\"415\" id=\"A4.F19.g1\" src=\"x17.png\" width=\"664\"/>\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_figure\">Figure 19: </span>Category-wise scores of 6 models on MT-bench.</figcaption>\\n</figure>\\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\\n</section>\\n</section>\\n<section class=\"ltx_appendix\" id=\"A5\">\\n<h2 class=\"ltx_title ltx_title_appendix\">\\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix E </span>Training Details of Vicuna Models</h2>\\n<div class=\"ltx_para ltx_noindent\" id=\"A5.p1\">\\n<p class=\"ltx_p\" id=\"A5.p1.1\">Vicuna is created by fine-tuning a LLaMA base model using user-shared conversations gathered from ShareGPT.com with its public APIs.\\nShareGPT is a website where users can share their ChatGPT conversations.\\nTo ensure data quality, we convert the HTML back to markdown and filter out some inappropriate or low-quality samples, which results in 125K conversations after data cleaning.<span class=\"ltx_note ltx_role_footnote\" id=\"footnote3\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">3</sup><span class=\"ltx_tag ltx_tag_note\">3</span>In this study, we use more data (125K) than the version in our earlier blog post (70K).</span></span></span>\\nWe then divide lengthy conversations into smaller segments that fit the model\\xe2\\x80\\x99s maximum context length.</p>\\n</div>\\n<div class=\"ltx_para ltx_noindent\" id=\"A5.p2\">\\n<p class=\"ltx_p\" id=\"A5.p2.1\">We construct three training datasets with different scales from this cleaned ShareGPT dataset. Their statistics are in <a class=\"ltx_ref\" href=\"#S4.T9\" title=\"Table 9 \\xe2\\x80\\xa3 4.3 Win rates under different judges \\xe2\\x80\\xa3 4 Agreement Evaluation \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Table</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">9</span></a>, where we also compare it with Alpaca\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib38\" title=\"\">38</a>]</cite> dataset. \\xe2\\x80\\x9cAll\\xe2\\x80\\x9d is the full dataset. \\xe2\\x80\\x9cSingle\\xe2\\x80\\x9d only includes the first turn of each conversation. \\xe2\\x80\\x9cSelected\\xe2\\x80\\x9d is a small high-quality dataset of 3K sequences. To construct the \\xe2\\x80\\x9cSelected\\xe2\\x80\\x9d dataset, we pick sequences that include at least 3 turns of conversations generated by GPT-4 and run a clustering algorithm to divide them into 3K clusters and pick the centroid of each cluster.</p>\\n</div>\\n<div class=\"ltx_para ltx_noindent\" id=\"A5.p3\">\\n<p class=\"ltx_p\" id=\"A5.p3.1\">All models (Vicuna-7B/13B) are trained with the same hyperparameters: global batch size=128, learning=2e-5, epochs=3, seq length=2048. Except for \\xe2\\x80\\x9cSelected\\xe2\\x80\\x9d, which we train for 5 epochs.\\nThe training code is built on top of the Alpaca code but additionally handles multi-turn conversations.\\nThe training is done with 8x A100 GPUs. The longest single training run takes around 2 days.\\nWe utilize SkyPilot\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib49\" title=\"\">49</a>]</cite> managed spot instances for saving training costs and FlashAttention\\xc2\\xa0<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib11\" title=\"\">11</a>]</cite> for memory optimizations.\\nThe training code is available at <a class=\"ltx_ref ltx_url ltx_font_typewriter\" href=\"https://github.com/lm-sys/FastChat\" title=\"\">https://github.com/lm-sys/FastChat</a>.</p>\\n</div>\\n<figure class=\"ltx_table\" id=\"A5.T15\">\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 15: </span>Dataset statistics</figcaption>\\n<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"A5.T15.1\">\\n<tbody class=\"ltx_tbody\">\\n<tr class=\"ltx_tr\" id=\"A5.T15.1.1.1\">\\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"A5.T15.1.1.1.1\">Dataset Name</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"A5.T15.1.1.1.2\">Alpaca</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"A5.T15.1.1.1.3\">Selected</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"A5.T15.1.1.1.4\">Single</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"A5.T15.1.1.1.5\">All</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A5.T15.1.2.2\">\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A5.T15.1.2.2.1\">#Token</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A5.T15.1.2.2.2\">4.4M</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A5.T15.1.2.2.3\">4.8M</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A5.T15.1.2.2.4\">184M</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A5.T15.1.2.2.5\">370M</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A5.T15.1.3.3\">\\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T15.1.3.3.1\">#Sequence</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T15.1.3.3.2\">52K</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T15.1.3.3.3\">3K</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T15.1.3.3.4\">257K</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T15.1.3.3.5\">257K</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A5.T15.1.4.4\">\\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T15.1.4.4.1\">Avg. turns of conversation</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T15.1.4.4.2\">1.0</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T15.1.4.4.3\">4.0</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T15.1.4.4.4\">1.0</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A5.T15.1.4.4.5\">2.9</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A5.T15.1.5.5\">\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A5.T15.1.5.5.1\">Avg. response length (token)</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A5.T15.1.5.5.2\">65</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A5.T15.1.5.5.3\">343</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A5.T15.1.5.5.4\">473</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"A5.T15.1.5.5.5\">373</td>\\n</tr>\\n</tbody>\\n</table>\\n</figure>\\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\\n<div class=\"ltx_pagination ltx_role_newpage\"></div>\\n</section>\\n<section class=\"ltx_appendix\" id=\"A6\">\\n<h2 class=\"ltx_title ltx_title_appendix\">\\n<span class=\"ltx_tag ltx_tag_appendix\">Appendix F </span>Exploring Vicuna as a judge</h2>\\n<div class=\"ltx_para ltx_noindent\" id=\"A6.p1\">\\n<p class=\"ltx_p\" id=\"A6.p1.1\">In this paper, we mostly evaluate the ability of close-sourced models such as GPT-4 as a proxy for human evaluations. However, model services such as GPT-4 can also become expensive with a growing number of evaluations. On the other hand, popular open-sourced LLMs, e.g. Vicuna-13B shows strong language understanding capability, and are much cheaper than close-sourced LLMs. In this section, we further explore the potential of using Vicuna-13B as a more cost-friendly proxy.</p>\\n</div>\\n<section class=\"ltx_subsection\" id=\"A6.SS1\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\">F.1 </span>Zero-Shot Vicuna</h3>\\n<div class=\"ltx_para ltx_noindent\" id=\"A6.SS1.p1\">\\n<p class=\"ltx_p\" id=\"A6.SS1.p1.1\">When using as-it-is (zero-shot), Vicuna-13B noticeably suffers from limitations we discuss, e.g. position bias. As shown in <a class=\"ltx_ref\" href=\"#A6.T16\" title=\"Table 16 \\xe2\\x80\\xa3 Agreement results \\xe2\\x80\\xa3 F.2 Arena Fine-tuned Vicuna \\xe2\\x80\\xa3 Appendix F Exploring Vicuna as a judge \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Table</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">16</span></a>, Vicuna-13B has a consistency rate from 11.2% to 16.2% across different prompt templates, much lower than all the closed-sourced models. In addition, it has a high error rate (from 22.5% to 78.8%) because of its weaker instruction-following capability. In many scenarios, Vicuna-13B provides responses such as \"Answer A is better than answer B\", without following the pre-defined template. These responses are rendered as natural languages and are difficult to be parsed automatically, making the model less useful in a scalable and automatic evaluation pipeline.</p>\\n</div>\\n</section>\\n<section class=\"ltx_subsection\" id=\"A6.SS2\">\\n<h3 class=\"ltx_title ltx_title_subsection\">\\n<span class=\"ltx_tag ltx_tag_subsection\">F.2 </span>Arena Fine-tuned Vicuna</h3>\\n<section class=\"ltx_paragraph\" id=\"A6.SS2.SSS0.Px1\">\\n<h4 class=\"ltx_title ltx_title_paragraph\">Training</h4>\\n<div class=\"ltx_para ltx_noindent\" id=\"A6.SS2.SSS0.Px1.p1\">\\n<p class=\"ltx_p\" id=\"A6.SS2.SSS0.Px1.p1.1\">Due to the incapability of the zero-shot Vicuna-13B model, we further finetune the model with human votes from Chatbot Arena. Specifically, we randomly sample 22K single-turn votes from the arena, covering all models supported by the time of this paper submission (GPT-4, GPT-3.5, Claude-v1, Vicuna-13b, Vicuna-7b, Koala-13B, Alpaca-13B,LLaMA-13B, Dolly-12B, FastChat-T5, RWKV-4-Raven, MPT-Chat, OpenAssistant, ChatGLM, and StableLM), to expose the model with a wider range of chatbot outputs and human preferences. We use 20K votes for training, and 2K for validation. To address the aforementioned weak instruction following problem, we formulate the problem as a 3-way sequence classification problem. Thus, the model simply needs to predict which one of the chat-bot outputs is better (or tie), without needing to exactly following the provided answer template.\\nIn particular, we construct an input by using the default prompt and the two model answers. The labels are A, B, and tie (including both-bad-vote and tie-vote).\\nWe train for 3 epochs with a cosine learning rate scheduler and a 2e-5 maximum learning rate. We use the 2K validation dataset to choose hyper-parameters, and test on the same 3K dataset in the main body of the paper.</p>\\n</div>\\n</section>\\n<section class=\"ltx_paragraph\" id=\"A6.SS2.SSS0.Px2\">\\n<h4 class=\"ltx_title ltx_title_paragraph\">Position bias results</h4>\\n<div class=\"ltx_para ltx_noindent\" id=\"A6.SS2.SSS0.Px2.p1\">\\n<p class=\"ltx_p\" id=\"A6.SS2.SSS0.Px2.p1.1\">The results for position bias are provided in <a class=\"ltx_ref\" href=\"#A6.T16\" title=\"Table 16 \\xe2\\x80\\xa3 Agreement results \\xe2\\x80\\xa3 F.2 Arena Fine-tuned Vicuna \\xe2\\x80\\xa3 Appendix F Exploring Vicuna as a judge \\xe2\\x80\\xa3 Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\"><span class=\"ltx_text ltx_ref_tag\">Table</span>\\xc2\\xa0<span class=\"ltx_text ltx_ref_tag\">16</span></a>.\\nThe consistency improves significantly from 16.2% to 65.0%. Due to the classification formulation, every output is recognizable (error rate 0%).\\nIn addition, we measure the classification accuracy over the test dataset.</p>\\n</div>\\n</section>\\n<section class=\"ltx_paragraph\" id=\"A6.SS2.SSS0.Px3\">\\n<h4 class=\"ltx_title ltx_title_paragraph\">Agreement results</h4>\\n<div class=\"ltx_para ltx_noindent\" id=\"A6.SS2.SSS0.Px3.p1\">\\n<p class=\"ltx_p\" id=\"A6.SS2.SSS0.Px3.p1.1\">It achieves 56.8% when including all three labels, and 85.5% when excluding tie predictions and labels, significantly outperforming random guesses of 33% and 50% respectively, and show positive signals to match GPT-4 (66% and 87% respectively).\\nIn conclusion, a further fine-tuned Vicuna-13B model shows strong potential to be used as a cheap open-sourced replacement for expensive closed-sourced LLMs. A similar conclusion is also found by a concurrent paper<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"#bib.bib42\" title=\"\">42</a>]</cite>.</p>\\n</div>\\n<figure class=\"ltx_table\" id=\"A6.T16\">\\n<figcaption class=\"ltx_caption ltx_centering\"><span class=\"ltx_tag ltx_tag_table\">Table 16: </span>Position bias of the Vicuna-13B model without and with further fine-tuning. We denote them as Vicuna-13B-Zero-Shot and Vicuna-13B-Fine-Tune respectively. Consistency is the percentage of cases where a judge gives consistent results when swapping the order of two assistants. \\xe2\\x80\\x9cBiased toward first\\xe2\\x80\\x9d is the percentage of cases when a judge favors the first answer. \\xe2\\x80\\x9cError\\xe2\\x80\\x9d indicates wrong output formats. The largest number in each column is in bold.</figcaption>\\n<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"A6.T16.1\">\\n<thead class=\"ltx_thead\">\\n<tr class=\"ltx_tr\" id=\"A6.T16.1.1.1\">\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A6.T16.1.1.1.1\">Judge</th>\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A6.T16.1.1.1.2\">Prompt</th>\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A6.T16.1.1.1.3\">Consistency</th>\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A6.T16.1.1.1.4\">Biased toward first</th>\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A6.T16.1.1.1.5\">Biased toward second</th>\\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"A6.T16.1.1.1.6\">Error</th>\\n</tr>\\n</thead>\\n<tbody class=\"ltx_tbody\">\\n<tr class=\"ltx_tr\" id=\"A6.T16.1.2.1\">\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A6.T16.1.2.1.1\" rowspan=\"3\"><span class=\"ltx_text\" id=\"A6.T16.1.2.1.1.1\">Vicuna-13B-Zero-Shot</span></td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A6.T16.1.2.1.2\">default</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A6.T16.1.2.1.3\">15.0%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A6.T16.1.2.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T16.1.2.1.4.1\">53.8%</span></td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A6.T16.1.2.1.5\">8.8%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"A6.T16.1.2.1.6\">22.5%</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A6.T16.1.3.2\">\\n<td class=\"ltx_td ltx_align_left\" id=\"A6.T16.1.3.2.1\">rename</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A6.T16.1.3.2.2\">16.2%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A6.T16.1.3.2.3\">12.5%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A6.T16.1.3.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T16.1.3.2.4.1\">40.0%</span></td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A6.T16.1.3.2.5\">31.2%</td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A6.T16.1.4.3\">\\n<td class=\"ltx_td ltx_align_left\" id=\"A6.T16.1.4.3.1\">score</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A6.T16.1.4.3.2\">11.2%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A6.T16.1.4.3.3\">10.0%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A6.T16.1.4.3.4\">0.0%</td>\\n<td class=\"ltx_td ltx_align_left\" id=\"A6.T16.1.4.3.5\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T16.1.4.3.5.1\">78.8%</span></td>\\n</tr>\\n<tr class=\"ltx_tr\" id=\"A6.T16.1.5.4\">\\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A6.T16.1.5.4.1\"><span class=\"ltx_text\" id=\"A6.T16.1.5.4.1.1\">Vicuna-13B-Fine-Tune</span></td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A6.T16.1.5.4.2\">default</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A6.T16.1.5.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"A6.T16.1.5.4.3.1\">65.0%</span></td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A6.T16.1.5.4.4\">27.5%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A6.T16.1.5.4.5\">7.5%</td>\\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"A6.T16.1.5.4.6\">0.0%</td>\\n</tr>\\n</tbody>\\n</table>\\n</figure>\\n</section>\\n</section>\\n</section>\\n</article>\\n</div>\\n<footer class=\"ltx_page_footer\">\\n<div class=\"ltx_page_logo\">Generated  on Sun Dec 24 02:00:19 2023 by <a class=\"ltx_LaTeXML_logo\" href=\"http://dlmf.nist.gov/LaTeXML/\"><span style=\"letter-spacing:-0.2em; margin-right:0.1em;\">L<span style=\"font-size:70%;position:relative; bottom:2.2pt;\">A</span>T<span style=\"position:relative; bottom:-0.4ex;\">E</span></span><span class=\"ltx_font_smallcaps\">xml</span><img alt=\"[LOGO]\" src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==\"/></a>\\n</div></footer>\\n</div>\\n</body>\\n</html>\\n'\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "print(paper.content)\n",
    "print(paper.status_code) # make sure you get status code of 200 to proceed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(paper.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_content = soup.select_one(\".ltx_page_content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\n"
     ]
    }
   ],
   "source": [
    "title = page_content.select_one(\"h1.ltx_title\").text.replace('\\n', '')\n",
    "title = \"Title: \"+title\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authors: Lianmin Zheng    Wei-Lin Chiang     Ying Sheng     Siyuan Zhuang  Zhanghao Wu    Yonghao Zhuang    Zi Lin    Zhuohan Li    Dacheng Li  Eric P. Xing    Hao Zhang   Joseph E. Gonzalez    Ion Stoica     UC Berkeley    UC San Diego    Carnegie Mellon University    Stanford    MBZUAI\n"
     ]
    }
   ],
   "source": [
    "authors = page_content.select_one(\".ltx_personname\").text.replace(\"start_FLOATSUPERSCRIPT\", \"\").replace(\"end_FLOATSUPERSCRIPT\",\"\")\n",
    "authors = re.sub(r\"\\d\", \"\", authors).replace(\"*\", '').replace('{}','').replace('^',\"\").replace(\"\\\\And\",\"\").replace(\"\\n\",\"\")\n",
    "authors = \"Authors: \" + authors\n",
    "print(authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Abstract\n",
      "Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences.\n",
      "To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions.\n",
      "We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them.\n",
      "We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform.\n",
      "Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans.\n",
      "Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain.\n",
      "Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna.\n",
      "The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "abstract = page_content.select_one(\".ltx_abstract\").text\n",
    "print(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There has been a proliferation of LLM-based chat assistants (chatbots) that leverage supervised instruction fine-tuning and reinforcement learning with human feedback (RLHF) to unlock new instruction following and conversational abilities\\xa0[31, 2, 30, 8, 52, 48, 14].\\nOnce aligned with humans, these chat models are strongly preferred by human users over the original, unaligned models on which they are built.\\nHowever, the heightened user preference does not always correspond to improved scores on traditional LLM benchmarks  benchmarks like MMLU\\xa0[19] and HELM\\xa0[24] cannot effectively tell the difference between these aligned models and the base models.\\nThis phenomenon suggests that there is a fundamental discrepancy between user perceptions of the usefulness of chatbots and the criteria adopted by conventional benchmarks.', 'We argue that this discrepancy primarily arises due to\\nexisting evaluation that only measures LLMs core capability on a confined set of tasks (e.g., multi-choice knowledge or retrieval questions), without adequately assessing its alignment with human preference in open-ended tasks, such as the ability to accurately adhere to instructions in multi-turn dialogues.\\nAs a demonstration, we show conversation histories with two models on an MMLU question in Figure\\xa01.\\nThe two models are LLaMA-13B\\xa0[39], a pre-trained base model without fine-tuning, and Vicuna-13B, our fine-tuned model from LLaMA-13B on high-quality conversations (the training details are in Appendix\\xa0E).\\nDespite the base LLaMA models showing competitive performance on conventional benchmarks (Table\\xa09), its answers to open-ended questions are often not preferred by humans.\\nThis misalignment of conventional benchmarks underscores the core problem driving this paper:\\nthe need for a robust and scalable automated method to evaluate LLM alignment with human preferences.', 'To study this, we introduce two benchmarks with human ratings as the primary evaluation metric: MT-bench and Chatbot Arena.\\nMT-bench is a series of open-ended questions that evaluate a chatbots multi-turn conversational and instruction-following ability  two critical elements for human preference. MT-bench is also carefully constructed to differentiate chatbots based on their core capabilities, such as reasoning and math.\\nIn addition, we develop Chatbot Arena, a crowdsourced platform featuring anonymous battles between chatbots in real-world scenarios  Users engage in conversations with two chatbots at the same time and rate their responses based on personal preferences.', 'While human evaluation is the gold standard for assessing human preferences, it is exceptionally slow and costly.\\nTo automate the evaluation, we explore the use of state-of-the-art LLMs, such as GPT-4, as a surrogate for humans.\\nBecause these models are often trained with RLHF, they already exhibit strong human alignment.\\nWe call this approach LLM-as-a-judge.\\nThis approach has been tried in our earlier blog post\\xa0[8] and other concurrent or follow-up work\\xa0[5, 29, 14, 12, 52, 18, 33, 40, 7, 43].\\nHowever, there has not been a systematic study of this approach.', 'In this paper, we study the LLM-as-a-judge approach by comparing it to the gold standard of human evaluation.\\nWe examine several potential limitations of the LLM-as-a-judge approach including position bias, verbosity bias, self-enhancement bias, and limited reasoning ability.\\nWe show that some of the biases are minor or can be mitigated.\\nOnce addressed, our results from 3K controlled expert votes and 3K crowdsourced human votes in the wild verify that GPT-4 judge match human evaluations at an agreement rate exceeding 80%, achieving the same level of human-human agreement (4.2, Table\\xa04).\\nConsequently, this suggests LLM-as-a-judge is a scalable method to swiftly evaluate human preference, serving as a promising alternative to traditional human evaluations. \\n', 'This paper makes two contributions: (1) a systematic study of LLM-as-a-judge; and (2) human preference datasets with high-quality questions and diverse user interactions from MT-bench and Chatbot Arena.\\nIn addition, we argue for the adoption of a hybrid evaluation framework for future LLM benchmarks: by combining the existing capability-based benchmarks and the new preference-based benchmarks with LLM-as-a-judge, one can swiftly and automatically evaluate both the core capabilities and human alignment of models.\\nWe publicly release 80 MT-bench questions, 3K expert votes, and 30K conversations with human preferences for future study.', 'With the recent advances of LLMs, LLM-based assistants start to exhibit artificial general intelligence across diverse tasks, from writing and chatting to coding\\xa0[5, 30, 1, 37]. However, evaluating their broad capabilities also becomes more challenging.\\nDespite the availability of numerous benchmarks for language models, they primarily focus on evaluating models on closed-ended questions with short responses.\\nGiven that these chat assistants can now precisely follow user instructions in multi-turn dialogues and answer open-ended questions in a zero-shot manner, current benchmarks are inadequate for assessing such capabilities. Existing benchmarks mostly fall into the following three categories.', 'Core-knowledge benchmarks, including MMLU\\xa0[19], HellaSwag\\xa0[50], ARC\\xa0[9], WinoGrande\\xa0[36], HumanEval\\xa0[6], GSM-8K\\xa0[10], and AGIEval\\xa0[51], evaluate the core capabilities of pre-trained LLMs using zero-shot and few-shot benchmark sets. They typically require LLMs to generate a short, specific answer to benchmark questions that can be automatically validated.', 'Instruction-following benchmarks, such as Flan\\xa0[27, 46], Self-instruct\\xa0[44], NaturalInstructions\\xa0[28], Super-NaturalInstructions\\xa0[45], expand to slightly more open-ended questions and more diverse tasks and are used to evaluate LLMs after instruction fine-tuning.', 'Conversational benchmarks, like CoQA\\xa0[35], MMDialog\\xa0[15] and OpenAssistant\\xa0[23], are closest to our intended use cases. However, the diversity and complexity of their questions often fall short in challenging the capabilities of the latest chatbots.', 'While largely overlooked by existing LLM benchmarks, human preferences serve as a direct measure of a chatbots utility in open-ended, multi-turn human-AI interactions. To bridge this gap, we introduce two novel benchmarks expressly tailored to assess human preferences. Simultaneously, these benchmarks are designed to distinguish the core capabilities of state-of-the-art models.', 'We create MT-bench, a benchmark consisting of 80 high-quality multi-turn questions.\\nMT-bench is designed to test multi-turn conversation and instruction-following ability, covering common use cases and focusing on challenging questions to differentiate models.\\nWe identify 8 common categories of user prompts to guide its construction: writing, roleplay, extraction, reasoning, math, coding, knowledge I (STEM), and knowledge II (humanities/social science).\\nFor each category, we then manually designed 10 multi-turn questions.\\nTable\\xa01 lists several sample questions.', 'Our second approach is Chatbot Arena, a crowdsourcing benchmark platform featuring anonymous battles.\\nOn this platform, users can interact with two anonymous models simultaneously, posing the same question to both. They vote for which model provides the preferred response, with the identities of the models disclosed post-voting. After running Chatbot Arena for one month, we have collected around 30K votes. Since the platform does not use pre-defined questions, it allows gathering a wide range of unrestricted use cases and votes in the wild, based on the diverse interests of users. A screenshot of the platform can be found at Section\\xa0C.2.\\n', 'While our initial evaluations using MT-bench and Chatbot Arena rely on human ratings, collecting human preferences can be costly and laborious\\xa0[44, 38, 31, 2, 13]. To overcome this, we aim to develop a more scalable and automated approach. Given that most questions in MT-bench and Chatbot Arena are open-ended without reference answers, devising a rule-based program to assess the outputs is extremely challenging.\\nTraditional evaluation metrics based on the similarity between outputs and reference answers (e.g., ROUGE\\xa0[25], BLEU\\xa0[32]) are also ineffective for these questions.', 'As LLMs continue to improve, they show potential in replacing human annotators in many tasks\\xa0[17, 20]. Specifically, we are interested in whether LLMs can effectively evaluate the responses of chat assistants and match human preferences. Next, we discuss the use and limitations of LLM-as-a-judge.', 'We propose 3 LLM-as-a-judge variations. They can be implemented independently or in combination:', 'Pairwise comparison. An LLM judge is presented with a question and two answers, and tasked to determine which one is better or declare a tie. The prompt used is given in Figure\\xa04 (Appendix).', 'Single answer grading.\\nAlternatively, an LLM judge is asked to directly assign a score to a single answer. The prompt used for this scenario is in Figure\\xa05 (Appendix).', 'Reference-guided grading.\\nIn certain cases, it may be beneficial to provide a reference solution if applicable. An example prompt we use for grading math problems is in Figure\\xa07 (Appendix).', 'These methods have different pros and cons. For example, the pairwise comparison may lack scalability when the number of players increases, given that the number of possible pairs grows quadratically; single answer grading may be unable to discern subtle differences between specific pairs, and its results may become unstable, as absolute scores are likely to fluctuate more than relative pairwise results if the judge model changes.', 'LLM-as-a-judge offers two key benefits: scalability and explainability. It reduces the need for human involvement, enabling scalable benchmarks and fast iterations. Additionally, LLM judges provide not only scores but also explanations, making their outputs interpretable, as shown in Figure\\xa01.', 'We identify certain biases and limitations of LLM judges. However, we will also present solutions later and show the agreement between LLM judges and humans is high despite these limitations.', 'Position bias is when an LLM exhibits a propensity to favor certain positions over others. This bias is not unique to our context and has been seen in human decision-making\\xa0[3, 34] and other ML domains\\xa0[22, 41].', 'Figure\\xa010 (Appendix) shows an example of position bias.\\nGPT-4 is tasked to evaluate two responses from GPT-3.5 and Vicuna-13B to an open-ended question. When GPT-3.5s answer is positioned first, GPT-4 considers GPT-3.5s answer more detailed and superior. However, upon switching the positions of the two responses, GPT-4s judgement flips, favoring Vicunas answer.', 'To analyze the position bias, we construct two similar answers to each first-turn question in MT-bench by calling GPT-3.5 twice with a temperature of 0.7.\\nWe then try three LLMs with two different prompts:\\ndefault is our default prompt in Figure\\xa04 (Appendix).\\nrename renames the assistants in our default prompt to see whether the bias is on positions or names.\\nAs in Table\\xa02, we found all of them exhibit strong position bias. Most LLM judges favor the first position. Claude-v1 also shows a name bias which makes it favors \"Assistant A\", as illustrated by the \"rename\" prompt. The position bias can be very significant. Only GPT-4 outputs consistent results in more than 60% of cases.', 'Note that this test is challenging because the answers are very similar and occasionally indistinguishable even to humans. We will show that position bias is less prominent in some cases in Section\\xa0D.1.\\nAs for the origin of this bias, we suspect that it could be rooted in the training data or inherent to the left-to-right architecture of causal transformers, but leave a deeper study as future work.', 'Verbosity bias is when an LLM judge favors longer, verbose responses, even if they are not as clear, high-quality, or accurate as shorter alternatives.', 'To examine this bias, we design a repetitive list attack with model answers from MT-bench.\\nWe first select 23 model answers from MT-bench that contain a numbered list.\\nWe then make them unnecessarily verbose by asking GPT-4 to rephrase the list without adding any new information and insert the rephrased new list to the beginning of the original list.\\nFor example, if the original response contains 5 items, then the new response will contain 10 items but the first 5 items are rephrased from the original 5 items. An example is shown in Figure\\xa011 (Appendix). We define the attack is successful if an LLM judge thinks the new response is better than the old response. Table\\xa04 shows the failure rate of LLM judges under this attack, demonstrating that all LLMs may be prone to verbosity bias though GPT-4 defends significantly better than others. As a calibration, we find LLM judges are able to correctly judge identical answers (i.e., they always return a tie for two identical answers) but cannot pass the more advanced repetitive list attack.', 'Self-enhancement bias.\\nWe adopt the term self-enhancement bias from social cognition literature\\xa0[4] to describe the effect that LLM judges may favor the answers generated by themselves.', 'We examine this effect statistically. Figure\\xa02(b) shows the win rate (w/o tie) of six models under different LLM judges and humans.\\nCompared to humans, we do observe that some judges favor certain models. For example, GPT-4 favors itself with a 10% higher win rate; Claude-v1 favors itself with a 25% higher win rate. However, they also favor other models and GPT-3.5 does not favor itself.\\nDue to limited data and small differences, our study cannot determine whether the models exhibit a self-enhancement bias.\\nConducting a controlled study is challenging because we cannot easily rephrase a response to fit the style of another model without changing the quality.', 'Limited capability in grading math and reasoning questions.\\nLLMs are known to have limited math and reasoning capability\\xa0[10], which results in its failure of grading such questions because they do not know the correct answers.\\nHowever, what is more intriguing is that it also shows limitations in grading basic math problems which it is capable of solving.\\nFor instance, in Figure\\xa012 (Appendix), we present an example of an elementary math question in which GPT-4 makes an incorrect judgment.\\nIts worth noting that although GPT-4 can solve the problem (when asked separately), it was misled by the provided answers, ultimately resulting in incorrect judgment.\\nThis pattern can also be seen in a reasoning question example in Figure\\xa013 (Appendix).\\nBoth GPT-3.5 and Claude-v1 show a similar weakness.\\nIn Section\\xa03.4, we will introduce a reference-guided method to mitigate such issues.\\n', 'We present a few methods to address position bias and the limited grading ability for math questions.', 'Swapping positions.\\nThe position bias can be addressed by simple solutions.\\nA conservative approach is to call a judge twice by swapping the order of two answers and only declare a win when an answer is preferred in both orders. If the results are inconsistent after swapping, we can call it a tie.\\nAnother more aggressive approach is to assign positions randomly, which can be effective at a large scale with the correct expectations. In the following experiments, we use the conservative one.', 'Few-shot judge.\\nWe assess whether few-shot examples can improve consistency in the position bias benchmark. We select three good judgment examples using MT-bench-like questions, GPT-3.5 and Vicuna for generating answers, and GPT-4 for generating judgments.\\nThe examples cover three cases: A is better, B is better, and tie.\\nAs shown in Table\\xa013 (Appendix), the few-shot judge can significantly increase the consistency of GPT-4 from 65.0% to 77.5%.\\nHowever, high consistency may not imply high accuracy and we are not sure whether the few-shot examples will introduce new biases. Besides, the longer prompts make API calls 44\\\\times4  more expensive. We use the zero-shot prompt by default in our following experiments but leave an additional study in Section\\xa0D.2.', 'Chain-of-thought and reference-guided judge.\\n\\nIn Section\\xa03.3, we have shown LLMs limited capability in grading math and reasoning questions.\\nWe propose two simple methods to mitigate this issue: chain-of-thought judge and reference-guided judge.\\nChain-of-thought is a widely used technique to improve LLMs reasoning capability\\xa0[47].\\nWe propose a similar technique to prompt an LLM judge to begin with answering the question independently and then start grading.\\nDetailed prompt in Figure\\xa06 (Appendix).\\nHowever, even with the CoT prompt, we find that in many cases LLM makes exactly the same mistake as the given answers in its problem-solving process (See example in Figure\\xa014 (Appendix), suggesting that LLM judge may still be misled by the context.\\nHence, we propose a reference-guided method, in which we first generate LLM judges answer independently, and then display it as a reference answer in the judge prompt.\\nIn Table\\xa04, we see a significant improvement in failure rate (from 70% to 15%) over the default prompt.', 'Fine-tuning a judge model.\\n\\nWe try fine-tuning a Vicuna-13B on arena data to act as a judge and show some promising preliminary results in Appendix\\xa0F.', 'In MT-bench, every question involves two turns to evaluate conversational abilities.\\nTherefore, when comparing two assistants, it becomes necessary to present a total of two questions and four responses, complicating the prompt design.\\nWe explore two possible designs, (1) breaking the two turns into two prompts or (2) displaying complete conversations in a single prompt.\\nOur finding is the former one can cause the LLM judge struggling to locate the assistants previous response precisely.\\nWe illustrate a case in Figure\\xa015 (Appendix) where GPT-4 makes an inaccurate judgment due to a faulty reference.\\nThis suggests the necessity of displaying a complete conversation to enable the LLM judge to better grasp the context.\\nWe then consider the alternative design that presents two full conversations in a single prompt in which we ask the LLM judge to focus on the second question (Figure\\xa08 (Appendix)).\\nThis approach has been found to significantly alleviate the aforementioned referencing issue.', 'We study the agreement between different LLM judges and humans on MT-bench and Chatbot Arena datasets. On MT-bench, we also study the agreement among humans. MT-bench represents a small-scale study with controlled human evaluation, while Chatbot Arena represents a larger-scale study with crowdsourced human evaluation in the wild.', 'MT-bench.\\nWe generate answers for all 80 questions with 6 models: GPT-4, GPT-3.5, Claude-V1, Vicuna-13B, Alpaca-13B\\xa0[38], and LLaMA-13B\\xa0[39]. We then use 2 kinds of judges: LLM judges and 58 expert-level human labelers. The labelers are mostly graduate students so they are considered experts and more skilled than average crowd workers.\\nWe let LLM judges evaluate all pairs and let each human evaluate at least 20 random multi-turn questions. This resulted in around 3K votes for all questions.\\nThe detailed data collection process is in Appendix\\xa0C.', 'Chatbot Arena.\\nWe randomly sample 3K single-turn votes from 30K arena data, which covers models including GPT-4, GPT-3.5, Claude, Vicuna-7B/13B, Koala-13B\\xa0[16], Alpaca-13B, LLaMA-13B, and Dolly-12B. We use two kinds of judges: LLM judges and collected crowd judges (2114 unique IPs).', 'Metrics.\\nWe define the agreement between two types of judges as the probability of randomly selected individuals (but not identical) of each type agreeing on a randomly selected question.\\nSee more explanation in Section\\xa0D.3.\\nAverage win rate is the average of win rates against all other players.\\nThese metrics can be computed with or without including tie votes.', 'We compute agreement on MT-bench data.\\nIn Table\\xa05(b), GPT-4 with both pairwise comparison and single answer grading show very high agreements with human experts.\\nThe agreement under setup S2 (w/o tie) between GPT-4 and humans reaches 85%, which is even higher than the agreement among humans (81%). This means GPT-4s judgments closely align with the majority of humans.\\nWe also show that GPT-4s judgments may help humans make better judgments. During our data collection, when a humans choice deviated from GPT-4, we presented GPT-4s judgments to humans and ask if they are reasonable (details in Section\\xa0C.1). Despite different views, humans deemed GPT-4s judgments reasonable in 75% of cases and are even willing to change their choices in 34% of cases.', 'The data from Arena shows a similar trend, as illustrated by Table\\xa07. Comparing GPT-4 and other LLM judges, we find they reach a similar non-tie agreement ratio between humans but the number of non-tied votes from GPT-4 is much larger. This means that GPT-4 is more affirmative and less suffered from position bias but other models also perform well when they give an affirmative answer.', 'In both tables, GPT-4 with single-answer grading matches both pairwise GPT-4 and human preferences very well.\\nThis means GPT-4 has a relatively stable internal rubric.\\nAlthough it may sometimes perform slightly worse than pairwise comparison and give more tie votes, it is a more scalable method.', 'We then perform a breakdown analysis by computing agreement on different model pairs and categories.\\nWe only include non-tied votes.\\nIn Table\\xa07, we observe the agreement between GPT-4 and human progressively increases in line with the performance disparity of the model pairs (i.e., larger win rate difference), from 70% to nearly 100%. This suggests that GPT-4 aligns with humans better when significant performance differences exist between the models.', 'We plot the average win rate of models under different judges on MT-bench and Chatbot Arena in Figure\\xa02 and Figure\\xa03, respectively. The win rate curves from LLM judges closely match the curves from humans. On MT-bench second turn, proprietary models like Claude and GPT-3.5 are more preferred by the humans compared to the first turn, meaning that a multi-turn benchmark can better differentiate some advanced abilities of models. We also list the per-category win rate of representative models in Table\\xa08 to show how MT-bench differentiates models, in which we see GPT-4 is significantly better than others.\\nVicuna-13B is noticeably worse than GPT-3.5/4 in reasoning, math, and coding categories.\\nNote that in math/coding category, GPT-3.5 and GPT-4 have similar overall win-rate because they both failed to answer some hard questions, but GPT-4 is still significantly better than GPT-3 in the direct pairwise comparison or single-answer grading.\\nPlease see a performance breakdown of MT-bench score for each category in Section\\xa0D.4.', 'Human preference benchmarks such as MT-bench and Chatbot Arena serve as valuable additions to the current standardized LLM benchmarks. They focus on different aspects of a model and the recommended way is to comprehensively evaluate models with both kinds of benchmarks.', 'We evaluate several model variants derived from LLaMA on MMLU\\xa0[19], Truthful QA\\xa0[26] (MC1), and MT-bench (GPT-4 judge). The training details are in Appendix\\xa0E.\\nSince we have shown that GPT-4 single-answer grading also performs well in Section\\xa04.2, we use GPT-4 single-answer grading for MT-bench in favor of its scalability and simplicity.\\nWe ask GPT-4 to give a score for each turn on a scale of 10 by using our prompt templates (Figure\\xa05, Figure\\xa09) and report an average score of 160=802160802160=80\\\\times 2160 = 80  2 turns.\\nTable\\xa09 shows the results.\\nWe find that fine-tuning on high-quality dialog datasets (i.e., ShareGPT) can consistently improve the model performance on MMLU and the improvement scales with fine-tuning data size.\\nOn the other hand, a small high-quality conversation dataset can quickly teach the model a style preferred by GPT-4 (or approximately human) but cannot improve MMLU significantly, as shown by the Vicuna-7B (selected) which is trained with only 4.8M tokens or 3K conversations. In Table\\xa09, no single benchmark can determine model quality, meaning that a comprehensive evaluation is needed. Our results indicate that using LLM-as-a-judge to approximate human preferences is highly feasible and could become a new standard in future benchmarks. We are also hosting a regularly updated leaderboard with more models 111https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard.\\nNotably, DynaBench\\xa0[21], a research platform dedicated to dynamic data collection and benchmarking, aligns with our spirit. DynaBench addresses the challenges posed by static standardized benchmarks, such as saturation and overfitting, by emphasizing dynamic data with human-in-the-loop. Our LLM-as-a-judge approach can automate and scale platforms of this nature.', 'Limitations.\\nThis paper emphasizes helpfulness but largely neglects safety. Honesty and harmlessness are crucial for a chat assistant as well\\xa0[2]. We anticipate similar methods can be used to evaluate these metrics by modifying the default prompt. Additionally, within helpfulness, there are multiple dimensions like accuracy, relevance, and creativity, but they are all combined into a single metric in this study. A more comprehensive evaluation can be developed by analyzing and separating these dimensions.\\nWe propose preliminary solutions to address the limitations and biases of LLM-as-a-judge in Section\\xa03.4, but we anticipate more advanced methods can be developed.', 'Data collection and release.\\nAppendix\\xa0C describes the detailed data collection and release processes, which include the instructions we give to users, the screenshots of the data collection interface, the information about participated users, and the content of the released data.', 'Societal impacts.\\nThe societal impact of this study is multi-faceted. Our evaluation methods can help enhance chatbot quality and user experiences. However, addressing biases in these methods is crucial. Our dataset enables better studies of human preferences and model behavior. Advanced chat assistants may replace certain human tasks, resulting in job displacements and new opportunities.', 'Future directions.\\n1) Benchmarking chatbots at scale with a broader set of categories 2) Open-source LLM judge aligned with human preference 3) Enhancing open models math/reasoning capability.', 'In this paper, we propose LLM-as-a-judge for chatbot evaluation and systematically examine its efficacy using human preference data from 58 experts on MT-bench, as well as thousands of crowd-users on Chatbot Arena.\\nOur results reveal that strong LLMs can achieve an agreement rate of over 80%, on par with the level of agreement among human experts, establishing a foundation for an LLM-based evaluation framework.', 'This project is partly supported by gifts from Anyscale, Astronomer, Google, IBM, Intel, Lacework, Microsoft, MBZUAI, Samsung SDS, Uber, and VMware. Lianmin Zheng is supported by a Meta Ph.D. Fellowship.\\nWe extend our thanks to Xinyang Geng, Hao Liu, Eric Wallace, Xuecheng Li, Tianyi Zhang, Qirong Ho, and Kevin Lin for their insightful discussions.']\n"
     ]
    }
   ],
   "source": [
    "# All p tags\n",
    "paragraphs = [i.text for i in page_content.select(\".ltx_section .ltx_para p\")]\n",
    "print(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaide\\Documents\\CodingProjects\\NeuroSymbolicAI\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"C:\\\\Users\\\\jaide\\\\Documents\\\\CodingProjects\\\\NeuroSymbolicAI\\\\rag-pipeline\")\n",
    "from rag_pipeline import RAG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from C:\\Users\\jaide\\.cache\\huggingface\\hub\\models--TheBloke--Mistral-7B-Instruct-v0.2-GGUF\\snapshots\\3a6fbf4a41a1d52e415a4958cde6856d34b2db93\\mistral-7b-instruct-v0.2.Q2_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q2_K:   65 tensors\n",
      "llama_model_loader: - type q3_K:  160 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q2_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 2.87 GiB (3.41 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  2939.57 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '10', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\"}\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "rag = RAG()\n",
    "rag.add_documents(\"collection\", [title] + [authors] + [abstract] + paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   52474.67 ms\n",
      "llama_print_timings:      sample time =      25.38 ms /    98 runs   (    0.26 ms per token,  3861.92 tokens per second)\n",
      "llama_print_timings: prompt eval time =   52473.53 ms /   506 tokens (  103.70 ms per token,     9.64 tokens per second)\n",
      "llama_print_timings:        eval time =   12307.63 ms /    97 runs   (  126.88 ms per token,     7.88 tokens per second)\n",
      "llama_print_timings:       total time =   65121.75 ms /   603 tokens\n"
     ]
    }
   ],
   "source": [
    "output = rag.rag_query(\"collection\", \"Can you tell me what chatbot arena is?\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Chatbot Arena is a platform where users engage in conversations with two chatbots at the same time and rate their responses based on personal preferences. It is a crowdsourced evaluation environment used to study and compare different chatbot models, such as GPT-4, GPT-3.5, Claude, Vicuna, Koala, Alpaca, LLaMA, and Dolly, by having human judges evaluate their conversational abilities in real-world scenarios.\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   50091.68 ms\n",
      "llama_print_timings:      sample time =       7.59 ms /    30 runs   (    0.25 ms per token,  3954.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =   50091.00 ms /   452 tokens (  110.82 ms per token,     9.02 tokens per second)\n",
      "llama_print_timings:        eval time =    3931.05 ms /    29 runs   (  135.55 ms per token,     7.38 tokens per second)\n",
      "llama_print_timings:       total time =   54124.93 ms /   481 tokens\n"
     ]
    }
   ],
   "source": [
    "output = rag.rag_query(\"collection\", \"What is the title of the paper?\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The title of the paper is \"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\".\n"
     ]
    }
   ],
   "source": [
    "print(output)\n",
    "# Retrieval system doesn't seem to be the best, like if there is only 1 word \"title\", it probably won't view it as similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Title: Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena', 'We argue that this discrepancy primarily arises due to\\nexisting evaluation that only measures LLMs core capability on a confined set of tasks (e.g., multi-choice knowledge or retrieval questions), without adequately assessing its alignment with human preference in open-ended tasks, such as the ability to accurately adhere to instructions in multi-turn dialogues.\\nAs a demonstration, we show conversation histories with two models on an MMLU question in Figure\\xa01.\\nThe two models are LLaMA-13B\\xa0[39], a pre-trained base model without fine-tuning, and Vicuna-13B, our fine-tuned model from LLaMA-13B on high-quality conversations (the training details are in Appendix\\xa0E).\\nDespite the base LLaMA models showing competitive performance on conventional benchmarks (Table\\xa09), its answers to open-ended questions are often not preferred by humans.\\nThis misalignment of conventional benchmarks underscores the core problem driving this paper:\\nthe need for a robust and scalable automated method to evaluate LLM alignment with human preferences.', 'Self-enhancement bias.\\nWe adopt the term self-enhancement bias from social cognition literature\\xa0[4] to describe the effect that LLM judges may favor the answers generated by themselves.']\n"
     ]
    }
   ],
   "source": [
    "print(rag.retrieve(\"collection\", \"What is the title\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
