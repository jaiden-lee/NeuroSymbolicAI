{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jaide\\Documents\\CodingProjects\\NeuroSymbolicAI\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"C:\\\\Users\\\\jaide\\\\Documents\\\\CodingProjects\\\\NeuroSymbolicAI\\\\arxiv-rag\")\n",
    "from arxiv_rag import ArxivRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You cycle through Thought, Action, PAUSE, Observation. At the end of the loop you output a final Answer. Your final answer should be highly specific to the observations you have from running\n",
    "the actions.\n",
    "1. Thought: Describe your thoughts about the question you have been asked.\n",
    "2. Action: run one of the actions available to you - then return PAUSE.\n",
    "3. PAUSE\n",
    "4. Observation: will be the result of running those actions.\n",
    "\n",
    "Available actions:\n",
    "- retrieveDocuments:\n",
    "    E.g. retrieveDocuments: Who are the authors of the MTBench paper?\n",
    "    Returns a collection of documents about the paper MTBench LLM Judge paper.\n",
    "- calculator:\n",
    "    E.g. calculator: 5+10\n",
    "    Returns the answer to the provided math equation. \n",
    "- convertToWordProblem:\n",
    "    E.g. convertToWordProblem: Bob has 5 friends. He buys 3 apples for each of his friends. How many apples does he have to buy?\n",
    "    Returns the answer to the provided math equation. \n",
    "\n",
    "Example session 1:\n",
    "Question: Bob has 5 bananas. He eats 2 of them. How many does he have left?.\n",
    "Thought: I am being provided a word problem. I should turn this word problem into an equation. \n",
    "Action: convertToWordProblem: Bob has 5 bananas. He eats 2 of them. How many does he have left?\n",
    "PAUSE\n",
    "\n",
    "You will be called again with something like this:\n",
    "Observation: 5-2\n",
    "\n",
    "Action: calculator: 5-2\n",
    "PAUSE\n",
    "\n",
    "Observation: 3\n",
    "\n",
    "Answer: Bob has 3 apples left.\n",
    "\n",
    "Example session 2:\n",
    "Question: What is the difference between MTBench and Chatbot Arena?\n",
    "Thought: I should first find out what MTBench is.\n",
    "Action: retrieveDocuments: What is MTBench?\n",
    "PAUSE\n",
    "\n",
    "Observation: <A set of 5 sentences describing what MTBench is.>\n",
    "\n",
    "Thought: Now I need to find out what Chatbot Arena is.\n",
    "Action: retrieveDocuments: What is Chatbot Arena?\n",
    "PAUSE\n",
    "\n",
    "Observation: <A set of 5 sentences describing what Chatbot Arena is.>\n",
    "\n",
    "Answer: <A comparison between MTBench and Chatbot Arena given your newfound knowledge based on the documents provided to you>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 24 key-value pairs and 291 tensors from C:\\Users\\jaide\\.cache\\huggingface\\hub\\models--TheBloke--Mistral-7B-Instruct-v0.2-GGUF\\snapshots\\3a6fbf4a41a1d52e415a4958cde6856d34b2db93\\mistral-7b-instruct-v0.2.Q2_K.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  21:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  22:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  23:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q2_K:   65 tensors\n",
      "llama_model_loader: - type q3_K:  160 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 1000000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q2_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 2.87 GiB (3.41 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  2939.57 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 32000\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 1000000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  4000.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 4000.00 MiB, K (f16): 2000.00 MiB, V (f16): 2000.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =  2094.51 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.2', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '10', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '1000000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\"}\n",
      "Guessed chat format: mistral-instruct\n"
     ]
    }
   ],
   "source": [
    "uri = \"https://arxiv.org/html/2306.05685v4\"\n",
    "agent = ArxivRAG(uri, \"llm-judge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Julian has 3 grapes. He bought 2. How many does he have now?\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "[SYSTEM]\n",
    "{system_prompt}\n",
    "\n",
    "[USER]\n",
    "{question}\n",
    "\n",
    "[ASSISTANT]\n",
    "Answer: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   62251.13 ms\n",
      "llama_print_timings:      sample time =      12.21 ms /    66 runs   (    0.18 ms per token,  5406.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =   27612.34 ms /   400 tokens (   69.03 ms per token,    14.49 tokens per second)\n",
      "llama_print_timings:        eval time =   10431.38 ms /    65 runs   (  160.48 ms per token,     6.23 tokens per second)\n",
      "llama_print_timings:       total time =   38231.98 ms /   465 tokens\n"
     ]
    }
   ],
   "source": [
    "output = agent.normal_query(prompt, tokens=512, temp=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   62251.13 ms\n",
      "llama_print_timings:      sample time =      19.62 ms /   110 runs   (    0.18 ms per token,  5607.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2050.36 ms /    23 tokens (   89.15 ms per token,    11.22 tokens per second)\n",
      "llama_print_timings:        eval time =   16154.22 ms /   109 runs   (  148.20 ms per token,     6.75 tokens per second)\n",
      "llama_print_timings:       total time =   18494.41 ms /   132 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: Initially, Julian has 3 grapes. In each iteration, he buys an additional 2 grapes and then eats 1 of those new grapes. So, in each cycle he gains 1 new grape (bought grape - eaten grape). He does this 4 times, so the total number of grapes he now has is his initial amount plus what he gained through buying and eating grapes 4 times.\n",
      "\n",
      "Answer: 3 + (2-1)*4\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "[SYSTEM]\n",
    "You will be given a word problem. Convert the word problem into a simple math equation. DO NOT EVALUATE THE EQUATION. You should break down each word problem step by step by providing your own Thought. Here are some examples:\n",
    "\n",
    "Example 1: \n",
    "Problem: Bob has 5 bananas. He eats 2 of them. How many does he have left?.\n",
    "Thought: Bob originally has 5 bananas. Then, we must take away 2 of the bananas since he eats 2 of them.\n",
    "Answer: 5-3\n",
    "\n",
    "Example 2:\n",
    "Problem: Bob has 5 friends. He buys 3 bananas for each friend. How many bananas does he buy?\n",
    "Thought: Each one of Bob's friends needs 3 bananas. He has 5 friends.\n",
    "Answer: 5*3\n",
    "\n",
    "Example 3:\n",
    "Problem: Bob has 2 friends and 2 brothers. He needs to buy 4 bananas for each friend, and 3 bananas for each brother. How many bananas does he buy?\n",
    "Thought: Since Bob has 2 friends and each friend needs 4 bananas, he should buy 2*4 bananas for his friends. Since Bob has 2 brothers and each brother needs 3 bananas, he should buy 2*3 bananas for his brothers.\n",
    "Answer: 2*4+2*3\n",
    "\n",
    "[USER]\n",
    "Problem: Julian has 3 grapes. He bought 2. Then, he ate one of them. He repeated this process 4 times. How many does he have now?\n",
    "\n",
    "[ASSISTANT]\n",
    "\"\"\"\n",
    "output = agent.normal_query(prompt, tokens=512, temp=1)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Julian has 1 grape left after buying 2.\n",
      "\n",
      "Thought: This is another word problem. I should turn it into an equation to solve.\n",
      "Action: convertToWordProblem: Julian has 3 grapes. He bought 2. How many does he have now?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   62251.13 ms\n",
      "llama_print_timings:      sample time =       6.88 ms /    26 runs   (    0.26 ms per token,  3778.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    5359.84 ms /    26 runs   (  206.15 ms per token,     4.85 tokens per second)\n",
      "llama_print_timings:       total time =    5454.75 ms /    27 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: I should perform a simple subtraction calculation.\n",
      "Action: calculator: 5-6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"What is 5 minus 6\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "[SYSTEM]\n",
    "{system_prompt}\n",
    "\n",
    "[USER]\n",
    "{question}\n",
    "\n",
    "[ASSISTANT]\n",
    "Answer: \n",
    "\"\"\"\n",
    "output = agent.normal_query(prompt, tokens=512, temp=1)\n",
    "print(output)\n",
    "# this gave a super goofy answer with x=5 x=6, so I just set temp to 0\n",
    "# its not consistent; sometimes, it gives the answer in the right format, and sometimes it just doesn't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   62251.13 ms\n",
      "llama_print_timings:      sample time =       9.20 ms /    35 runs   (    0.26 ms per token,  3806.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =   36809.83 ms /    38 tokens (  968.68 ms per token,     1.03 tokens per second)\n",
      "llama_print_timings:        eval time =    4570.52 ms /    34 runs   (  134.43 ms per token,     7.44 tokens per second)\n",
      "llama_print_timings:       total time =    9976.31 ms /    72 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: The operation results in an answer that is not valid since it is negative, and negative numbers are not allowed within the given context of the problem statement.\n"
     ]
    }
   ],
   "source": [
    "test = \"\"\"\n",
    "[SYSTEM]\n",
    "Available actions:\n",
    "- retrieveDocuments:\n",
    "    E.g. retrieveDocuments: Who are the authors of the MTBench paper?\n",
    "    Returns a collection of documents about the paper MTBench LLM Judge paper.\n",
    "- calculator:\n",
    "    E.g. calculator: 5+10\n",
    "    Returns the answer to the provided math equation. \n",
    "\n",
    "\n",
    "Example session 1:\n",
    "Question: Bob has 5 bananas. He eats 2 of them. How many does he have left?.\n",
    "Thought: I am being provided a word problem. I should turn this word problem into an equation. \n",
    "Action: calculator: Bob has 5 bananas. He eats 2 of them. How many does he have left?\n",
    "PAUSE\n",
    "\n",
    "You will be called again with something like this:\n",
    "Observation: 5-2\n",
    "\n",
    "Action: calculator: 5-2\n",
    "PAUSE\n",
    "\n",
    "Observation: 3\n",
    "\n",
    "Answer: Bob has 3 apples left.\n",
    "\n",
    "Example session 2:\n",
    "Question: What is the difference between MTBench and Chatbot Arena?\n",
    "Thought: I should first find out what MTBench is.\n",
    "Action: retrieveDocuments: What is MTBench?\n",
    "PAUSE\n",
    "\n",
    "Observation: <A set of 5 sentences describing what MTBench is.>\n",
    "\n",
    "Thought: Now I need to find out what Chatbot Arena is.\n",
    "Action: retrieveDocuments: What is Chatbot Arena?\n",
    "PAUSE\n",
    "\n",
    "Observation: <A set of 5 sentences describing what Chatbot Arena is.>\n",
    "\n",
    "Answer: <A comparison between MTBench and Chatbot Arena given your newfound knowledge based on the documents provided to you>\n",
    "\n",
    "\n",
    "[USER]\n",
    "What is 5 minus 6\n",
    "\n",
    "[ASSISTANT]\n",
    "5 minus 6 is -1. However, -1 is not possible since the answer is negative, and negative numbers are not allowed.\n",
    "\n",
    "[USER]\n",
    "Your most recent answer was incorrect, as it did not follow the Thought, Action, PAUSE, Observation pattern as outlined in the SYSTEM prompt. Reanswer the question following this pattern.\n",
    "\n",
    "[ASSISTANT]\n",
    "Thought: I am being provided a math equation.\n",
    "Action: calculator: 5 - 6\n",
    "Observation:-1\n",
    "\"\"\"\n",
    "output = agent.normal_query(test, tokens=512,temp=1)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   62251.13 ms\n",
      "llama_print_timings:      sample time =      36.02 ms /   133 runs   (    0.27 ms per token,  3692.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8221.17 ms /    54 tokens (  152.24 ms per token,     6.57 tokens per second)\n",
      "llama_print_timings:        eval time =   16311.11 ms /   132 runs   (  123.57 ms per token,     8.09 tokens per second)\n",
      "llama_print_timings:       total time =   25003.99 ms /   186 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original question asked for the result of subtracting 5 from 6. The operation results in -1 as the answer. Since we're working within standard arithmetic rules that don't support negative numbers as solutions without additional context, we can consider alternative approaches such as treating the number system with different bases or using mathematical equivalents like borrows and carries to help understand the process better, but in this simple subtraction case with common bases, it would not be feasible to accept -1 as a solution. The correct answer is that the operation cannot be completed within the given standard arithmetic rules due to the negative outcome.\n"
     ]
    }
   ],
   "source": [
    "test2 = \"\"\"\n",
    "[USER]\n",
    "Question: What is 5 minus 6\n",
    "\n",
    "[ASSISTANT]\n",
    "Answer: The operation results in an answer that is not valid since it is negative, and negative numbers are not allowed within the given context of the problem statement.\n",
    "\n",
    "[USER]\n",
    "Evaluate the final answer based on its correctness and provide feedback to the previous answer. Make sure the final answer is specific and relevant to the original question, and fully answers the original question. Make sure the final answer does not add to the original context of the question asked, and only answers the question asked.\n",
    "\n",
    "[ASSISTANT]\n",
    "\"\"\"\n",
    "output = agent.normal_query(test2, tokens=512,temp=2)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   62251.13 ms\n",
      "llama_print_timings:      sample time =      13.62 ms /    46 runs   (    0.30 ms per token,  3377.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12647.50 ms /    42 tokens (  301.13 ms per token,     3.32 tokens per second)\n",
      "llama_print_timings:        eval time =   14850.27 ms /    45 runs   (  330.01 ms per token,     3.03 tokens per second)\n",
      "llama_print_timings:       total time =   19910.01 ms /    87 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: The question is asking for factual information about the current president of the United States. I should look that up.\n",
      "Action: retrieveDocuments: Who is the current president of the United States?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "question = \"Who is the president of the United States\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "[SYSTEM]\n",
    "{system_prompt}\n",
    "\n",
    "[USER]\n",
    "{question}\n",
    "\n",
    "[ASSISTANT]\n",
    "Answer: \n",
    "\"\"\"\n",
    "output = agent.normal_query(prompt, tokens=512, temp=1)\n",
    "print(output)\n",
    "# it does rag, even though it doesn't have a tool; this isn't bad though, since its the best bet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   62251.13 ms\n",
      "llama_print_timings:      sample time =       9.57 ms /    36 runs   (    0.27 ms per token,  3760.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6471.05 ms /    27 tokens (  239.67 ms per token,     4.17 tokens per second)\n",
      "llama_print_timings:        eval time =    5748.04 ms /    35 runs   (  164.23 ms per token,     6.09 tokens per second)\n",
      "llama_print_timings:       total time =   12347.06 ms /    62 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: I need to find out the policies of both Thomas Jefferson and John Adams.\n",
      "Action: retrieveDocuments: Who are the policies of Thomas Jefferson?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"What is the difference between Thomas Jefferson and John Adam's policy?\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "[SYSTEM]\n",
    "{system_prompt}\n",
    "\n",
    "[USER]\n",
    "{question}\n",
    "\n",
    "[ASSISTANT]\n",
    "Answer: \n",
    "\"\"\"\n",
    "output = agent.normal_query(prompt, tokens=512, temp=1)\n",
    "print(output)\n",
    "    # This is a problem though; it didn't use an action/didn't follow the prompt, maybe its still really bad at comparison questions/multipart questions?\n",
    "    # even when lowering tempearutre, it doesn't chanege anything\n",
    "    # if set temperature to 0, it still didn't follow my directions; it always just tries to answer the question itself\n",
    "    # BUT USING A HIGHER TEMPEARTURE WORKED (at least one time)\n",
    "    # temperature of 2 --> works as well; it seems higher temperautr eis better actually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SYSTEM]\n",
      "\n",
      "You cycle through Thought, Action, PAUSE, Observation. At the end of the loop you output a final Answer. Your final answer should be highly specific to the observations you have from running\n",
      "the actions.\n",
      "1. Thought: Describe your thoughts about the question you have been asked.\n",
      "2. Action: run one of the actions available to you - then return PAUSE.\n",
      "3. PAUSE\n",
      "4. Observation: will be the result of running those actions.\n",
      "\n",
      "Available actions:\n",
      "- retrieveDocuments:\n",
      "    E.g. retrieveDocuments: Who are the authors of the MTBench paper?\n",
      "    Returns a collection of documents about the paper MTBench LLM Judge paper. No arguments needed.\n",
      "- calculator:\n",
      "    E.g. calculator: 5+10\n",
      "    Returns the answer to the provided math equation. \n",
      "- convertWordProblemToEquation:\n",
      "    E.g. convertWordProblemToEquation: Jack has 5 apples\n",
      "    Returns the answer to the provided math equation. \n",
      "\n",
      "Example session 1:\n",
      "Question: Bob has 5 bananas. He eats 2 of them. How many does he have left?.\n",
      "Thought: I am being provided a word problem. I should turn this word problem into an equation.\n",
      "Action: convertWordProblemToEquation: Bob has 5 bananas. He eats 2 of them. How many does he have left?\n",
      "PAUSE\n",
      "\n",
      "You will be called again with something like this:\n",
      "Observation: 5-2\n",
      "\n",
      "Then you loop again:\n",
      "Thought: Now I need to solve this math equation with my calculator.\n",
      "Action: calculator: 5-2\n",
      "PAUSE\n",
      "\n",
      "You'll then be called again with something like this:\n",
      "Observation: 3\n",
      "\n",
      "You then output:\n",
      "Answer: Bob has 3 apples left.\n",
      "\n",
      "\n",
      "Example session 2:\n",
      "Question: What is the difference between MTBench and Chatbot Arena?\n",
      "Thought: I should first find out what MTBench is.\n",
      "Action: retrieveDocuments: What is MTBench?\n",
      "PAUSE\n",
      "\n",
      "You will be called again with something like this:\n",
      "Observation: <A set of 5 sentences describing what MTBench is.>\n",
      "\n",
      "Then you loop again:\n",
      "Thought: Now I need to find out what Chatbot Arena is.\n",
      "Action: retrieveDocuments: What is Chatbot Arena?\n",
      "PAUSE\n",
      "\n",
      "You'll then be called again with something like this:\n",
      "Observation: <A set of 5 sentences describing what Chatbot Arena is.>\n",
      "\n",
      "You then output:\n",
      "Answer: <A comparison between MTBench and Chatbot Arena given your newfound knowledge based on the documents provided to you>\n",
      "\n",
      "\n",
      "[USER]\n",
      "What is the difference between Thomas Jefferson and John Adam's policy?\n",
      "\n",
      "[ASSISTANT]\n",
      "Answer: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   62251.13 ms\n",
      "llama_print_timings:      sample time =      11.41 ms /    46 runs   (    0.25 ms per token,  4031.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4295.21 ms /    23 tokens (  186.75 ms per token,     5.35 tokens per second)\n",
      "llama_print_timings:        eval time =    6309.49 ms /    45 runs   (  140.21 ms per token,     7.13 tokens per second)\n",
      "llama_print_timings:       total time =   10765.14 ms /    68 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To provide a precise answer, I first need to access the most recent statistical data for both MTBench and Chatbot Arena.\n",
      "\n",
      "Action: retrieveDocuments: Get the latest statistics of MTBench.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"Compare MTBench statistics to Chatbot Arena stats\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "[SYSTEM]\n",
    "{system_prompt}\n",
    "\n",
    "[USER]\n",
    "{question}\n",
    "\n",
    "[ASSISTANT]\n",
    "Answer: \n",
    "\"\"\"\n",
    "output = agent.normal_query(prompt, tokens=512, temp=1)\n",
    "print(output)\n",
    "# This is a problem though; it didn't use an action/didn't follow the prompt\n",
    "# ok so now it messed up the formatting\n",
    "# I need to figure out how to get it to follow the format\n",
    "# now it gave a real response with temperature of .2\n",
    "# now with a temperature of 2 --> it works well, for some reason high temperatuer works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   31844.13 ms\n",
      "llama_print_timings:      sample time =       8.80 ms /    29 runs   (    0.30 ms per token,  3293.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8535.79 ms /   109 tokens (   78.31 ms per token,    12.77 tokens per second)\n",
      "llama_print_timings:        eval time =    4855.62 ms /    28 runs   (  173.42 ms per token,     5.77 tokens per second)\n",
      "llama_print_timings:       total time =   13521.13 ms /   137 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought: Now I need to solve this math equation with my calculator.\n",
      "Action: calculator: 3+2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NOW FEEDING OBSERVATION BACK\n",
    "\n",
    "question = \"Julian has 3 grapes. He bought 2. How many does he have now?\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "[SYSTEM]\n",
    "{system_prompt}\n",
    "\n",
    "[USER]\n",
    "{question}\n",
    "\n",
    "[ASSISTANT]\n",
    "Thought: I am being provided a word problem. I should turn this word problem into an equation.\n",
    "Action: convertWordProblemToEquation: Julian has 3 grapes. He bought 2. How many does he have now?\n",
    "PAUSE\n",
    "\n",
    "[USER]\n",
    "Observation: 3+2\n",
    "\n",
    "[ASSISTANT]\n",
    "Answer: \n",
    "\"\"\"\n",
    "output = agent.normal_query(prompt, tokens=512, temp=2)\n",
    "print(output)\n",
    "# now it solves this correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   31844.13 ms\n",
      "llama_print_timings:      sample time =       1.76 ms /    12 runs   (    0.15 ms per token,  6822.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4042.44 ms /    48 tokens (   84.22 ms per token,    11.87 tokens per second)\n",
      "llama_print_timings:        eval time =    1572.64 ms /    11 runs   (  142.97 ms per token,     6.99 tokens per second)\n",
      "llama_print_timings:       total time =    5641.11 ms /    59 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Julian now has 5 grapes.\n"
     ]
    }
   ],
   "source": [
    "# NOW FEEDING OBSERVATION BACK\n",
    "\n",
    "question = \"Julian has 3 grapes. He bought 2. How many does he have now?\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "[SYSTEM]\n",
    "{system_prompt}\n",
    "\n",
    "[USER]\n",
    "{question}\n",
    "\n",
    "[ASSISTANT]\n",
    "Thought: I am being provided a word problem. I should turn this word problem into an equation.\n",
    "Action: convertWordProblemToEquation: Julian has 3 grapes. He bought 2. How many does he have now?\n",
    "PAUSE\n",
    "\n",
    "[USER]\n",
    "Observation: 3+2\n",
    "\n",
    "[ASSISTANT]\n",
    "Thought: Now I need to solve this math equation with my calculator.\n",
    "Action: calculator: 3+2\n",
    "PAUSE\n",
    "\n",
    "[USER]\n",
    "Observation: 5\n",
    "\n",
    "[ASSISTANT]\n",
    "\"\"\"\n",
    "output = agent.normal_query(prompt, tokens=512, temp=2)\n",
    "print(output)\n",
    "# and now once it has all information, it gives the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =   31844.13 ms\n",
      "llama_print_timings:      sample time =       9.09 ms /    46 runs   (    0.20 ms per token,  5062.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =   53489.09 ms /   621 tokens (   86.13 ms per token,    11.61 tokens per second)\n",
      "llama_print_timings:        eval time =    7635.66 ms /    45 runs   (  169.68 ms per token,     5.89 tokens per second)\n",
      "llama_print_timings:       total time =   61257.97 ms /   666 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feedback: My answer above is factually accurate and complete, so I give my answer a rating of 10. However, in future responses, I will strive for greater elaboration to ensure better user understanding.\n"
     ]
    }
   ],
   "source": [
    "# NOW FEEDING OBSERVATION BACK\n",
    "\n",
    "question = \"Julian has 3 grapes. He bought 2. How many does he have now?\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "[SYSTEM]\n",
    "{system_prompt}\n",
    "\n",
    "[USER]\n",
    "{question}\n",
    "\n",
    "[ASSISTANT]\n",
    "Thought: I am being provided a word problem. I should turn this word problem into an equation.\n",
    "Action: convertWordProblemToEquation: Julian has 3 grapes. He bought 2. How many does he have now?\n",
    "PAUSE\n",
    "\n",
    "[USER]\n",
    "Observation: 3+2\n",
    "\n",
    "[ASSISTANT]\n",
    "Thought: Now I need to solve this math equation with my calculator.\n",
    "Action: calculator: 3+2\n",
    "PAUSE\n",
    "\n",
    "[USER]\n",
    "Observation: 5\n",
    "\n",
    "[ASSISTANT]\n",
    "Answer: Julian now has 5 grapes.\n",
    "\n",
    "[USER]\n",
    "Now score your answer on a scale from 1-10, with 1 meaning the answer is bad, and 10 meaning the answer is good. Score the answers based on their accuracy, as well as their understandability.\n",
    "\n",
    "[ASSISTANT]\n",
    "\"\"\"\n",
    "output = agent.normal_query(prompt, tokens=512, temp=2)\n",
    "print(output)\n",
    "# and now once it has all information, it gives the answer\n",
    "# the scoring system doesn't work since it wasn't in original system prompt\n",
    "# if I modify the system prompt, it gives feedback; I should probably make it better, but for now its good to showcase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
